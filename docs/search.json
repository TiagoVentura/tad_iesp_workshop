[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Advanced Text-As-Data: Word Embeddings, Deep Learning, and Large Language Models",
    "section": "",
    "text": "In recent years, the surge in the availability of textual data, ranging from the digitalization of archival documents, political speeches, social media posts, and online news articles, has led to a growing demand for statistical analysis using large corpora. Once dominated by sparse bag-of-words models, the field of Natural Language Processing (NLP) is now increasingly driven by dense vector representations, deep learning, and the rapid evolution of Large Language Models (LLMs). This course offers an introduction to this new generation of models, serving as hands-on approach to this new landscape of computational text analysis with a focus on political science research and applications.\nThe class will cover four broad topics. We start with an overview of how to represent text as data, from a sparse representation via bag-of-words models, to a dense representation using word embeddings. We then discuss the use of deep learning models for text representation and downstream classification tasks. From here, we will discuss the foundation of the state-of-art machine learning models for text analysis: transformer models. Lastly, we will discuss several applications of Large Language Models in social science tasks.\nThe course will consist of lectures and hands-on coding in class. The lecture will be conducted in English, but students are free to ask questions in Portuguese. Students will have time in the afternoon to practice the code seen in class, and we will suggest additional coding exercises. We assume students attending this class have taken, at a minimum, an introductory course in statistics and have basic knowledge of probability distributions, calculus, hypothesis testing, and linear models. The course will use a mix of R and Python, two computational languages students should be familiar with. That being said, students should be able to follow the course even if they are just starting with any of the two programming languages."
  },
  {
    "objectID": "index.html#course-description",
    "href": "index.html#course-description",
    "title": "Advanced Text-As-Data: Word Embeddings, Deep Learning, and Large Language Models",
    "section": "",
    "text": "In recent years, the surge in the availability of textual data, ranging from the digitalization of archival documents, political speeches, social media posts, and online news articles, has led to a growing demand for statistical analysis using large corpora. Once dominated by sparse bag-of-words models, the field of Natural Language Processing (NLP) is now increasingly driven by dense vector representations, deep learning, and the rapid evolution of Large Language Models (LLMs). This course offers an introduction to this new generation of models, serving as hands-on approach to this new landscape of computational text analysis with a focus on political science research and applications.\nThe class will cover four broad topics. We start with an overview of how to represent text as data, from a sparse representation via bag-of-words models, to a dense representation using word embeddings. We then discuss the use of deep learning models for text representation and downstream classification tasks. From here, we will discuss the foundation of the state-of-art machine learning models for text analysis: transformer models. Lastly, we will discuss several applications of Large Language Models in social science tasks.\nThe course will consist of lectures and hands-on coding in class. The lecture will be conducted in English, but students are free to ask questions in Portuguese. Students will have time in the afternoon to practice the code seen in class, and we will suggest additional coding exercises. We assume students attending this class have taken, at a minimum, an introductory course in statistics and have basic knowledge of probability distributions, calculus, hypothesis testing, and linear models. The course will use a mix of R and Python, two computational languages students should be familiar with. That being said, students should be able to follow the course even if they are just starting with any of the two programming languages."
  },
  {
    "objectID": "index.html#instructors",
    "href": "index.html#instructors",
    "title": "Advanced Text-As-Data: Word Embeddings, Deep Learning, and Large Language Models",
    "section": "Instructors",
    "text": "Instructors\nTiago Ventura\n\nAssistant Professor in Computational Social Science, Georgetown University\nPronouns: He/Him\nEmail: tv186@georgetown.edu\n\nSebastian Vallejo\n\nAssistant Professor in the Department of Political Science at the University of Western Ontario\nPronouns: He/Him\nEmail: sebastian.vallejo@uwo.ca"
  },
  {
    "objectID": "schedule.html",
    "href": "schedule.html",
    "title": "Schedule",
    "section": "",
    "text": "Week\n\n\nTopic\n\n\nDate\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "schedule.html#weekly-schedule",
    "href": "schedule.html#weekly-schedule",
    "title": "Schedule",
    "section": "",
    "text": "Week\n\n\nTopic\n\n\nDate\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "syllabus.html",
    "href": "syllabus.html",
    "title": "Advanced Text-As-Data: Word Embeddings, Deep Learning, and Large Language Models",
    "section": "",
    "text": "In recent years, the surge in the availability of textual data, ranging from the digitalization of archival documents, political speeches, social media posts, and online news articles, has led to a growing demand for statistical analysis using large corpora. Once dominated by sparse bag-of-words models, the field of Natural Language Processing (NLP) is now increasingly driven by dense vector representations, deep learning, and the rapid evolution of Large Language Models (LLMs). This course offers an introduction to this new generation of models, serving as hands-on approach to this new landscape of computational text analysis with a focus on political science research and applications.\nThe class will cover four broad topics. We start with an overview of how to represent text as data, from a sparse representation via bag-of-words models, to a dense representation using word embeddings. We then discuss the use of deep learning models for text representation and downstream classification tasks. From here, we will discuss the foundation of the state-of-art machine learning models for text analysis: transformer models. Lastly, we will discuss several applications of Large Language Models in social science tasks.\nThe course will consist of lectures and hands-on coding in class. The lecture will be conducted in English, but students are free to ask questions in Portuguese. Students will have time in the afternoon to practice the code seen in class, and we will suggest additional coding exercises. We assume students attending this class have taken, at a minimum, an introductory course in statistics and have basic knowledge of probability distributions, calculus, hypothesis testing, and linear models. The course will use a mix of R and Python, two computational languages students should be familiar with. That being said, students should be able to follow the course even if they are just starting with any of the two programming languages."
  },
  {
    "objectID": "syllabus.html#course-description",
    "href": "syllabus.html#course-description",
    "title": "Advanced Text-As-Data: Word Embeddings, Deep Learning, and Large Language Models",
    "section": "",
    "text": "In recent years, the surge in the availability of textual data, ranging from the digitalization of archival documents, political speeches, social media posts, and online news articles, has led to a growing demand for statistical analysis using large corpora. Once dominated by sparse bag-of-words models, the field of Natural Language Processing (NLP) is now increasingly driven by dense vector representations, deep learning, and the rapid evolution of Large Language Models (LLMs). This course offers an introduction to this new generation of models, serving as hands-on approach to this new landscape of computational text analysis with a focus on political science research and applications.\nThe class will cover four broad topics. We start with an overview of how to represent text as data, from a sparse representation via bag-of-words models, to a dense representation using word embeddings. We then discuss the use of deep learning models for text representation and downstream classification tasks. From here, we will discuss the foundation of the state-of-art machine learning models for text analysis: transformer models. Lastly, we will discuss several applications of Large Language Models in social science tasks.\nThe course will consist of lectures and hands-on coding in class. The lecture will be conducted in English, but students are free to ask questions in Portuguese. Students will have time in the afternoon to practice the code seen in class, and we will suggest additional coding exercises. We assume students attending this class have taken, at a minimum, an introductory course in statistics and have basic knowledge of probability distributions, calculus, hypothesis testing, and linear models. The course will use a mix of R and Python, two computational languages students should be familiar with. That being said, students should be able to follow the course even if they are just starting with any of the two programming languages."
  },
  {
    "objectID": "syllabus.html#learning-goals",
    "href": "syllabus.html#learning-goals",
    "title": "Syllabus: PPOL 5203 - Data Science I: Foundations",
    "section": "Learning Goals",
    "text": "Learning Goals\nAfter completing this course, the students will be able to:\n\nGeneral understanding of python’s object oriented programming syntax and data structures.\nCompetency using version control (Git/Github).\nLearn to manipulate and explore data with Pandas and other tools.\nGeneral understanding of analyzing algorithms and data structures.\nLearn to extract and process data from structured and unstructured sources.\nGet some intuition of modeling text data in Python.\nLearn the basics of machine learning as a modeling approach.\nLearn basics of using SQL to query databases."
  },
  {
    "objectID": "syllabus.html#instructors-and-tas",
    "href": "syllabus.html#instructors-and-tas",
    "title": "Syllabus: PPOL 5203 - Data Science I: Foundations",
    "section": "Instructors and TAs",
    "text": "Instructors and TAs\n\nInstructor\n\nProfessor: Dr. Tiago Ventura\nPronouns: He/Him\nEmail: tv186@georgetown.edu\nOffice hours:\n\nTime: Every Tuesday, 4pm - 6pm\nLocation: 125E, Office Number 766"
  },
  {
    "objectID": "syllabus.html#teaching-assistants",
    "href": "syllabus.html#teaching-assistants",
    "title": "Syllabus: PPOL 5203 - Data Science I: Foundations",
    "section": "Teaching Assistants:",
    "text": "Teaching Assistants:\n\nAastha Jha (DSPP Second-Year Student)\n\nEmail: aj935@georgetown.edu\nOffice Hours:\n\nEvery Wednesdays, from 1pm to 2pm.\n\nLocation:\n\n\nShirui Zhou (DSPP Alumni)\n\nEmail: sz614@georgetown.edu\nOffice Hours:\n\nEvery Monday, from 1pm to 2pm\nLocation:"
  },
  {
    "objectID": "syllabus.html#our-classes",
    "href": "syllabus.html#our-classes",
    "title": "Syllabus: PPOL 5203 - Data Science I: Foundations",
    "section": "Our classes",
    "text": "Our classes\nClasses will take place at the scheduled class time/place and will involve a combination of lectures, coding walkthrough, breakout group sessions, and questions. We will start our classes with a lecture highlighting what I consider to be the broader substantive and programming concepts covered in the class. From that, we will switch to a mix of coding walk through and breakout group sessions.\nFor every lecture, you will have access to a notebook (.ipynb) covering the topics and code discussed in class. I will upload these materials (which I call lecture notes every day before the class starts). In addition, you will also have access (in at least a week in advance), of required readings (book chapters, articles, blog posts or coding tutorials) for every class. What you will take from this class will be tremendously improved if you work through all these materials.\nNote that this class is scheduled to meet weekly for 2.5 hours. I will do my best to make our meetings dynamic and enjoyable for all parts involved. We will take one or two breaks in each of our lecture."
  },
  {
    "objectID": "syllabus.html#required-materials",
    "href": "syllabus.html#required-materials",
    "title": "Advanced Text-As-Data: Word Embeddings, Deep Learning, and Large Language Models",
    "section": "Required Materials",
    "text": "Required Materials\nReadings: We will rely primarily on the following textbook for this course. The textbook is freely available online\n\nDaniel Jurafsky and James H. Martin. Speech and Language Processing, 3nd Edition. - [SLP]\n\nThe weekly articles are listed in the syllabus"
  },
  {
    "objectID": "syllabus.html#course-infrastructure",
    "href": "syllabus.html#course-infrastructure",
    "title": "Syllabus: PPOL 5203 - Data Science I: Foundations",
    "section": "Course Infrastructure",
    "text": "Course Infrastructure\nClass Website: A class website https://tiagoventura.github.io/ppol5203 will be used throughout the course and should be checked on a regular basis for lecture materials and required readings.\nClass Slack Channel: The class also has a dedicated slack channel (ppol-564-fall-2024.slack.com). The channel serves as an open forum to discuss, collaborate, pose problems/questions, and offer solutions. Students are encouraged to pose any questions they have there as this will provide the professor and TA the means of answering the question so that all can see the response. If you’re unfamiliar with, please consult the following start-up tutorial (https://get.slack.help/hc/en-us/articles/218080037-Getting-started-for-new-members). Please follow the invite link to be added to the Slack channel.\nCanvas: A Canvas site (http://canvas.georgetown.edu) will be used throughout the course and should be checked on a regular basis for announcements. ll announcements for the assignments and classes will be posted on Canvas; they will not be distributed in class or by e-mail. Support for Canvas is available at (202) 687-4949\nNOTE: Students are encouraged to run lecture code on their own machines. If you do not have access to a laptop on which you can install python3, please contact the professor and/or TA for assistance. Only python3 will be used in this course."
  },
  {
    "objectID": "syllabus.html#weekly-schedule",
    "href": "syllabus.html#weekly-schedule",
    "title": "Syllabus: PPOL 5203 - Data Science I: Foundations",
    "section": "Weekly Schedule",
    "text": "Weekly Schedule\n\n\n\n\n\n\nWeek\n\n\nTopic\n\n\nDate\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "syllabus.html#course-requirements",
    "href": "syllabus.html#course-requirements",
    "title": "Syllabus: PPOL 5203 - Data Science I: Foundations",
    "section": "Course Requirements",
    "text": "Course Requirements\n\n\n\nAssignment\nPercentage of Grade\n\n\n\n\nParticipation/Attendance\n5%\n\n\nCoding Discussion\n5%\n\n\nProblem sets\n50%\n\n\nFinal Project\n40%\n\n\n\nParticipation and Attendance (5%):\nData science is an cooperative endeavor, and it’s crucial to view your fellow classmates as valuable assets rather than rivals. Your performance in the following aspects will be considered when assessing this part of your grade:\n\nActive involvement during class sessions, fostering a dynamic learning environment.\nContributions made to your group’s ultimate project.\nAssisting classmates by addressing problem set queries through GitHub issues. Supporting your peers will enhance your evaluation in terms of teamwork and engagement\nAssisting classmates with slack questions, sharing interesting materials on slack, asking question, and anything that provides healthy contributtions to the course.\n\nCoding Discussion(5%)\nEvery class will involve some lecture time, and some coding time. The coding time will be divided between me showing you things, and you working on small problem sets. These problem sets are purposefully constructed to help you understand the concepts we go through in class. You participation and involvement in these group exercises will also be part of your grade.\nProblem Sets (50%)\nStudents will be assigned five problem sets over the course of the semesters. While you are encouraged to discuss the problem sets with your peers and/or consult online resources, the finished product must be your own work. The goal of the assignment is to reinforce the student’s comprehension of the materials covered in each section.\nThe problems sets will assess your ability to apply the concepts to data that is substantially messier, and problems that are substantially more difficult, than the ones in the coding discussion in class.\nI will distribute the assignment through a mix of canvas and github. The assignments can be in the form of a Jupyter Notebook (.ipynb). Students must submit completed assignments as a rendered .html file and the corresponding source code (.ipynb).\nThe assignments will be graded in accuracy and quality of the programming style. For instance, our grading team will be looking at:\n\n\nall code must run;\n\n\nsolutions should be readable\n\n\nCode should be thoroughly commented (the Professor/TA should be able to understand the codes purpose by reading the comment),\nCoding solutions should be broken up into individual code chunks in Jupyter/R Markdown notebooks, not clumped together into one large code chunk (See examples in class or reach out to the TA/Professor if this is unclear),\nEach student defined function must contain a doc string explaining what the function does, each input argument, and what the function returns;\n\n\nCommentary, responses, and/or solutions should all be written in Markdown and explain sufficiently the outpus.\n\n\nAll solutions must be completed in Python.\n\n\nThe follow schedule lays out when each assignment will be assigned.\n\n\n\nAssignment\nDate Assigned\nDate Due\n\n\n\n\nNo. 1\nWeek 2\nBefore EOD of Friday of Week 3\n\n\nNo. 2\nWeek 4\nBefore EOD of Friday of Week 5\n\n\nNo. 3\nWeek 6\nBefore EOD of Friday of Week 7\n\n\nNo. 4\nWeek 8\nBefore EOD of Friday of Week 10\n\n\nNo. 5\nWeek 10\nBefore EOD of Friday of Week 11\n\n\n\nFinal Project (40%): Data science is an applied field and the DSPP is particularly geared towards providing students the tools to make policy and substantive contributtions using data and recent computational developments. In this sense, it is fundamental that you understand how to conduct a complete analysis from collecting data, to cleaning and analyzing it, to presenting your findings. For this reason, a considerable part of your grade will come from a an independent data science project, applying concepts learned throughout the course.\nThe project is composed of three parts:\n\na 2 page project proposal: (which should be discussed and approved by me)\nan in-class presentation,\nA 10-page project report.\n\nDue dates and breakdowns for the project are as follows:\n\n\n\nRequirement\nDue\nLength\nPercentage\n\n\n\n\nProject Proposal\nNovember 15\n2 pages\n5%\n\n\nPresentation\nDecember 10\n10-15 minutes\n10%\n\n\nProject Report\nDecember 17\n10 pages\n25%\n\n\n\nImportant notes about the final project\n\nFor the project proposal, you need to schedule a 30min with me at least a week before the due date. For this meeting, I expect you to send me a draft of your ideas. We will do the group assignment and start scheduling meetings by week 4, I will share with you a calendar invite to organize our meetings.\nFor the presentation, You will have 10-15 minutes in our last class of the semester to present you project.\nTake the final project seriously. After you finish your Masters, in any path you take, you will need to show concrete examples of your portfolio. This is a good opportunity to start building it.\nYour groups will be randomly assigned.\n\nSubmission of the Final Project\nThe end product should be a github repository that contains:\n\nThe raw source data you used for the project. If the data is too large for GitHub, talk with me, and we will find a solution\nYour proposal\nA README for the repository that, for each file, describes in detail:\n\nInputs to the file: e.g., raw data; a file containing credentials needed to access an API\nWhat the file does: describe major transformations.\nOutput: if the file produces any outputs (e.g., a cleaned dataset; a figure or graph).\nA set of code files that transform that data into a form usable to answer the question you have posed in your descriptive research proposal.\nYour final 10 pages report (I will share a template later in the semester)\n\n\nOf course, no commits after the due date will be considered in the assessment."
  },
  {
    "objectID": "syllabus.html#grading",
    "href": "syllabus.html#grading",
    "title": "Syllabus: PPOL 5203 - Data Science I: Foundations",
    "section": "Grading",
    "text": "Grading\nCourse grades will be determined according to the following scale:\n\n\n\nLetter\nRange\n\n\n\n\nA\n95% – 100%\n\n\nA-\n91% – 94%\n\n\nB+\n87% – 90%\n\n\nB\n84% – 86%\n\n\nB-\n80% – 83%\n\n\nC\n70% – 79%\n\n\nF\n&lt; 70%\n\n\n\nGrades may be curved if there are no students receiving A’s on the non-curved grading scale.\nLate problem sets will be penalized a letter grade per day."
  },
  {
    "objectID": "syllabus.html#communication",
    "href": "syllabus.html#communication",
    "title": "Syllabus: PPOL 5203 - Data Science I: Foundations",
    "section": "Communication",
    "text": "Communication\n\nClass-relevant and/or coding-related questions, Slack is the preferred method of communication. Please use the general or the relevant channel for these questions.\nFor private questions concerning the class, email is the preferred method of communication. All email messages must originate from your Georgetown University email account(s). Please use a professional salutation, proper spelling and grammar, and patience in waiting for a response. The professor reserves the right to not respond to emails that are drafted inappropriately. Please email the professor and the TA directly rather than through the Canvas messaging system. Emails sent through CANVAS will be ignored.\nI will try my best to respond to all emails/slack questions within 24 hours of being sent during a weekday. I will not respond to emails/slack sent late Friday (after 5:00 pm) or during the weekend until Monday (9:00 am). Please plan accordingly if you have questions regarding current or upcoming assignments.\nOnly reach out to the professor or teaching assistant regarding a technical question, error, or issue after you made a good faith effort to debugging/isolate your problem prior to reaching out. Learning how to search for help online is a important skill for data scientists.\n\n\nChatGPT and others\nIn the last year, the internet was inundated with popularization of Large Language Models, particularly the easy use of ChatGPT. As a Data Scientist, LLMs will be part of your daily work. I see ChatGPT as Google on steroids, so I assume ChatGPT will be part of your daily work in this course, and it is part of my work as a researcher.\nThat being said, ChatGPT does not replace your training as a data scientist. If you are using ChatGPT instead of learning, I consider you are cheating in the course. And most importantly, you are wasting your time and resources. So that’s our policy for using LLMs models in class:\n\nDo not copy the responses from chatgpt – a lot of them are wrong or will just not run on your computer.\nUse chatgpt as a auxiliary source.\nIf your entire homework comes straight from chatgpt, I will consider it plagiarism.\n\nIf you use chatgpt, I ask you to mention on your code how chatgpt worked for you."
  },
  {
    "objectID": "syllabus.html#electronic-devices",
    "href": "syllabus.html#electronic-devices",
    "title": "Syllabus: PPOL 5203 - Data Science I: Foundations",
    "section": "Electronic Devices",
    "text": "Electronic Devices\nWhen meeting in-person: the use of laptops, tablets, or other mobile devices is permitted only for class-related work. Audio and video recording is not allowed unless prior approval is given by the professor. Please mute all electronic devices during class."
  },
  {
    "objectID": "syllabus.html#georgetown-policies",
    "href": "syllabus.html#georgetown-policies",
    "title": "Syllabus: PPOL 5203 - Data Science I: Foundations",
    "section": "Georgetown Policies",
    "text": "Georgetown Policies\n\nDisability\nIf you believe you have a disability, then you should contact the Academic Resource Center (arc@georgetown.edu) for further information. The Center is located in the Leavey Center, Suite 335 (202-687-8354). The Academic Resource Center is the campus office responsible for reviewing documentation provided by students with disabilities and for determining reasonable accommodations in accordance with the Americans with Disabilities Act (ASA) and University policies. For more information, go to http://academicsupport.georgetown.edu/disability/\n\n\nImportant Academic Policies and Academic Integrity\nMcCourt School students are expected to uphold the academic policies set forth by Georgetown University and the Graduate School of Arts and Sciences. Students should therefore familiarize themselves with all the rules, regulations, and procedures relevant to their pursuit of a Graduate School degree. The policies are located at: http://grad.georgetown.edu/academics/policies/\nApplied to this course, while I encourage collaboration on assignments and use of resources like StackOverflow, the problem sets will ask you to list who you worked on the problem set with and cite StackOverflow if it is the direct source of a code snippet.\n\n\nStatement on Sexual Misconduct\nGeorgetown University and its faculty are committed to supporting survivors and those impacted by sexual misconduct, which includes sexual assault, sexual harassment, relationship violence, and stalking. Georgetown requires faculty members, unless otherwise designated as confidential, to report all disclosures of sexual misconduct to the University Title IX Coordinator or a Deputy Title IX Coordinator. If you disclose an incident of sexual misconduct to a professor in or outside of the classroom (with the exception of disclosures in papers), that faculty member must report the incident to the Title IX Coordinator, or Deputy Title IX Coordinator. The coordinator will, in turn, reach out to the student to provide support, resources, and the option to meet. [Please note that the student is not required to meet with the Title IX coordinator.]. More information about reporting options and resources can be found on the Sexual Misconduct\nWebsite: https://sexualassault.georgetown.edu/resourcecenter\nIf you would prefer to speak to someone confidentially, Georgetown has a number of fully confidential professional resources that can provide support and assistance. These resources include: Health Education Services for Sexual Assault Response and Prevention: confidential email: sarp[at]georgetown.edu\nCounseling and Psychiatric Services (CAPS): 202.687.6985 or after hours, call (833) 960-3006 to reach Fonemed, a telehealth service; individuals may ask for the on-call CAPS clinician\nMore information about reporting options and resources can be found on the Sexual Misconduct Website.\n\n\nProvost’s Policy on Religious Observances\nGeorgetown University promotes respect for all religions. Any student who is unable to attend classes or to participate in any examination, presentation, or assignment on a given day because of the observance of a major religious holiday or related travel shall be excused and provided with the opportunity to make up, without unreasonable burden, any work that has been missed for this reason and shall not in any other way be penalized for the absence or rescheduled work. Students will remain responsible for all assigned work. Students should notify professors in writing at the beginning of the semester of religious observances that conflict with their classes. The Office of the Provost, in consultation with Campus Ministry and the Registrar, will publish, before classes begin for a given term, a list of major religious holidays likely to affect Georgetown students. The Provost and the Main Campus Executive Faculty encourage faculty to accommodate students whose bona fide religious observances in other ways impede normal participation in a course. Students who cannot be accommodated should discuss the matter with an advising dean."
  },
  {
    "objectID": "syllabus.html#instructors",
    "href": "syllabus.html#instructors",
    "title": "Advanced Text-As-Data: Word Embeddings, Deep Learning, and Large Language Models",
    "section": "Instructors",
    "text": "Instructors\nTiago Ventura\n\nAssistant Professor in Computational Social Science, Georgetown University\nPronouns: He/Him\nEmail: tv186@georgetown.edu\n\nSebastian Vallejo\n\nAssistant Professor in the Department of Political Science at the University of Western Ontario\nPronouns: He/Him\nEmail: sebastian.vallejo@uwo.ca"
  },
  {
    "objectID": "syllabus.html#weekly-schedule-readings",
    "href": "syllabus.html#weekly-schedule-readings",
    "title": "Advanced Text-As-Data: Word Embeddings, Deep Learning, and Large Language Models",
    "section": "Weekly Schedule & Readings",
    "text": "Weekly Schedule & Readings\n\nDay 1: Text Representation: Sparse & Dense Vectors. Deep Learning Models for Text Analysis\n\nReadings:\n\nSLP: Chapters 7\nLin, Gechun, and Christopher Lucas. “An Introduction to Neural Networks for the Social Sciences.” (2023)\n\n\n\n\nDay 2: Word Embeddings: Theory and Applications\n\nTheory Papers:\n\nSLP Chapter 6, Vector Semantics and Embeddings\nMeyer, David. How Exactly Does Word2Vec Work?\nSpirling and Rodriguez, Word embeddings: What works, what doesn’t, and how to tell the difference for applied research\n\nApplied Papers:\n\nAustin C. Kozlowski, Austin C., Matt Taddy, and James A. Evans. 2019. “The Geometry of Culture: Analyzing the Meanings of Class through Word Embeddings.” American Sociological Review 84, no. 5: 905–49. https://doi.org/10.1177/0003122419877135\nGarg, Nikhil, Londa Schiebinger, Dan Jurafsky and James Zou. 2018. “Word embeddings quantify 100 years of gender and ethnic stereotypes.” Proceedings of the National Academy of Sciences 115(16):E3635–E3644.\nRodman, E., 2020. A Timely Intervention: Tracking the Changing Meanings of Political Concepts with Word Vectors. Political Analysis, 28(1), pp.87-111\nRheault, Ludovic, and Christopher Cochrane. “Word embeddings for the analysis of ideological placement in parliamentary corpora.” Political Analysis 28, no. 1 (2020): 112-133.\nGennaro, Gloria, and Elliott Ash. “Emotion and reason in political language.” The Economic Journal 132, no. 643 (2022): 1037-1059.\n\n\n\n\nWeek 3: Transformes: Theory and Fine-tuning a Transformers-based model\nTheory Papers\n\nSLP: Chapter 10.\nJay Alammar. 2018. “The Illustrated Transformer.” https://jalammar.github.io/illustratedtransformer/\nVaswani, A., et al. (2017). Attention is all you need. Advances in neural information processing systems, 30;\nTimoneda and Vallejo Vera (2025). BERT, RoBERTa or DeBERTa? Comparing Performance Across Transformer Models in Political Science Text. Journal of Politics.\n\nApplied Papers\n\nVallejo Vera et al. (2025). Semi-Supervised Classification of Overt and Covert Racism in Text. Working Paper.\n\n\n\nWeek 4: Large Language Models: Social Science Applications\n\nApplied Papers\n\nWu, Patrick Y., Joshua A. Tucker, Jonathan Nagler, and Solomon Messing. “Large language models can be used to estimate the ideologies of politicians in a zero-shot learning setting.” arXiv preprint arXiv:2303.12057 (2023).\nRathje, Steve, Dan-Mircea Mirea, Ilia Sucholutsky, Raja Marjieh, Claire Robertson, and Jay J. Van Bavel. GPT is an effective tool for multilingual psychological text analysis. (2023).\nDavidson, Thomas: Start Generating: Harnessing Generative Artificial Intelligence for Sociological Research, PrePrint\nBisbee, James, Joshua Clinton, Cassy Dorff, Brenton Kenkel, and Jennifer Larson. 2023. “Synthetic Replacements for Human Survey Data? the Perils of Large Language Models.” SocArXiv. May 4.\nArgyle, L.P., Busby, E.C., Fulda, N., Gubler, J.R., Rytting, C. and Wingate, D., 2023. Out of one, many: Using language models to simulate human samples. Political Analysis, 31(3), pp.337-351.\nWalker, C., & Timoneda, J. C. (2024). Identifying the sources of ideological bias in GPT models through linguistic variation in output. arXiv preprint arXiv:2409.06043."
  },
  {
    "objectID": "slides/day_1.html#logistics",
    "href": "slides/day_1.html#logistics",
    "title": "Advanced Text-As-Data  Winter School - Iesp UERJ",
    "section": "Logistics",
    "text": "Logistics\n\nAll materials will be here: https://tiagoventura.github.io/tad_iesp_workshop/\nSyllabus is more dense than we can cover in a week. Take your time to work through it!\nOur time: Lectures in the afternoon, and code/exercises for you to work through at your time.\nThis is an advanced class in text-as-data. It requires:\n\nsome background in programming, mainly Python\nsome notion of probability theory, particularly maximum likelihood estimation (LEGO III at IESP)\n\nYou have a TA: Felipe Lamarca. Use him for your questions!\nThe classes are English, but feel free to ask questions in Portuguese if you prefer!"
  },
  {
    "objectID": "slides/day_1.html#rise-of-the-digital-information-age",
    "href": "slides/day_1.html#rise-of-the-digital-information-age",
    "title": "Advanced Text-As-Data - Winter School - Iesp UERJ",
    "section": "Rise of the digital information age",
    "text": "Rise of the digital information age"
  },
  {
    "objectID": "slides/day_1.html#official-documents-congressional-speeches-bills-press-releases-transcripts-from-all-over-the-world",
    "href": "slides/day_1.html#official-documents-congressional-speeches-bills-press-releases-transcripts-from-all-over-the-world",
    "title": "Advanced Text-As-Data - Winter School - Iesp UERJ",
    "section": "Official Documents: Congressional Speeches, Bills, Press Releases, Transcripts, from all over the world!!",
    "text": "Official Documents: Congressional Speeches, Bills, Press Releases, Transcripts, from all over the world!!"
  },
  {
    "objectID": "slides/day_1.html#social-media",
    "href": "slides/day_1.html#social-media",
    "title": "Advanced Text-As-Data - Winter School - Iesp UERJ",
    "section": "Social Media",
    "text": "Social Media\n\n\nError in knitr::include_graphics(\"week1_figs/redes.png\"): Cannot find the file(s): \"week1_figs/redes.png\""
  },
  {
    "objectID": "slides/day_1.html#the-internet-news-comments-blogs-etc",
    "href": "slides/day_1.html#the-internet-news-comments-blogs-etc",
    "title": "Advanced Text-As-Data - Winter School - Iesp UERJ",
    "section": "The internet: News, Comments, Blogs, etc…",
    "text": "The internet: News, Comments, Blogs, etc…"
  },
  {
    "objectID": "slides/day_1.html#what-is-this-class-about",
    "href": "slides/day_1.html#what-is-this-class-about",
    "title": "Advanced Text-As-Data  Winter School - Iesp UERJ",
    "section": "What is this class about?",
    "text": "What is this class about?\n\nFor many years, social scientists uses text in their empirical analysis:\nClose reading of documents.\nQualitative Analysis of interviews\nContent Analysis\nDigital Revolution:\n\nProduction of textual information increased with the internet.\nThe capacity to store, access and share this large volume of data also increased.\nAt the same time, the cost of accessing large computing power reduced… think about your laptops…\nAnd even powerful computational models (that do not fit in your laptop) became easily accessible.\n\nThis class covers methods (and many applications) of using textual data and advanced computational linguistics model to answer social science problems\nBut… with an modern flavor.\n\nDeep Learning Revolution / Large Language Models / Representation Learning."
  },
  {
    "objectID": "slides/day_1.html#vector-space-model",
    "href": "slides/day_1.html#vector-space-model",
    "title": "Advanced Text-As-Data  Winter School - Iesp UERJ",
    "section": "Vector Space Model",
    "text": "Vector Space Model\nTo represent documents as numbers, we will use the vector space model representation:\n\nA document \\(D_i\\) is represented as a collection of features \\(W\\) (words, tokens, n-grams..)\nEach feature \\(w_i\\) can be place in a real line, then a document \\(D_i\\) is a point in a \\(W\\) dimensional space\n\n\nImagine the sentence below: “If that is a joke, I love it. If not, can’t wait to unpack that with you later.”\n\nSorted Vocabulary =(a, can’t, i, if, is, it, joke, later, love, not, that, to, unpack, wait, with, you”)\nFeature Representation = (1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1)\nFeatures will typically be the n-gram (mostly unigram) frequencies of the tokens in the document, or some function of those frequencies\n\n\n\nNow each document is now a vector (vector space model)\n\nstacking these vectors will give you our workhose representation for text: Document Feature Matrix"
  },
  {
    "objectID": "slides/day_1.html#visualizing-vector-space-model",
    "href": "slides/day_1.html#visualizing-vector-space-model",
    "title": "Advanced Text-As-Data  Winter School - Iesp UERJ",
    "section": "Visualizing Vector Space Model",
    "text": "Visualizing Vector Space Model\n\n\n\nDocuments\n\n\nDocument 1 = “yes yes yes no no no”\nDocument 2 = “yes yes yes yes yes yes”"
  },
  {
    "objectID": "slides/day_1.html#visualizing-vector-space-model-1",
    "href": "slides/day_1.html#visualizing-vector-space-model-1",
    "title": "Advanced Text-As-Data  Winter School - Iesp UERJ",
    "section": "Visualizing Vector Space Model",
    "text": "Visualizing Vector Space Model\nIn the vector space, we can use geometry to build well-defined comparison measures between the documents"
  },
  {
    "objectID": "slides/day_1.html#document-feature-matrix",
    "href": "slides/day_1.html#document-feature-matrix",
    "title": "Advanced Text-As-Data - Winter School - Iesp UERJ",
    "section": "6. Document-Feature Matrix",
    "text": "6. Document-Feature Matrix\n\n\n\n\n\n\n\n\n\n\n\nSource: Arthur Spirling TAD Class"
  },
  {
    "objectID": "slides/day_1.html#vector-space-model-vs-representation-learning",
    "href": "slides/day_1.html#vector-space-model-vs-representation-learning",
    "title": "Advanced Text-As-Data  Winter School - Iesp UERJ",
    "section": "Vector Space Model vs Representation Learning",
    "text": "Vector Space Model vs Representation Learning\nThe vector space model is super useful, and has been used in many many many applications in computational linguistics and social science applications of text-as-data. Including:\n\nDescriptive statistics of documents (count words, text-similarity, complexity, etc..)\nSupervised Machine Learning Models for Text Classification (DFM becomes the input of the models)\nUnsupervised Machine Learning (Topic Models & Clustering)\n\n\nBut… Embedded in this model, there is the idea we represent words as a one-hot encoding.\n\n“cat”: [0,0, 0, 0, 0, 0, 1, 0, ….., V] , on a V dimensional vector\n“dog”: [0,0, 0, 0, 0, 0, 0, 1, …., V], on a V dimensional vector\n\n\n\nWhat these vectors look like?\n\nreally sparse\nthose vectors are orthogonal\nno natural notion of similarity"
  },
  {
    "objectID": "slides/day_1.html#distributional-semantics",
    "href": "slides/day_1.html#distributional-semantics",
    "title": "Advanced Text-As-Data  Winter School - Iesp UERJ",
    "section": "Distributional Semantics",
    "text": "Distributional Semantics\n\n“you shall know a word by the company it keeps.” J. R. Firth 1957\n\nDistributional semantics: words that are used in the same contexts tend to be similar in their meaning.\n\n\nHow can we use this insight to build a word representation?\n\nMove from sparse representation to dense representation\nRepresent words as vectors of numbers with high number of dimensions\nEach feature on this vectors embeds some information from the word (gender? noun? sentiment? stance?)\nLearn this representation from the unlabeled data."
  },
  {
    "objectID": "slides/day_1.html#sparse-vs-dense-vectors",
    "href": "slides/day_1.html#sparse-vs-dense-vectors",
    "title": "Advanced Text-As-Data  Winter School - Iesp UERJ",
    "section": "Sparse vs Dense Vectors",
    "text": "Sparse vs Dense Vectors\n\nOne-hot encoding / Sparse Representation:\n\ncat = \\(\\begin{bmatrix} 0,0, 0, 0, 0, 0, 1, 0, 0 \\end{bmatrix}\\)\ndog = \\(\\begin{bmatrix} 0,0, 0, 0, 0, 1, 0, 0, 0 \\end{bmatrix}\\)\n\nWord Embedding / Dense Representation:\n\ncat = \\(\\begin{bmatrix} 0.25, -0.75, 0.90, 0.12, -0.50, 0.33, 0.66, -0.88, 0.10, -0.45 \\end{bmatrix}\\)\ndog = \\(\\begin{bmatrix} 0.25, 1.75, 0.90, 0.12, -0.50, 0.33, 0.66, -0.88, 0.10, -0.45 \\end{bmatrix}\\)"
  },
  {
    "objectID": "slides/day_1.html#with-colors-and-real-word-vectors",
    "href": "slides/day_1.html#with-colors-and-real-word-vectors",
    "title": "Advanced Text-As-Data  Winter School - Iesp UERJ",
    "section": "With colors and real word vectors",
    "text": "With colors and real word vectors\n\nSource: Illustrated Word2Vec"
  },
  {
    "objectID": "slides/day_1.html#word-embeddings",
    "href": "slides/day_1.html#word-embeddings",
    "title": "Advanced Text-As-Data  Winter School - Iesp UERJ",
    "section": "Word Embeddings",
    "text": "Word Embeddings\n\nEncoding similarity: vectors are not ortogonal anymore!\nAutomatic Generalization: learn about one word allow us to automatically learn about related words\nEncodes Meaning: by learning the context, I can learn what a word means.\nAs a consequence:\n\nWord Embeddings improves by ORDERS OF MAGNITUDE several Text-as-Data Tasks.\nAllows to deal with unseen words.\nForm the core idea of state-of-the-art models, such as LLMs."
  },
  {
    "objectID": "slides/day_1.html#deep-learning-for-text-analysis",
    "href": "slides/day_1.html#deep-learning-for-text-analysis",
    "title": "Advanced Text-As-Data  Winter School - Iesp UERJ",
    "section": "Deep Learning for Text Analysis",
    "text": "Deep Learning for Text Analysis"
  },
  {
    "objectID": "slides/day_1.html#basics-of-machine-learning",
    "href": "slides/day_1.html#basics-of-machine-learning",
    "title": "Advanced Text-As-Data  Winter School - Iesp UERJ",
    "section": "Basics of Machine Learning",
    "text": "Basics of Machine Learning\nDeep Learning is a subfield of machine learning based on using neural networks models to learn.\nAs in any other statistical model, the goal of machine learning is to use data to learn about some output.\n\\[ y = f(X) + \\epsilon\\]\n\nWhere:\n\\(y\\) is the outcome/dependent/response variable\n\\(X\\) is a matrix of predictors/features/independent variables\n\\(f()\\) is some fixed but unknown function mapping X to y. The “signal” in the data\n\\(\\epsilon\\) is some random error term. The “noise” in the data."
  },
  {
    "objectID": "slides/day_1.html#linear-models-ols",
    "href": "slides/day_1.html#linear-models-ols",
    "title": "Advanced Text-As-Data  Winter School - Iesp UERJ",
    "section": "Linear models (OLS)",
    "text": "Linear models (OLS)\nThe simplest model we can use is an linear model (the classic OLS regression)\n\\[ y = b_0 + WX + \\epsilon\\]\n\nWhere:\n\n\\(W\\) is a vector of dimension p,\n\\(X\\) is the feature vector of dimension p\n\\(b\\) is a bias term (intercept)"
  },
  {
    "objectID": "slides/day_1.html#using-matrix-algebra",
    "href": "slides/day_1.html#using-matrix-algebra",
    "title": "Advanced Text-As-Data  Winter School - Iesp UERJ",
    "section": "Using Matrix Algebra",
    "text": "Using Matrix Algebra\n\\[\\mathbf{W} = \\begin{bmatrix} w_1 & w_2 & \\dots & w_p\\end{bmatrix}\\]\n\\[\\mathbf{X} = \\begin{bmatrix} X_1 \\\\ X_2 \\\\ X_3 \\\\ \\vdots \\\\ X_p \\end{bmatrix}\\]\nWith matrix multiplication:\n\\[\\mathbf{W} \\mathbf{X} + b = w_1 X_1 + w_2 X_2 + \\dots + w_p X_p + b\\]"
  },
  {
    "objectID": "slides/day_1.html#logistic-regression",
    "href": "slides/day_1.html#logistic-regression",
    "title": "Advanced Text-As-Data  Winter School - Iesp UERJ",
    "section": "Logistic Regression",
    "text": "Logistic Regression\nIf we want to model some type of non-linearity, necessary for example when our outcome is binary, we can add a transformation function to make things non-linear:\n\\[ y = \\sigma (b_0 + WX + \\epsilon)\\]\nWhere:\n\\[ \\sigma(b_0 + WX + \\epsilon) = \\frac{1}{1 + \\exp(-b_0 + WX + \\epsilon)}\\]"
  },
  {
    "objectID": "slides/day_1.html#seeing-this-as-a-neural-network-representation",
    "href": "slides/day_1.html#seeing-this-as-a-neural-network-representation",
    "title": "Advanced Text-As-Data - Winter School - Iesp UERJ",
    "section": "Seeing this as a neural network representation",
    "text": "Seeing this as a neural network representation\nLet’s assume we have a simple model of voting. We want to predict if individual \\(i\\) will vote (\\(y=1\\)), and we will use four socio demographic factors to make this prediction.\nClassic statistical approach with logistic regression:\n\\[ \\hat{Y_i} = \\sigma(b_0 + WX + \\epsilon) \\] We use MLE (Maximum Likelihood estimation) to find the parameters \\(W\\) and \\(b_0\\)"
  },
  {
    "objectID": "slides/day_1.html#neural-network-graphical-representation",
    "href": "slides/day_1.html#neural-network-graphical-representation",
    "title": "Advanced Text-As-Data  Winter School - Iesp UERJ",
    "section": "Neural Network Graphical Representation",
    "text": "Neural Network Graphical Representation\n\n\n\n\n\n\n\nSource: https://carpentries-incubator.github.io/ml4bio-workshop/05-logit-ann/index.html"
  },
  {
    "objectID": "slides/day_1.html#neural-networks",
    "href": "slides/day_1.html#neural-networks",
    "title": "Advanced Text-As-Data - Winter School - Iesp UERJ",
    "section": "Neural Networks",
    "text": "Neural Networks\nA Neural Network is equivalent to stacking multiple logistic regressions vertically and repeat this process multiple times across many layers.\nAs a Matrix, instead of:\n\\[\\mathbf{W}_{previous} = \\begin{bmatrix} w_1 & w_2 & \\dots & w_p\\end{bmatrix}\\]\nWe use this set of parameters:\n\\[\n\\mathbf{W} = \\begin{bmatrix} w_{11} & w_{12} & w_{13} & \\dots & w_{1p} \\\\\n                               w_{21} & w_{22} & w_{23} & \\dots & w_{2p} \\\\\n                               w_{31} & w_{32} & w_{33} & \\dots & w_{3p} \\\\\n                               \\vdots & \\vdots & \\vdots & \\vdots & \\vdots \\\\\n                               w_{k1} & w_{k2} & w_{k3} & \\dots & w_{kp}\n                \\end{bmatrix}\n\\]"
  },
  {
    "objectID": "slides/day_1.html#section",
    "href": "slides/day_1.html#section",
    "title": "Advanced Text-As-Data - Winter School - Iesp UERJ",
    "section": "",
    "text": "Then, every line becomes a different logistic regression\n\\[\\mathbf{WX} = \\begin{bmatrix}  \\sigma(w_{11} X_1 + w_{12} X_2 + \\dots + w_{1p}X_p) \\\\ \\sigma(w_{21} X_1 + w_{22} X_2 + \\dots + w_{2p}X_p) \\\\ \\sigma(w_{31} X_1 + w_{32} X_2 + \\dots + w_{3p}X_p )\\\\ \\vdots \\\\ \\sigma(w_{k1} X_1 + w_{k2} X_2 + \\dots + w_{kp}X_p) \\end{bmatrix}\\] We then combine all of those with another set of parameters:\n\\[\n\\begin{align*}\n    \\mathbf{HA} &= h1 \\cdot \\sigma(w_{11} X_1 + w_{12} X_2 + \\dots + w_{1p}X_p)\\\\\n                &+ h_2\\cdot \\sigma(w_{21} X_1 + w_{22} X_2 + \\dots + w_{2p}X_p)\\\\\n                &+ h_3 \\cdot \\sigma (w_{31} X_1 + w_{32} X_2 + \\dots + w_{3p}X_p)\\\\\n                &+ \\dots + h_k + \\sigma(w_{k1} X_1 + w_{k2} X_2 + \\dots + w_{kp}X_p)\n\\end{align*}\n\\]"
  },
  {
    "objectID": "slides/day_1.html#feed-forward-on-a-deep-neural-network",
    "href": "slides/day_1.html#feed-forward-on-a-deep-neural-network",
    "title": "Advanced Text-As-Data  Winter School - Iesp UERJ",
    "section": "Feed Forward on a Deep Neural Network",
    "text": "Feed Forward on a Deep Neural Network"
  },
  {
    "objectID": "slides/day_1.html#key-components",
    "href": "slides/day_1.html#key-components",
    "title": "Advanced Text-As-Data  Winter School - Iesp UERJ",
    "section": "Key Components",
    "text": "Key Components\n\nInput: p features (the original data) of an observation are linearly transformed into k1 features using a weight matrix of size k1×p\nEmbedding Matrices: Paremeters you multiply your data by.\nNeurons: Number of dimensions on your embedding matrices\nHidden Layer: the transformation that consists of the linear transformation and an activation function\nOutput Layer: the transformation that consists of the linear transformation and then (usually) a sigmoid (or some other activation function) to produce the final output predictions"
  },
  {
    "objectID": "slides/day_1.html#section-1",
    "href": "slides/day_1.html#section-1",
    "title": "Advanced Text-As-Data  Winter School - Iesp UERJ",
    "section": "",
    "text": "Then, every line becomes a different logistic regression\n\\[\\mathbf{WX} = \\begin{bmatrix}  \\sigma(w_{11} X_1 + w_{12} X_2 + \\dots + w_{1p}X_p) \\\\ \\sigma(w_{21} X_1 + w_{22} X_2 + \\dots + w_{2p}X_p) \\\\ \\sigma(w_{31} X_1 + w_{32} X_2 + \\dots + w_{3p}X_p )\\\\ \\vdots \\\\ \\sigma(w_{k1} X_1 + w_{k2} X_2 + \\dots + w_{kp}X_p) \\end{bmatrix}\\] We then combine all of those with another set of parameters:\n\\[\n\\begin{align*}\n    \\mathbf{HA} &= h1 \\cdot \\sigma(w_{11} X_1 + w_{12} X_2 + \\dots + w_{1p}X_p)\\\\\n                &+ h_2\\cdot \\sigma(w_{21} X_1 + w_{22} X_2 + \\dots + w_{2p}X_p)\\\\\n                &+ h_3 \\cdot \\sigma (w_{31} X_1 + w_{32} X_2 + \\dots + w_{3p}X_p)\\\\\n                &+ \\dots + h_k + \\sigma(w_{k1} X_1 + w_{k2} X_2 + \\dots + w_{kp}X_p)\n\\end{align*}\n\\]"
  },
  {
    "objectID": "slides/day_1.html#logistic-regression-as-a-neural-network",
    "href": "slides/day_1.html#logistic-regression-as-a-neural-network",
    "title": "Advanced Text-As-Data  Winter School - Iesp UERJ",
    "section": "Logistic Regression as a Neural Network",
    "text": "Logistic Regression as a Neural Network\nAssume we have a simple model of voting. We want to predict if individual \\(i\\) will vote (\\(y=1\\)), and we will use four socio demographic factors to make this prediction.\nClassic statistical approach with logistic regression:\n\\[ \\hat{P(Y_i=1|X)} = \\sigma(b_0 + WX + \\epsilon) \\] We use MLE (Maximum Likelihood estimation) to find the parameters \\(W\\) and \\(b_0\\). We assume:\n\\[ Y_i \\sim \\text{Bernoulli}(\\pi_i) \\]\nThe likelihood function for \\(n\\) independent observations is:\n\\[L(W, b_0) = \\prod_{i=1}^{n} \\pi_i^{y_i} (1 - \\pi_i)^{1 - y_i}.\\]"
  },
  {
    "objectID": "slides/day_1.html#lets-take-a-step-back",
    "href": "slides/day_1.html#lets-take-a-step-back",
    "title": "Advanced Text-As-Data  Winter School - Iesp UERJ",
    "section": "Let’s take a step back!",
    "text": "Let’s take a step back!\nWeights matrices are just parameters… the \\(\\beta_s\\) of our regression models.\n\nThere are too MANY of them!! Neural Networks are a black box!\nBut… they also serve as way to project covariates (or words!) in a dense dimensional space.\nRemember:\n\nOne-hot encoding / Sparse Representation:\n\ncat = \\(\\begin{bmatrix} 0,0, 0, 0, 0, 0, 1, 0, 0 \\end{bmatrix}\\)\ndog = \\(\\begin{bmatrix} 0,0, 0, 0, 0, 1, 0, 0, 0 \\end{bmatrix}\\)\n\nWord Embedding / Dense Representation:\n\ncat = \\(\\begin{bmatrix} 0.25, -0.75, 0.90, 0.12, -0.50, 0.33, 0.66, -0.88, 0.10, -0.45 \\end{bmatrix}\\)\ndog = \\(\\begin{bmatrix} 0.25, 1.75, 0.90, 0.12, -0.50, 0.33, 0.66, -0.88, 0.10, -0.45 \\end{bmatrix}\\)"
  },
  {
    "objectID": "slides/day_1.html#deep-neural-network-for-textual-data",
    "href": "slides/day_1.html#deep-neural-network-for-textual-data",
    "title": "Advanced Text-As-Data  Winter School - Iesp UERJ",
    "section": "Deep Neural Network for Textual Data",
    "text": "Deep Neural Network for Textual Data"
  },
  {
    "objectID": "slides/day_1.html#estimation-gradient-descent",
    "href": "slides/day_1.html#estimation-gradient-descent",
    "title": "Advanced Text-As-Data - Winter School - Iesp UERJ",
    "section": "Estimation: Gradient Descent",
    "text": "Estimation: Gradient Descent\nLinear Regression\n\\[\\text{RSS} = \\sum_{i=1}^{n} (y_i - \\widehat{y}_i)^2 \\]\nLogistic Regression\n\\[L(\\beta) = -\\sum_{i=1}^{n} \\left( y_i \\log(\\widehat{y}_i) + (1-y_i)\\log(1 - \\widehat{y}_i) \\right)\\]\nFully-Connected Neural Network (Classification, Binary)\n\\[L(\\mathbf{W}) = -\\frac{1}{n}\\sum_{i=1}^{n} \\left( y_i \\log(\\widehat{y}_i) + (1-y_i)\\log(1 - \\widehat{y}_i) \\right)\\]\nThis is called a binary cross entropy loss function"
  },
  {
    "objectID": "slides/day_1.html#section-2",
    "href": "slides/day_1.html#section-2",
    "title": "Advanced Text-As-Data - Winter School - Iesp UERJ",
    "section": "",
    "text": "Error in knitr::include_graphics(\"./figs/nlp_nn.png\"): Cannot find the file(s): \"./figs/nlp_nn.png\""
  },
  {
    "objectID": "slides/day_1.html#updating-parameters",
    "href": "slides/day_1.html#updating-parameters",
    "title": "Advanced Text-As-Data  Winter School - Iesp UERJ",
    "section": "Updating parameters",
    "text": "Updating parameters\n\nDefine a loss function\nGradient Descent Algorithm:\n\nInitialize weights randomly\nFeed Forward: Matrix Multiplication + Activation function\nGet the loss for this iteration\nCompute gradient (partial derivatives): \\(\\frac{\\partial J(\\mathbf{W})}{\\partial \\mathbf{W}}\\)\nUpdate weights: \\(W_{new}: W_{old} -\\eta \\cdot \\frac{\\partial J(\\mathbf{W})}{\\partial \\mathbf{W}}\\)\nLoop until convergence:"
  },
  {
    "objectID": "syllabus_most_updated.html",
    "href": "syllabus_most_updated.html",
    "title": "Advanced Text-As-Data: Word Embeddings, Deep Learning, and Large Language Models",
    "section": "",
    "text": "In recent years, the surge in the availability of textual data, ranging from the digitalization of archival documents, political speeches, social media posts, and online news articles, has led to a growing demand for statistical analysis using large corpora. Once dominated by sparse bag-of-words models, the field of Natural Language Processing (NLP) is now increasingly driven by dense vector representations, deep learning, and the rapid evolution of Large Language Models (LLMs). This course offers an introduction to this new generation of models, serving as hands-on approach to this new landscape of computational text analysis with a focus on political science research and applications.\nThe class will cover four broad topics. We start with an overview of how to represent text as data, from a sparse representation via bag-of-words models, to a dense representation using word embeddings. We then discuss the use of deep learning models for text representation and downstream classification tasks. From here, we will discuss the foundation of the state-of-art machine learning models for text analysis: transformer models. Lastly, we will discuss several applications of Large Language Models in social science tasks.\nThe course will consist of lectures and hands-on coding in class. The lecture will be conducted in English, but students are free to ask questions in Portuguese. Students will have time in the afternoon to practice the code seen in class, and we will suggest additional coding exercises. We assume students attending this class have taken, at a minimum, an introductory course in statistics and have basic knowledge of probability distributions, calculus, hypothesis testing, and linear models. The course will use a mix of R and Python, two computational languages students should be familiar with. That being said, students should be able to follow the course even if they are just starting with any of the two programming languages."
  },
  {
    "objectID": "syllabus_most_updated.html#course-description",
    "href": "syllabus_most_updated.html#course-description",
    "title": "Advanced Text-As-Data: Word Embeddings, Deep Learning, and Large Language Models",
    "section": "",
    "text": "In recent years, the surge in the availability of textual data, ranging from the digitalization of archival documents, political speeches, social media posts, and online news articles, has led to a growing demand for statistical analysis using large corpora. Once dominated by sparse bag-of-words models, the field of Natural Language Processing (NLP) is now increasingly driven by dense vector representations, deep learning, and the rapid evolution of Large Language Models (LLMs). This course offers an introduction to this new generation of models, serving as hands-on approach to this new landscape of computational text analysis with a focus on political science research and applications.\nThe class will cover four broad topics. We start with an overview of how to represent text as data, from a sparse representation via bag-of-words models, to a dense representation using word embeddings. We then discuss the use of deep learning models for text representation and downstream classification tasks. From here, we will discuss the foundation of the state-of-art machine learning models for text analysis: transformer models. Lastly, we will discuss several applications of Large Language Models in social science tasks.\nThe course will consist of lectures and hands-on coding in class. The lecture will be conducted in English, but students are free to ask questions in Portuguese. Students will have time in the afternoon to practice the code seen in class, and we will suggest additional coding exercises. We assume students attending this class have taken, at a minimum, an introductory course in statistics and have basic knowledge of probability distributions, calculus, hypothesis testing, and linear models. The course will use a mix of R and Python, two computational languages students should be familiar with. That being said, students should be able to follow the course even if they are just starting with any of the two programming languages."
  },
  {
    "objectID": "syllabus_most_updated.html#instructors",
    "href": "syllabus_most_updated.html#instructors",
    "title": "Advanced Text-As-Data: Word Embeddings, Deep Learning, and Large Language Models",
    "section": "Instructors",
    "text": "Instructors\nTiago Ventura\n\nAssistant Professor in Computational Social Science, Georgetown University\nPronouns: He/Him\nEmail: tv186@georgetown.edu\n\nSebastian Vallejo\n\nAssistant Professor in the Department of Political Science at the University of Western Ontario\nPronouns: He/Him\nEmail: sebastian.vallejo@uwo.ca"
  },
  {
    "objectID": "syllabus_most_updated.html#required-materials",
    "href": "syllabus_most_updated.html#required-materials",
    "title": "Advanced Text-As-Data: Word Embeddings, Deep Learning, and Large Language Models",
    "section": "Required Materials",
    "text": "Required Materials\nReadings: We will rely primarily on the following textbook for this course. The textbook is freely available online\n\nDaniel Jurafsky and James H. Martin. Speech and Language Processing, 3nd Edition. - [SLP]\n\nThe weekly articles are listed in the syllabus"
  },
  {
    "objectID": "syllabus_most_updated.html#weekly-schedule-readings",
    "href": "syllabus_most_updated.html#weekly-schedule-readings",
    "title": "Advanced Text-As-Data: Word Embeddings, Deep Learning, and Large Language Models",
    "section": "Weekly Schedule & Readings",
    "text": "Weekly Schedule & Readings\n\nDay 1: Text Representation: Sparse & Dense Vectors. Deep Learning Models for Text Analysis\n\nReadings:\n\nSLP: Chapters 7\nLin, Gechun, and Christopher Lucas. “An Introduction to Neural Networks for the Social Sciences.” (2023)\n\n\n\n\nDay 2: Word Embeddings: Theory and Applications\n\nTheory Papers:\n\nSLP Chapter 6, Vector Semantics and Embeddings\nMeyer, David. How Exactly Does Word2Vec Work?\nSpirling and Rodriguez, Word embeddings: What works, what doesn’t, and how to tell the difference for applied research\n\nApplied Papers:\n\nAustin C. Kozlowski, Austin C., Matt Taddy, and James A. Evans. 2019. “The Geometry of Culture: Analyzing the Meanings of Class through Word Embeddings.” American Sociological Review 84, no. 5: 905–49. https://doi.org/10.1177/0003122419877135\nGarg, Nikhil, Londa Schiebinger, Dan Jurafsky and James Zou. 2018. “Word embeddings quantify 100 years of gender and ethnic stereotypes.” Proceedings of the National Academy of Sciences 115(16):E3635–E3644.\nRodman, E., 2020. A Timely Intervention: Tracking the Changing Meanings of Political Concepts with Word Vectors. Political Analysis, 28(1), pp.87-111\nRheault, Ludovic, and Christopher Cochrane. “Word embeddings for the analysis of ideological placement in parliamentary corpora.” Political Analysis 28, no. 1 (2020): 112-133.\nGennaro, Gloria, and Elliott Ash. “Emotion and reason in political language.” The Economic Journal 132, no. 643 (2022): 1037-1059.\n\n\n\n\nDay 3: Transformes: Theory and Fine-tuning a Transformers-based model\n\nTheory Papers\n\n[SLP] - Chapter 10.\nJay Alammar. 2018. “The Illustrated Transformer.”\nVaswani, A., et al. (2017). Attention is all you need. Advances in neural information processing systems, 30.\nTimoneda and Vallejo Vera (2025). BERT, RoBERTa or DeBERTa? Comparing Performance Across Transformer Models in Political Science Text. Journal of Politics.\n\nApplied Papers\n\nVallejo Vera et al. (2025). Semi-Supervised Classification of Overt and Covert Racism in Text. Working Paper.\n\n\n\n\nDay 4: Large Language Models: Social Science Applications\n\nApplied Papers\n\nWu, Patrick Y., Joshua A. Tucker, Jonathan Nagler, and Solomon Messing. “Large language models can be used to estimate the ideologies of politicians in a zero-shot learning setting.” arXiv preprint arXiv:2303.12057 (2023).\nRathje, Steve, Dan-Mircea Mirea, Ilia Sucholutsky, Raja Marjieh, Claire Robertson, and Jay J. Van Bavel. GPT is an effective tool for multilingual psychological text analysis. (2023).\nDavidson, Thomas: Start Generating: Harnessing Generative Artificial Intelligence for Sociological Research, PrePrint\nBisbee, James, Joshua Clinton, Cassy Dorff, Brenton Kenkel, and Jennifer Larson. 2023. “Synthetic Replacements for Human Survey Data? the Perils of Large Language Models.” SocArXiv. May 4.\nArgyle, L.P., Busby, E.C., Fulda, N., Gubler, J.R., Rytting, C. and Wingate, D., 2023. Out of one, many: Using language models to simulate human samples. Political Analysis, 31(3), pp.337-351.\nWalker, C., & Timoneda, J. C. (2024). Identifying the sources of ideological bias in GPT models through linguistic variation in output. arXiv preprint arXiv:2409.06043.\nTimoneda, Joan C., and Sebastián Vallejo Vera. “Memory Is All You Need: Testing How Model Memory Affects LLM Performance in Annotation Tasks.” arXiv preprint arXiv:2503.04874 (2025)."
  },
  {
    "objectID": "materials.html",
    "href": "materials.html",
    "title": "Schedule and Course Materials",
    "section": "",
    "text": "Here you can find all materials (readings, slides, code, datasets, assignments) for the class."
  },
  {
    "objectID": "materials.html#day-1---introduction-neural-networks",
    "href": "materials.html#day-1---introduction-neural-networks",
    "title": "Schedule and Course Materials",
    "section": "Day 1 - Introduction & Neural Networks",
    "text": "Day 1 - Introduction & Neural Networks\n\nSlides\nCode:\n\nLogistic Regression as Neural Networks:  notebook \nIntroduction to Pytorch"
  },
  {
    "objectID": "materials.html#day-2---word-embeddings",
    "href": "materials.html#day-2---word-embeddings",
    "title": "Schedule and Course Materials",
    "section": "Day 2 - Word Embeddings",
    "text": "Day 2 - Word Embeddings\n\nSlides\nCode:\n\nWord Vectors with Gensim:  notebook \nEstimating Work Vectors:  notebook"
  },
  {
    "objectID": "materials.html#day-3---transformers",
    "href": "materials.html#day-3---transformers",
    "title": "Schedule and Course Materials",
    "section": "Day 3 - Transformers",
    "text": "Day 3 - Transformers\n\nSlides\nCode:"
  },
  {
    "objectID": "materials.html#day-4---large-language-models",
    "href": "materials.html#day-4---large-language-models",
    "title": "Schedule and Course Materials",
    "section": "Day 4 - Large Language Models",
    "text": "Day 4 - Large Language Models\n\nSlides\nCode:"
  },
  {
    "objectID": "slides/day_1.html",
    "href": "slides/day_1.html",
    "title": "Advanced Text-As-Data - Winter School - Iesp UERJ",
    "section": "",
    "text": "All materials will be here: https://tiagoventura.github.io/tad_iesp_workshop/\nSyllabus is more dense than we can cover in a week. Take your time to work through it!\nOur time: Lectures in the afternoon, and code/exercises for you to work through at your time.\nThis is an advanced class in text-as-data. It requires:\n\nsome background in programming, mainly Python\nsome notion of probability theory, particularly maximum likelihood estimation (LEGO III at IESP)\n\nYou have a TA: Felipe Lamarca. Use him for your questions!\nThe classes are English, but feel free to ask questions in Portuguese if you prefer!\n\n\n\n\n\nWe ask you to:\n\nKeep you cameras open\nAsk questions throughout, feel free to interrupt us.\nThat’s it!"
  },
  {
    "objectID": "slides/day_2.html",
    "href": "slides/day_2.html",
    "title": "Advanced Text-As-Data - Winter School - Iesp UERJ",
    "section": "",
    "text": "Thank you so much for responding to the survey! Here are the most important feedback I receive:\n\nmore structured walk throughs of the code\nIf you could a few resources we can look at for mathematical depth for class topic. Really enjoyed the in-depth discussion on LDA\nI hope we can have a few discussion questions along with weekly readings\nIncluding discussion of papers that are from non-polisci applications? For example, econ, sociology, psychology, etc.?\n\nStop Doing:\n\nlong alone time with code\nSometimes we spend a bit too long on the recap part"
  },
  {
    "objectID": "slides/day_2.html#survey-responses",
    "href": "slides/day_2.html#survey-responses",
    "title": "Advanced Text-As-Data - Winter School - Iesp UERJ",
    "section": "Survey Responses",
    "text": "Survey Responses\nThank you so much for responding to the survey! Here are the most important feedback I receive:\n\nmore structured walk throughs of the code\nIf you could a few resources we can look at for mathematical depth for class topic. Really enjoyed the in-depth discussion on LDA\nI hope we can have a few discussion questions along with weekly readings\nIncluding discussion of papers that are from non-polisci applications? For example, econ, sociology, psychology, etc.?\n\nStop Doing:\n\nlong alone time with code\nSometimes we spend a bit too long on the recap part"
  },
  {
    "objectID": "slides/day_2.html#plans-for-today",
    "href": "slides/day_2.html#plans-for-today",
    "title": "Advanced Text-As-Data - Winter School - Iesp UERJ",
    "section": "Plans for Today:",
    "text": "Plans for Today:\n\nWord Embeddings\n\nSemantics, Distributional Hypothesis, Moving from Sparse to Dense Vectors\nWord2Vec Algorithm\n\nMathematical Model\nEstimate with Neural Networks\nEstimate using co-occurance matrices\n\n\nPractice\n\nWork through:\n\nEstimating Word Embeddings\nWorking with pre-trained models\n\n\nDiscuss Word Embeddings Applications"
  },
  {
    "objectID": "slides/day_2.html#vector-space-model",
    "href": "slides/day_2.html#vector-space-model",
    "title": "Advanced Text-As-Data - Winter School - Iesp UERJ",
    "section": "Vector Space Model",
    "text": "Vector Space Model\nIn the vector space model, we learned:\n\nA document \\(D_i\\) is represented as a collection of features \\(W\\) (words, tokens, n-grams..)\nEach feature \\(w_i\\) can be place in a real line, then a document \\(D_i\\) is a point in a \\(W\\) dimensional space.\n\nEmbedded in this model, there is the idea we represent words as a one-hot encoding.\n\n“cat”: [0,0, 0, 0, 0, 0, 1, 0, ….., V] , on a V dimensional vector\n“dog”: [0,0, 0, 0, 0, 0, 0, 1, …., V], on a V dimensional vector\n\nWhat these vectors look like?\n\nreally sparse\nvectors are orthogonal\nno natural notion of similarity"
  },
  {
    "objectID": "slides/day_2.html#distributional-semantics",
    "href": "slides/day_2.html#distributional-semantics",
    "title": "Advanced Text-As-Data - Winter School - Iesp UERJ",
    "section": "Distributional Semantics",
    "text": "Distributional Semantics\n\n“you shall know a word by the company it keeps.” J. R. Firth 1957\n\nDistributional semantics: words that are used in the same contexts tend to be similar in their meaning.\n\n\nHow can we use this insight to build a word representation?\n\nMove from sparse representation to dense representation\nRepresent words as vectors of numbers with high number of dimensions\nEach feature on this vectors embeds some information from the word (gender? noun? sentiment? stance?)\nLearn this representation from the unlabeled data."
  },
  {
    "objectID": "slides/day_2.html#sparse-vs-dense-vectors",
    "href": "slides/day_2.html#sparse-vs-dense-vectors",
    "title": "Advanced Text-As-Data - Winter School - Iesp UERJ",
    "section": "Sparse vs Dense Vectors",
    "text": "Sparse vs Dense Vectors\nOne-hot encoding / Sparse Representation:\n\ncat = \\(\\begin{bmatrix} 0,0, 0, 0, 0, 0, 1, 0, 0 \\end{bmatrix}\\)\ndog = \\(\\begin{bmatrix} 0,0, 0, 0, 0, 1, 0, 0, 0 \\end{bmatrix}\\)\n\nWord Embedding / Dense Representation:\n\ncat = \\(\\begin{bmatrix} 0.25, -0.75, 0.90, 0.12, -0.50, 0.33, 0.66, -0.88, 0.10, -0.45 \\end{bmatrix}\\)\ndog = \\(\\begin{bmatrix} 0.25, 1.75, 0.90, 0.12, -0.50, 0.33, 0.66, -0.88, 0.10, -0.45 \\end{bmatrix}\\)"
  },
  {
    "objectID": "slides/day_2.html#with-colors-and-real-word-vectors",
    "href": "slides/day_2.html#with-colors-and-real-word-vectors",
    "title": "Advanced Text-As-Data - Winter School - Iesp UERJ",
    "section": "With colors and real word vectors",
    "text": "With colors and real word vectors\n\nSource: Illustrated Word2Vec"
  },
  {
    "objectID": "slides/day_2.html#why-word-embeddings",
    "href": "slides/day_2.html#why-word-embeddings",
    "title": "Advanced Text-As-Data - Winter School - Iesp UERJ",
    "section": "Why Word Embeddings?",
    "text": "Why Word Embeddings?\nEncoding similarity: vectors are not ortogonal anymore!\nAutomatic Generalization: learn about one word allow us to automatically learn about related words\nEncodes Meaning: by learning the context, I can learn what a word means.\nAs a consequence:\n\nWord Embeddings improves several NLP/Text-as-Data Tasks.\nAllows to deal with unseen words.\nForm the core idea of state-of-the-art models, such as LLMs."
  },
  {
    "objectID": "slides/day_2.html#estimating-word-embeddings",
    "href": "slides/day_2.html#estimating-word-embeddings",
    "title": "Advanced Text-As-Data - Winter School - Iesp UERJ",
    "section": "Estimating Word Embeddings",
    "text": "Estimating Word Embeddings\nApproches:\n\n\nCount-based methods: look at how often words co-occur with neighbors.\n\nuse this matrix, and some some factorization to retrieve vectors for the words\nThis approach is called “GloVE” algorithm\nfast, not computationally intensive, but not the best representation\nwe will see code doing this next week\n\n\n\n\n\nNeural Networks: rely on the idea of self-supervision\n\nuse unlabeled data and use words to predict sequence\nthe famous word2vec algorithm\n\nSkipgram: predicts context words\nContinuous Bag of Words: predict center word"
  },
  {
    "objectID": "slides/day_2.html#word2vec-a-framework-for-learning-word-vectors-mikolov-et-al.-2013",
    "href": "slides/day_2.html#word2vec-a-framework-for-learning-word-vectors-mikolov-et-al.-2013",
    "title": "Advanced Text-As-Data - Winter School - Iesp UERJ",
    "section": "Word2Vec: a framework for learning word vectors (Mikolov et al. 2013)",
    "text": "Word2Vec: a framework for learning word vectors (Mikolov et al. 2013)\nCore Idea:\n\n\nWe have a large corpus (“body”) of text: a long list of words\nEvery word in a fixed vocabulary is represented by a vector\nGo through each position t in the text, which has a center word \\(c\\) and context (“outside”) words \\(t\\)\nUse the similarity of the word vectors for \\(c\\) and \\(t\\) to calculate the probability of o given c (or vice versa)\nKeep adjusting the word vectors to maximize this probability\n\nNeural Network + Gradient Descent"
  },
  {
    "objectID": "slides/day_2.html#skigram-example-self-supervision",
    "href": "slides/day_2.html#skigram-example-self-supervision",
    "title": "Advanced Text-As-Data - Winter School - Iesp UERJ",
    "section": "Skigram Example: Self-Supervision",
    "text": "Skigram Example: Self-Supervision\n\nSource: CS224N"
  },
  {
    "objectID": "slides/day_2.html#skigram-example-self-supervision-1",
    "href": "slides/day_2.html#skigram-example-self-supervision-1",
    "title": "Advanced Text-As-Data - Winter School - Iesp UERJ",
    "section": "Skigram Example: Self-Supervision",
    "text": "Skigram Example: Self-Supervision\n\nSource: CS224N"
  },
  {
    "objectID": "slides/day_2.html#encoding-similarity",
    "href": "slides/day_2.html#encoding-similarity",
    "title": "Advanced Text-As-Data - Winter School - Iesp UERJ",
    "section": "Encoding Similarity",
    "text": "Encoding Similarity\nTo estimate the model, we first need to formalize the probability function we want to estimate.\n\n\nThis is similar to a logistic regression\n\n\n\nIn logistic regression: probability of a event occur given data X and parameters \\(\\beta\\).:\n\n$ P(y=1| X, ) = X * \\(\\beta\\) $\n\\(X*\\beta\\) is not a proper probability function, so we make it to proper probability by using a logit transformation.\n\\(P(y=1|X, \\beta ) = \\frac{exp(XB)}{1 + exp(XB)}\\)\nThrow this transformation inside of a bernouilli distribution, get the likelihood function, and find the parameters using MLE."
  },
  {
    "objectID": "slides/day_2.html#pametrizing-pw_tw_t-1",
    "href": "slides/day_2.html#pametrizing-pw_tw_t-1",
    "title": "Advanced Text-As-Data - Winter School - Iesp UERJ",
    "section": "Pametrizing \\(P(w_t|w_{t-1})\\)",
    "text": "Pametrizing \\(P(w_t|w_{t-1})\\)\n\n\\(P(w_t|w_{t-1})\\) must be condition on how similar these words are.\n\nExactly the same intuition behind placing documents in the vector space model.\nNow words are vectors!\n\\(P(w_t|w_{t-1}) = u_c \\cdot u_t\\)\n\n\\(u_c \\cdot u_t\\)\ndot product between vectors\nmeasures similarity using vector projection\n\\(u_c\\): center vector\n\\(u_t\\): target vectors\n\n\n\\(u_c \\cdot u_t\\) is also not a proper probability distribution: Logit on them!\n\n\\[P(w_t|w_{t-1}) = \\frac{exp(u_c \\cdot u_t)}{{\\sum_{w}^V exp(u_c*u_w)}}\\]"
  },
  {
    "objectID": "slides/day_2.html#softmax-transformation",
    "href": "slides/day_2.html#softmax-transformation",
    "title": "Advanced Text-As-Data - Winter School - Iesp UERJ",
    "section": "Softmax Transformation",
    "text": "Softmax Transformation\n\\[P(w_t|w_{t-1}) = \\frac{exp(u_c \\cdot u_t)}{{\\sum_{w}^V exp(u_c*u_w)}}\\]\n\nDot product compares similarity between vectors\nnumerator: center vs target vectors\nexponentiation makes everything positive\nDenominator: normalize over entire vocabulary to give probability distribution\nWhat is the meaning of softmax?\n\nmax: assign high values to be 1\nsoft: still assigns some probability to smaller values\ngeneralization of the logit ~ multinomial logistic function."
  },
  {
    "objectID": "slides/day_2.html#word2vec-objective-function",
    "href": "slides/day_2.html#word2vec-objective-function",
    "title": "Advanced Text-As-Data - Winter School - Iesp UERJ",
    "section": "Word2Vec: Objective Function",
    "text": "Word2Vec: Objective Function\n\nFor each position \\(t\\), predict context words within a window of fixed size \\(m\\), given center word \\(w\\).\nLikelihood Function\n\\[ L(\\theta) = \\prod_{t=1}^{T} \\prod_{\\substack{-m&lt;= j&lt;=m \\\\ j \\neq 0}}^{m} P(w_{t+j} | w_t; \\theta) \\]\n\nAssuming independence, this means you multiplying the probability of every target for every center word in your dictionary.\nThis likelihood function will change if you do skipgram with negative sampling (See SLP chapter 6)\n\n\n\n\nObjective Function: Negative log likelihood\n\\[J(\\theta) = - \\frac{1}{T}log(L(\\theta))\\]\n\nbetter to take the gradient with sums\nthe average increases the numerical stability of the gradient."
  },
  {
    "objectID": "slides/day_2.html#neural-networks-brief-overview",
    "href": "slides/day_2.html#neural-networks-brief-overview",
    "title": "Advanced Text-As-Data - Winter School - Iesp UERJ",
    "section": "Neural Networks: Brief overview",
    "text": "Neural Networks: Brief overview"
  },
  {
    "objectID": "slides/day_2.html#skipgram-architecture",
    "href": "slides/day_2.html#skipgram-architecture",
    "title": "Advanced Text-As-Data - Winter School - Iesp UERJ",
    "section": "Skipgram Architecture",
    "text": "Skipgram Architecture"
  },
  {
    "objectID": "slides/day_2.html#check-your-matrices",
    "href": "slides/day_2.html#check-your-matrices",
    "title": "Advanced Text-As-Data - Winter School - Iesp UERJ",
    "section": "Check your matrices",
    "text": "Check your matrices\n\n\n\n\n\n\n\n\n  \nPractice with a vocabulary of size 5, a embedding with 3 dimensions, and the task is to predict the next word.\n\nStep 1: v_1^5 * W_5^3\nStep 2: w_1^3 * C_3^5\nStep 3: Softmax entire vector"
  },
  {
    "objectID": "slides/day_2.html#word-embeddings-matrices",
    "href": "slides/day_2.html#word-embeddings-matrices",
    "title": "Advanced Text-As-Data - Winter School - Iesp UERJ",
    "section": "Word Embeddings Matrices",
    "text": "Word Embeddings Matrices"
  },
  {
    "objectID": "slides/day_2.html#applications",
    "href": "slides/day_2.html#applications",
    "title": "Advanced Text-As-Data - Winter School - Iesp UERJ",
    "section": "Applications:",
    "text": "Applications:\nOnce we’ve optimized, we can extract the word specific vectors from W as embedding vectors. These real valued vectors can be used for analogies and related tasks\n\nWe will see several applications next week. Most important:\n\nAlternative to bag-of-words feature representation in supervised learning tasks\nSupport for other automated text analysis tasks: expand dictionaries\nUnderstanding word meaning: variation over time, bias, variation by groups\nas a scaling method"
  },
  {
    "objectID": "slides/day_2.html#training-embeddings",
    "href": "slides/day_2.html#training-embeddings",
    "title": "Advanced Text-As-Data - Winter School - Iesp UERJ",
    "section": "Training Embeddings",
    "text": "Training Embeddings\nEmbeddings need quite a lot of text to train: e.g. want to disambiguate meanings from contexts. You can download pre-trained, or get the code and train locally\n\nWord2Vec is trained on the Google News dataset (∼ 100B words, 2013)\nGloVe are trained on different things: Wikipedia (2014) + Gigaword (6B words), Common Crawl, Twitter. And uses a co-occurence matrix instead of Neural Networks\nfastext from facebook"
  },
  {
    "objectID": "slides/day_2.html#decisions-on-embeddings-rodriguez-and-spirling-2022",
    "href": "slides/day_2.html#decisions-on-embeddings-rodriguez-and-spirling-2022",
    "title": "Advanced Text-As-Data - Winter School - Iesp UERJ",
    "section": "Decisions on embeddings, Rodriguez and Spirling, 2022",
    "text": "Decisions on embeddings, Rodriguez and Spirling, 2022\nWhen using/training embeddings, we face four key decisions:\n\nWindow size\nNumber of dimensions for the embedding matrix\nPre-trained versus locally fit variants\nWhich algorithm to use?"
  },
  {
    "objectID": "slides/day_2.html#findings",
    "href": "slides/day_2.html#findings",
    "title": "Advanced Text-As-Data - Winter School - Iesp UERJ",
    "section": "Findings",
    "text": "Findings\n\n\npopular, easily available pretrained embeddings perform at a level close to—or surpassing—both human coders andmore complicated locally fit models.\nGloVe pretrained word embeddings achieve on average—for the set of political queries—80% of human performance and are generally preferred to locally trained embeddings\nLarger window size and embeddings are often preferred."
  },
  {
    "objectID": "slides/day_3.html#evolving-methods",
    "href": "slides/day_3.html#evolving-methods",
    "title": "Advanced Text-As-Data - Winter School - Iesp UERJ",
    "section": "Evolving Methods",
    "text": "Evolving Methods\nIn the last couple of days we have learned about the different goals of NLP: contextualize content, find categories in text… ultimately, measure some latent characteristic of a set of texts.\n\nBAG-OF-WORDS: Wordfish (Slapin and Proksch, 2008).\nEMBEDDINGS: Party ideological placement, i.e., party-embeddings (Rheault and Cochrane, 2020).\nTRANSFORMERS: Supervised models for text classification (Timoneda and Vallejo Vera, 2025a)."
  },
  {
    "objectID": "slides/day_3.html#evolving-methods-i",
    "href": "slides/day_3.html#evolving-methods-i",
    "title": "Advanced Text-As-Data - Winter School - Iesp UERJ",
    "section": "Evolving Methods I",
    "text": "Evolving Methods I\nIn the last couple of days we have learned about the different goals of NLP: contextualize content, find categories in text… ultimately, measure some latent characteristic of a set of texts.\n\nBAG-OF-WORDS: Wordfish (Slapin and Proksch, 2008).\nEMBEDDINGS: Party ideological placement, i.e., party-embeddings (Rheault and Cochrane, 2020).\nTRANSFORMERS: Supervised models for text classification (Timoneda and Vallejo Vera, 2025a)."
  },
  {
    "objectID": "slides/day_3.html#evolving-methods-ii",
    "href": "slides/day_3.html#evolving-methods-ii",
    "title": "Advanced Text-As-Data - Winter School - Iesp UERJ",
    "section": "Evolving Methods II",
    "text": "Evolving Methods II\nMore recently, we’ve seen and increased focus on Generative Large Language Models (LLM):\n\nAPPLIED: latent dimensions, i.e., ideology (Kato and Cochrane, 2025; Wu et al., 2023); annotators (Timoneda and Vallejo Vera, 2025b)\nLIMITATIONS: biases from party cues (Vallejo Vera and Driggers, 2025); bias from language (Walker and Timoneda, 2024).\nMETA: effect of LLMs on respondents (coming soon); LLM adoption, e.g., in the classroom."
  },
  {
    "objectID": "slides/day_3.html#what-are-transformers",
    "href": "slides/day_3.html#what-are-transformers",
    "title": "Advanced Text-As-Data - Winter School - Iesp UERJ",
    "section": "What are Transformers?",
    "text": "What are Transformers?\n\nTransformers are a machine-learning architecture used in language-based models.\nWe can think of Transformers as the engines that drive machine-learning models and improve (significantly) their performance.\nTransformers are the architecture behind BERT, RoBERTa, DeBERTa, etc."
  },
  {
    "objectID": "slides/day_2.html#vector-space-model-from-yesterday",
    "href": "slides/day_2.html#vector-space-model-from-yesterday",
    "title": "Advanced Text-As-Data - Winter School - Iesp UERJ",
    "section": "Vector Space Model: From yesterday",
    "text": "Vector Space Model: From yesterday\nIn the vector space model, we learned:\n\nA document \\(D_i\\) is represented as a collection of features \\(W\\) (words, tokens, n-grams..)\nEach feature \\(w_i\\) can be place in a real line, then a document \\(D_i\\) is a point in a \\(W\\) dimensional space.\n\nEmbedded in this model, there is the idea we represent words as a one-hot encoding.\n\n“cat”: [0,0, 0, 0, 0, 0, 1, 0, ….., V] , on a V dimensional vector\n“dog”: [0,0, 0, 0, 0, 0, 0, 1, …., V], on a V dimensional vector\n\nWhat these vectors look like?\n\nreally sparse\nvectors are orthogonal\nno natural notion of similarity"
  },
  {
    "objectID": "slides/day_2.html#neural-networks",
    "href": "slides/day_2.html#neural-networks",
    "title": "Advanced Text-As-Data - Winter School - Iesp UERJ",
    "section": "Neural Networks",
    "text": "Neural Networks"
  },
  {
    "objectID": "slides/day_2.html#applications-1",
    "href": "slides/day_2.html#applications-1",
    "title": "Advanced Text-As-Data - Winter School - Iesp UERJ",
    "section": "Applications",
    "text": "Applications\nLet’s discuss now several applications of embeddings on social science papers. These paper show:\n\nHow to use embeddings to track semantic changes over time\nHow to use embeddings to measure emotion in political language.\nHow to use embeddings to measure gender and ethnic stereotypes\nAnd a favorite of political scientists, how to use embeddings to measure ideology."
  },
  {
    "objectID": "slides/day_2.html#capturing-cultural-dimensions-with-embeddings",
    "href": "slides/day_2.html#capturing-cultural-dimensions-with-embeddings",
    "title": "Advanced Text-As-Data - Winter School - Iesp UERJ",
    "section": "Capturing cultural dimensions with embeddings",
    "text": "Capturing cultural dimensions with embeddings\n\nAustin C. Kozlowski, Austin C., Matt Taddy, and James A. Evans. 2019. “The Geometry of Culture: Analyzing the Meanings of Class through Word Embeddings.” American Sociological Review 84, no. 5: 905–49. https://doi.org/10.1177/0003122419877135.\n\n\nWord Embeddings can be use to capture cultural dimensions\nDimensions of word embedding vector space models closely correspond to meaningful “cultural dimensions,” such as rich-poor, moral-immoral, and masculine-feminine.\na word vector’s position on these dimensions reflects the word’s respective cultural associations"
  },
  {
    "objectID": "slides/day_2.html#method",
    "href": "slides/day_2.html#method",
    "title": "Advanced Text-As-Data - Winter School - Iesp UERJ",
    "section": "Method",
    "text": "Method"
  },
  {
    "objectID": "slides/day_2.html#results",
    "href": "slides/day_2.html#results",
    "title": "Advanced Text-As-Data - Winter School - Iesp UERJ",
    "section": "Results",
    "text": "Results"
  },
  {
    "objectID": "slides/day_2.html#ideological-scaling",
    "href": "slides/day_2.html#ideological-scaling",
    "title": "Advanced Text-As-Data - Winter School - Iesp UERJ",
    "section": "Ideological Scaling",
    "text": "Ideological Scaling\n\nRheault, Ludovic, and Christopher Cochrane. “Word embeddings for the analysis of ideological placement in parliamentary corpora.” Political Analysis 28, no. 1 (2020): 112-133.\n\n\nCan word vectors be used to produce scaling estimates of ideological placement on political text?\n\nYes, and word vectors are even better\n\nIt captures semantics\nNo need of training data (self-supervision)"
  },
  {
    "objectID": "slides/day_2.html#method-1",
    "href": "slides/day_2.html#method-1",
    "title": "Advanced Text-As-Data - Winter School - Iesp UERJ",
    "section": "Method",
    "text": "Method"
  },
  {
    "objectID": "slides/day_2.html#results-1",
    "href": "slides/day_2.html#results-1",
    "title": "Advanced Text-As-Data - Winter School - Iesp UERJ",
    "section": "Results",
    "text": "Results"
  },
  {
    "objectID": "slides/day_2.html#measuring-emotion",
    "href": "slides/day_2.html#measuring-emotion",
    "title": "Advanced Text-As-Data - Winter School - Iesp UERJ",
    "section": "Measuring Emotion",
    "text": "Measuring Emotion\n\nGennaro, Gloria, and Elliott Ash. “Emotion and reason in political language.” The Economic Journal 132, no. 643 (2022): 1037-1059."
  },
  {
    "objectID": "slides/day_2.html#method-2",
    "href": "slides/day_2.html#method-2",
    "title": "Advanced Text-As-Data - Winter School - Iesp UERJ",
    "section": "Method",
    "text": "Method\n\nBuilding seed lists: They start with small seed lists of words clearly associated with “emotion” and “reason”\nExpanding dictionaries with word embeddings: Instead of just using these short lists, they expand them automatically using word embeddings.\nEmotionality Score:\n\n\\[\nY_i = \\frac{\\text{sim}(\\vec{d}_i, \\vec{A}) + b}{\\text{sim}(\\vec{d}_i, \\vec{C}) + b}\n\\] # Coding\n\n\nText-as-Data"
  },
  {
    "objectID": "materials.html#day-1---word-representation-introduction-to-neural-networks",
    "href": "materials.html#day-1---word-representation-introduction-to-neural-networks",
    "title": "Schedule and Course Materials",
    "section": "Day 1 - Word Representation & Introduction to Neural Networks",
    "text": "Day 1 - Word Representation & Introduction to Neural Networks\n\nSlides\nCode:\n\nNeural Networks From Scratch:  notebook ,  data \n\nAdditional materials:\n\nIntroduction to Pytorch"
  },
  {
    "objectID": "syllabus.html#schedule-readings",
    "href": "syllabus.html#schedule-readings",
    "title": "Advanced Text-As-Data: Word Embeddings, Deep Learning, and Large Language Models",
    "section": "Schedule & Readings",
    "text": "Schedule & Readings\n\nDay 1: Text Representation: Sparse & Dense Vectors. Deep Learning Models for Text Analysis\n\nReadings:\n\nSLP: Chapters 7\nLin, Gechun, and Christopher Lucas. “An Introduction to Neural Networks for the Social Sciences.” (2023)\n\n\n\n\nDay 2: Word Embeddings: Theory and Applications\n\nTheory Papers:\n\nSLP Chapter 6, Vector Semantics and Embeddings\nMeyer, David. How Exactly Does Word2Vec Work?\nSpirling and Rodriguez, Word embeddings: What works, what doesn’t, and how to tell the difference for applied research\n\nApplied Papers:\n\nAustin C. Kozlowski, Austin C., Matt Taddy, and James A. Evans. 2019. “The Geometry of Culture: Analyzing the Meanings of Class through Word Embeddings.” American Sociological Review 84, no. 5: 905–49. https://doi.org/10.1177/0003122419877135\nGarg, Nikhil, Londa Schiebinger, Dan Jurafsky and James Zou. 2018. “Word embeddings quantify 100 years of gender and ethnic stereotypes.” Proceedings of the National Academy of Sciences 115(16):E3635–E3644.\nRodman, E., 2020. A Timely Intervention: Tracking the Changing Meanings of Political Concepts with Word Vectors. Political Analysis, 28(1), pp.87-111\nRheault, Ludovic, and Christopher Cochrane. “Word embeddings for the analysis of ideological placement in parliamentary corpora.” Political Analysis 28, no. 1 (2020): 112-133.\nGennaro, Gloria, and Elliott Ash. “Emotion and reason in political language.” The Economic Journal 132, no. 643 (2022): 1037-1059.\n\n\n\n\nDay 3: Transformes: Theory and Fine-tuning a Transformers-based model\n\nTheory Papers\n\n[SLP] - Chapter 10.\nJay Alammar. 2018. “The Illustrated Transformer.”\nVaswani, A., et al. (2017). Attention is all you need. Advances in neural information processing systems, 30.\nTimoneda and Vallejo Vera (2025). BERT, RoBERTa or DeBERTa? Comparing Performance Across Transformer Models in Political Science Text. Journal of Politics.\n\nApplied Papers\n\nVallejo Vera et al. (2025). Semi-Supervised Classification of Overt and Covert Racism in Text. Working Paper.\n\n\n\n\nDay 4: Large Language Models: Social Science Applications\n\nApplied Papers\n\nWu, Patrick Y., Joshua A. Tucker, Jonathan Nagler, and Solomon Messing. “Large language models can be used to estimate the ideologies of politicians in a zero-shot learning setting.” arXiv preprint arXiv:2303.12057 (2023).\nRathje, Steve, Dan-Mircea Mirea, Ilia Sucholutsky, Raja Marjieh, Claire Robertson, and Jay J. Van Bavel. GPT is an effective tool for multilingual psychological text analysis. (2023).\nDavidson, Thomas: Start Generating: Harnessing Generative Artificial Intelligence for Sociological Research, PrePrint\nBisbee, James, Joshua Clinton, Cassy Dorff, Brenton Kenkel, and Jennifer Larson. 2023. “Synthetic Replacements for Human Survey Data? the Perils of Large Language Models.” SocArXiv. May 4.\nArgyle, L.P., Busby, E.C., Fulda, N., Gubler, J.R., Rytting, C. and Wingate, D., 2023. Out of one, many: Using language models to simulate human samples. Political Analysis, 31(3), pp.337-351.\nWalker, C., & Timoneda, J. C. (2024). Identifying the sources of ideological bias in GPT models through linguistic variation in output. arXiv preprint arXiv:2409.06043.\nTimoneda, Joan C., and Sebastián Vallejo Vera. “Memory Is All You Need: Testing How Model Memory Affects LLM Performance in Annotation Tasks.” arXiv preprint arXiv:2503.04874 (2025)."
  },
  {
    "objectID": "code/day2_word_vectors.html",
    "href": "code/day2_word_vectors.html",
    "title": "Word vectors",
    "section": "",
    "text": "In this notebook, we will cover:\n\nWorking with pre-trained word embeddings\nUnderstanding some applications of working embeddings\nTraining word embeddings from co-occurence matrices.\n\nThis notebook borrows from multiple sources. But most of these materials are inspired/borrowd from the Stanford CS224N: Natural Language Processing with Deep Learning"
  },
  {
    "objectID": "code/day2_word_vectors.html#introduction",
    "href": "code/day2_word_vectors.html#introduction",
    "title": "Word vectors",
    "section": "",
    "text": "In this notebook, we will cover:\n\nWorking with pre-trained word embeddings\nUnderstanding some applications of working embeddings\nTraining word embeddings from co-occurence matrices.\n\nThis notebook borrows from multiple sources. But most of these materials are inspired/borrowd from the Stanford CS224N: Natural Language Processing with Deep Learning"
  },
  {
    "objectID": "code/day2_word_vectors.html#pre-trained-word-embeddings-with-gensim",
    "href": "code/day2_word_vectors.html#pre-trained-word-embeddings-with-gensim",
    "title": "Word vectors",
    "section": "Pre-Trained Word Embeddings with Gensim",
    "text": "Pre-Trained Word Embeddings with Gensim\nFor looking at word vectors, We will use Gensim. Gensim is an open source python library for natural language processing. Gensim isn’t really a deep learning package. It’s a package for word and text similarity modeling, which started with (LDA-style) topic models and grew into SVD and neural word representations. But its efficient and scalable, and quite widely used.\nGensim allow you to either easily train word embeddings via word2vec algorithms, or retrieve pre-trained word embeddings. Gensim provides a library of several sets of word vectors that you can easily load. Here we will work the Glove Embeddings.\n\nGlove.\nGloVe, which stands for Global Vectors for Word Representation, is a method created by Pennington and colleagues at Stanford as an improvement on word2vec for learning word embeddings. It’s an unsupervised model that builds word vectors by looking at how often words appear together across an entire text corpus. By using this global co-occurrence information, GloVe produces word vectors that capture interesting relationships and patterns in the vector space.\nWhat makes GloVe unique is that it combines ideas from two approaches: the global perspective of methods like Latent Semantic Analysis (LSA), which rely on matrix factorization, and the local context-based training of word2vec. Instead of just using a sliding window to learn from nearby words, GloVe explicitly constructs a word co-occurrence matrix that reflects word pair frequencies in the whole dataset.\nFor a detailed explanation, see the original paper by Jeffrey Pennington, Richard Socher, and Christopher D. Manning (2014): GloVe: Global Vectors for Word Representation.(https://nlp.stanford.edu/projects/glove/)\n\nimport numpy as np\n\n# Get the interactive Tools for Matplotlib\n# %matplotlib notebook\n%matplotlib inline\nimport matplotlib.pyplot as plt\nplt.style.use('ggplot')\n\n# PCA\nfrom sklearn.decomposition import PCA\n\n# Gensim\nimport gensim.downloader as api\nfrom gensim.models import KeyedVectors\n\n\nLoading Glove Embeddings\n\nmodel = api.load(\"glove-wiki-gigaword-100\")\nprint(type(model))\n\n&lt;class 'gensim.models.keyedvectors.KeyedVectors'&gt;\n\n\n\n# what is this exactly? Check the dimensions. \n\n# Extract vocabulary size and embedding dimension\nvocab_size = len(model)\nembedding_dim = model.vector_size\n\n# Build the matrix (rows = vocab, columns = embedding dim)\nembedding_matrix = np.zeros((vocab_size, embedding_dim))\n\n# Fill the matrix\nfor i, word in enumerate(model.index_to_key):\n    embedding_matrix[i] = model[word]\n\n\n# see dimensions    \nprint(embedding_matrix.shape)\n\n(400000, 100)\n\n\n\n## see what it looks like\nembedding_matrix[10:]\n\narray([[-0.14401001,  0.32554001,  0.14257   , ...,  0.25398001,\n         1.10780001, -0.073074  ],\n       [-1.25569999,  0.61036003,  0.56792998, ..., -1.58770001,\n         0.76984   , -0.64998001],\n       [-0.093337  ,  0.19043   ,  0.68457001, ..., -0.68175   ,\n         0.28804001,  0.54892999],\n       ...,\n       [ 0.36087999, -0.16919   , -0.32703999, ...,  0.27138999,\n        -0.29188001,  0.16109   ],\n       [-0.10461   , -0.50470001, -0.49331   , ...,  0.42526999,\n        -0.51249999, -0.17054   ],\n       [ 0.28365001, -0.62629998, -0.44351   , ...,  0.43678001,\n        -0.82607001, -0.15701   ]])\n\n\n\n\nCheck the vectors for some words\n\n# check some words\nmodel['bread']\n\narray([-0.66146  ,  0.94335  , -0.72214  ,  0.17403  , -0.42524  ,\n        0.36303  ,  1.0135   , -0.14802  ,  0.25817  , -0.20326  ,\n       -0.64338  ,  0.16632  ,  0.61518  ,  1.397    , -0.094506 ,\n        0.0041843, -0.18976  , -0.55421  , -0.39371  , -0.22501  ,\n       -0.34643  ,  0.32076  ,  0.34395  , -0.7034   ,  0.23932  ,\n        0.69951  , -0.16461  , -0.31819  , -0.34034  , -0.44906  ,\n       -0.069667 ,  0.35348  ,  0.17498  , -0.95057  , -0.2209   ,\n        1.0647   ,  0.23231  ,  0.32569  ,  0.47662  , -1.1206   ,\n        0.28168  , -0.75172  , -0.54654  , -0.66337  ,  0.34804  ,\n       -0.69058  , -0.77092  , -0.40167  , -0.069351 , -0.049238 ,\n       -0.39351  ,  0.16735  , -0.14512  ,  1.0083   , -1.0608   ,\n       -0.87314  , -0.29339  ,  0.68278  ,  0.61634  , -0.088844 ,\n        0.88094  ,  0.099809 , -0.27161  , -0.58026  ,  0.50364  ,\n       -0.93814  ,  0.67576  , -0.43124  , -0.10517  , -1.2404   ,\n       -0.74353  ,  0.28637  ,  0.29012  ,  0.89377  ,  0.67406  ,\n        0.86422  , -0.30693  , -0.14718  ,  0.078353 ,  0.74013  ,\n        0.32658  , -0.052579 , -1.1665   ,  0.87079  , -0.69402  ,\n       -0.75977  , -0.37164  , -0.11887  ,  0.18551  ,  0.041883 ,\n        0.59352  ,  0.30519  , -0.54819  , -0.29424  , -1.4912   ,\n       -1.6548   ,  0.98982  ,  0.27325  ,  1.009    ,  0.94544  ],\n      dtype=float32)\n\n\n\nmodel['croissant']\n\narray([-0.25144  ,  0.52157  , -0.75452  ,  0.28039  , -0.31388  ,\n        0.274    ,  1.1971   , -0.10519  ,  0.82544  , -0.33398  ,\n       -0.21417  ,  0.22216  ,  0.14982  ,  0.47384  ,  0.41984  ,\n        0.69397  , -0.25999  , -0.44414  ,  0.58296  , -0.30851  ,\n       -0.076455 ,  0.33468  ,  0.28055  , -0.99012  ,  0.30349  ,\n        0.39128  ,  0.031526 , -0.095395 , -0.004745 , -0.81347  ,\n        0.27869  , -0.1812   ,  0.14632  , -0.42186  ,  0.13857  ,\n        1.139    ,  0.14925  , -0.051459 ,  0.37875  , -0.2613   ,\n        0.011081 , -0.28881  , -0.38662  , -0.3135   , -0.1954   ,\n        0.19248  , -0.52995  , -0.40674  , -0.25159  ,  0.06272  ,\n       -0.32724  ,  0.28374  , -0.2155   , -0.061832 , -0.50134  ,\n        0.0093959,  0.30715  ,  0.3873   , -0.74554  , -0.45947  ,\n        0.40032  , -0.1378   , -0.26968  , -0.3946   , -0.64876  ,\n       -0.47149  , -0.085536 ,  0.092795 , -0.034018 , -0.61906  ,\n        0.19123  ,  0.20563  ,  0.29056  , -0.010908 ,  0.15313  ,\n        0.33144  ,  0.33806  ,  0.061708 ,  0.20785  ,  0.65348  ,\n       -0.053222 ,  0.18589  ,  0.32647  , -0.11923  ,  0.42008  ,\n       -0.26931  ,  0.025489 ,  0.0036535,  0.1327   , -0.22763  ,\n        0.07564  ,  0.55773  ,  0.2978   ,  0.28144  ,  0.19775  ,\n       -0.23582  ,  0.65303  ,  0.089897 ,  0.35844  ,  0.14304  ],\n      dtype=float32)\n\n\n\n\nSimilarity\nGensim has several pre-built functions to work with word vectors. For example, we can easily calculate the similarity between words\n\nmodel.most_similar('usa')\n\n[('canada', 0.6544384360313416),\n ('america', 0.645224392414093),\n ('u.s.a.', 0.6184033155441284),\n ('united', 0.6017189621925354),\n ('states', 0.5970699191093445),\n ('australia', 0.5838716626167297),\n ('world', 0.5590084791183472),\n ('2010', 0.5580702424049377),\n ('2012', 0.5504006743431091),\n ('davis', 0.5464468002319336)]\n\n\n\nmodel.most_similar('banana')\n\n[('coconut', 0.7097253203392029),\n ('mango', 0.7054824829101562),\n ('bananas', 0.6887733936309814),\n ('potato', 0.6629636883735657),\n ('pineapple', 0.6534532308578491),\n ('fruit', 0.6519854664802551),\n ('peanut', 0.6420576572418213),\n ('pecan', 0.6349173784255981),\n ('cashew', 0.6294420957565308),\n ('papaya', 0.6246591210365295)]\n\n\n\nmodel.most_similar('croissant')\n\n[('croissants', 0.6829844117164612),\n ('brioche', 0.6283303499221802),\n ('baguette', 0.5968102812767029),\n ('focaccia', 0.5876684784889221),\n ('pudding', 0.5803956389427185),\n ('souffle', 0.5614769458770752),\n ('baguettes', 0.5558240413665771),\n ('tortilla', 0.5449503064155579),\n ('pastries', 0.5427731275558472),\n ('calzone', 0.5374532341957092)]\n\n\n\n\nMost distinct words\n\nmodel.most_similar(negative='banana')\n\n[('shunichi', 0.49618107080459595),\n ('ieronymos', 0.4736502468585968),\n ('pengrowth', 0.4668096601963043),\n ('höss', 0.4636845886707306),\n ('damaskinos', 0.4617849290370941),\n ('yadin', 0.4617374837398529),\n ('hundertwasser', 0.458895742893219),\n ('ncpa', 0.4577338993549347),\n ('maccormac', 0.4566109776496887),\n ('rothfeld', 0.4523947238922119)]\n\n\n\n\nWords Geometry\nWord vectors allow operation across words.This is what is called vector arithmetic.\n\nresult = model.most_similar(positive=['woman', 'king'], negative=['man'])\nprint(\"{}: {:.4f}\".format(*result[0]))\n\nqueen: 0.7699\n\n\nWhat’s happening?\nThe model.most_similar() function finds words that are closest (in vector space) to the combination you specify.\nHere you’re giving it:\nPositive words: [‘woman’, ‘king’]\nNegative words: [‘man’]\nVector Operations\noutput= (vector for ’woman’ ) + (vector for ’king’ ) − ( vector for ’man’ )\n\n# write a function\ndef analogy(x1, x2, y1):\n    result = model.most_similar(positive=[y1, x2], negative=[x1])\n    return result[0][0]\n\n\nanalogy('man', 'king', 'woman')\n\n'queen'\n\n\n\nanalogy('australia', 'beer', 'france')\n\n'champagne'\n\n\n\nanalogy('pencil', 'sketching', 'camera')\n\n'photographing'\n\n\n\n\n\nApplication: The Geometry of Culture: Analyzing the Meanings of Class through Word Embeddings\nThis simple analogy between word vectors is the core of the method developed in the paper “The Geometry of Culture”. The idea here is to build a new vector with positive and negative words, and then project new words over this cultural vector.\nLet’s see in code:\n\n# step 1: define words that identify lower and upper class\nupper_class = ['rich', 'manager', 'elite', 'capitalist']\nlower_class = ['poor', 'worker', 'laborer', 'socialist']\n\n# Retrieve the vectors of these words\nupper_vecs = np.array([model[w] for w in upper_class])\nlower_vecs = np.array([model[w] for w in lower_class])\n\n# visualize\nprint(upper_vecs[0])\n\n[-0.1328     0.92261    0.93544   -0.63118    0.50402   -0.30338\n -0.55406    0.071319  -0.13877    0.11294   -0.30848   -0.35173\n  0.27514   -0.015935  -0.19586   -0.26719    0.1023    -1.127\n -0.35836    0.80409   -0.12859    0.15648    0.57618   -0.21437\n -0.029065   0.49854   -0.27588   -0.051191  -0.23875    0.24421\n  0.22476    0.070125   0.50378   -1.1223     0.4764    -0.06986\n -0.027617   0.7853    -0.68022   -0.036379   0.37911   -0.93087\n -0.011035  -0.082644  -0.27459    0.09938   -0.26818    0.29252\n  0.62947   -0.22741   -0.6275     0.031109  -0.054661   0.16065\n -0.38845   -1.9376    -0.10413   -0.64834    1.4837    -0.13894\n -0.049802   0.55547   -0.21925   -0.90887    1.4822    -0.6478\n  1.1769     0.1142     0.33713   -0.7661     0.37561    0.0037597\n -0.28029    0.12944   -0.17194   -0.10839   -0.13159   -0.7647\n -0.17149    0.98379    0.83413    0.53028   -0.46108   -0.59009\n -0.24216    0.18024   -0.28845   -0.40467   -0.65468   -0.79142\n  0.33132   -0.52434   -0.26786   -0.53644   -1.1112     0.29879\n -0.9842    -0.70996   -0.50225    0.21959  ]\n\n\n\n### Step 2: Average, Take the Difference, and Normalize the Vector\nupper_mean = np.mean(upper_vecs, axis=0)\nlower_mean = np.mean(lower_vecs, axis=0)\n\n## Difference\nclass_vector = upper_mean - lower_mean\n\n## normalize the vectors\nclass_vector = class_vector / np.linalg.norm(class_vector)\n\n\n### Step 3: projecting words in the class vectors\n\n## Select some occupations\noccupations = ['teacher', 'lawyer', 'janitor', 'nurse',\n                'engineer', 'ceo', 'manager', 'cashier',\n                'doctor', 'mechanic', 'bartender', 'professor',\n                'farmer']\n\n# build an empty list for the projection\nprojections = []\n\nfor job in occupations:\n    job_vec = model[job]\n    # projection - equal to a cosine similarity because we normalized the vectors\n    proj = np.dot(job_vec, class_vector)\n    projections.append((job, proj))\n\n# Sort\nprojections.sort(key=lambda x: x[1])\n\n\n## plot\nfrom plotnine import *\nimport pandas as pd\n\n\n# build a datafram\ndf = pd.DataFrame(projections, columns=['job', 'projection'])\n\n# sort\ndf = df.sort_values('projection')\n\n# plots\n\np = (\n    ggplot(df, aes(y='projection', x='job'))  # x is projection, y is job\n    + geom_col(fill='red')\n    + geom_vline(xintercept=0, linetype='dashed', color='black')\n    + labs(\n        title='Occupation Projected on Class',\n        x='Cosine similarity: upper-class → lower-class',\n        y=''\n    )\n    + theme_minimal()\n    + theme(\n        figure_size=(10, 6),\n        axis_title_x=element_text(size=12),\n        axis_title_y=element_text(size=12),\n        plot_title=element_text(weight='bold', size=14),\n    ) +\n    coord_flip()\n)\n\np\n\n/Users/tb186/anaconda3/envs/cs224n/lib/python3.12/site-packages/plotnine/_mpl/utils.py:23: UserWarning: Glyph 8594 (\\N{RIGHTWARDS ARROW}) missing from font(s) Helvetica.\n/Users/tb186/anaconda3/envs/cs224n/lib/python3.12/site-packages/plotnine/ggplot.py:687: UserWarning: Glyph 8594 (\\N{RIGHTWARDS ARROW}) missing from font(s) Helvetica."
  },
  {
    "objectID": "code/day_1_logistic_as_neural_network.html",
    "href": "code/day_1_logistic_as_neural_network.html",
    "title": "Logistic Regression with a Neural Network mindset",
    "section": "",
    "text": "This is an assigment from Andrew Ng’s Deep Learning Specialization. I solved the assignment, and believe this is one of the most interesting learning exercises to see the building blocks of neural networks.\nI removed some unecessary text, but other than that, this is heavily taken from Andres Ng’s course.\nInstructions: - Do not use loops (for/while) in your code, unless the instructions explicitly ask you to do so.\nYou will learn to: - Build the general architecture of a learning algorithm, including: - Initializing parameters - Calculating the cost function and its gradient - Using an optimization algorithm (gradient descent) - Gather all three functions above into a main model function, in the right order."
  },
  {
    "objectID": "code/day_1_logistic_as_neural_network.html#packages",
    "href": "code/day_1_logistic_as_neural_network.html#packages",
    "title": "Logistic Regression with a Neural Network mindset",
    "section": "1 - Packages",
    "text": "1 - Packages\nFirst, let’s run the cell below to import all the packages that you will need during this assignment. - numpy is the fundamental package for scientific computing with Python. - h5py is a common package to interact with a dataset that is stored on an H5 file. - matplotlib is a famous library to plot graphs in Python. - PIL and scipy are used here to test your model with your own picture at the end.\n\n# make sure to have these libraries installed. you can use pip install &lt;library&gt;\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport h5py\nimport scipy\nfrom PIL import Image\nfrom scipy import ndimage\n# notice the lr_utils needs to be in the same directory of your notebook\nfrom lr_utils import load_dataset\n\n%matplotlib inline"
  },
  {
    "objectID": "code/day_1_logistic_as_neural_network.html#overview-of-the-problem-set",
    "href": "code/day_1_logistic_as_neural_network.html#overview-of-the-problem-set",
    "title": "Logistic Regression with a Neural Network mindset",
    "section": "2 - Overview of the Problem set",
    "text": "2 - Overview of the Problem set\nProblem Statement: You are given a dataset (“data.h5”) containing: - a training set of m_train images labeled as cat (y=1) or non-cat (y=0) - a test set of m_test images labeled as cat or non-cat - each image is of shape (num_px, num_px, 3) where 3 is for the 3 channels (RGB). Thus, each image is square (height = num_px) and (width = num_px).\nYou will build a simple image-recognition algorithm that can correctly classify pictures as cat or non-cat.\nLet’s get more familiar with the dataset. Load the data by running the following code.\n\n# Loading the data (cat/non-cat)\ntrain_set_x_orig, train_set_y, test_set_x_orig, test_set_y, classes = load_dataset()\n\nWe added “_orig” at the end of image datasets (train and test) because we are going to preprocess them. After preprocessing, we will end up with train_set_x and test_set_x (the labels train_set_y and test_set_y don’t need any preprocessing).\nEach line of your train_set_x_orig and test_set_x_orig is an array representing an image. You can visualize an example by running the following code. Feel free also to change the index value and re-run to see other images.\n\n# Example of a picture\nindex = 25\nplt.imshow(train_set_x_orig[index])\nprint (\"y = \" + str(train_set_y[:, index]) + \", it's a '\" + classes[np.squeeze(train_set_y[:, index])].decode(\"utf-8\") +  \"' picture.\")\n\ny = [1], it's a 'cat' picture.\n\n\n\n\n\nMany software bugs in deep learning come from having matrix/vector dimensions that don’t fit. If you can keep your matrix/vector dimensions straight you will go a long way toward eliminating many bugs.\nExercise: Find the values for: - m_train (number of training examples) - m_test (number of test examples) - num_px (= height = width of a training image) Remember that train_set_x_orig is a numpy-array of shape (m_train, num_px, num_px, 3). For instance, you can access m_train by writing train_set_x_orig.shape[0].\n\n### START CODE HERE ### (≈ 3 lines of code)\nm_train = train_set_x_orig.shape[0]\nm_test = test_set_x_orig.shape[0]\nnum_px = train_set_x_orig.shape[1]\n### END CODE HERE ###\n\nprint (\"Number of training examples: m_train = \" + str(m_train))\nprint (\"Number of testing examples: m_test = \" + str(m_test))\nprint (\"Height/Width of each image: num_px = \" + str(num_px))\nprint (\"Each image is of size: (\" + str(num_px) + \", \" + str(num_px) + \", 3)\")\nprint (\"train_set_x shape: \" + str(train_set_x_orig.shape))\nprint (\"train_set_y shape: \" + str(train_set_y.shape))\nprint (\"test_set_x shape: \" + str(test_set_x_orig.shape))\nprint (\"test_set_y shape: \" + str(test_set_y.shape))\n\nNumber of training examples: m_train = 209\nNumber of testing examples: m_test = 50\nHeight/Width of each image: num_px = 64\nEach image is of size: (64, 64, 3)\ntrain_set_x shape: (209, 64, 64, 3)\ntrain_set_y shape: (1, 209)\ntest_set_x shape: (50, 64, 64, 3)\ntest_set_y shape: (1, 50)\n\n\nExpected Output for m_train, m_test and num_px:\n\n\n\n\nm_train\n\n\n209\n\n\n\n\nm_test\n\n\n50\n\n\n\n\nnum_px\n\n\n64\n\n\n\n\nFor convenience, you should now reshape images of shape (num_px, num_px, 3) in a numpy-array of shape (num_px \\(*\\) num_px \\(*\\) 3, 1). After this, our training (and test) dataset is a numpy-array where each column represents a flattened image. There should be m_train (respectively m_test) columns.\nExercise: Reshape the training and test data sets so that images of size (num_px, num_px, 3) are flattened into single vectors of shape (num_px \\(*\\) num_px \\(*\\) 3, 1).\nA trick when you want to flatten a matrix X of shape (a,b,c,d) to a matrix X_flatten of shape (b\\(*\\)c\\(*\\)d, a) is to use:\nX_flatten = X.reshape(X.shape[0], -1).T      # X.T is the transpose of X\n\n# Reshape the training and test examples\ntrain_set_x_flatten = train_set_x_orig.reshape(train_set_x_orig.shape[0], -1).T\ntest_set_x_flatten = test_set_x_orig.reshape(test_set_x_orig.shape[0], -1).T\n\nprint (\"train_set_x_flatten shape: \" + str(train_set_x_flatten.shape))\nprint (\"train_set_y shape: \" + str(train_set_y.shape))\nprint (\"test_set_x_flatten shape: \" + str(test_set_x_flatten.shape))\nprint (\"test_set_y shape: \" + str(test_set_y.shape))\nprint (\"sanity check after reshaping: \" + str(train_set_x_flatten[0:5,0]))\n\ntrain_set_x_flatten shape: (12288, 209)\ntrain_set_y shape: (1, 209)\ntest_set_x_flatten shape: (12288, 50)\ntest_set_y shape: (1, 50)\nsanity check after reshaping: [17 31 56 22 33]\n\n\nTo represent color images, the red, green and blue channels (RGB) must be specified for each pixel, and so the pixel value is actually a vector of three numbers ranging from 0 to 255.\nOne common preprocessing step in machine learning is to center and standardize your dataset, meaning that you substract the mean of the whole numpy array from each example, and then divide each example by the standard deviation of the whole numpy array. But for picture datasets, it is simpler and more convenient and works almost as well to just divide every row of the dataset by 255 (the maximum value of a pixel channel).\n\nLet’s standardize our dataset.\n\ntrain_set_x = train_set_x_flatten/255.\ntest_set_x = test_set_x_flatten/255."
  },
  {
    "objectID": "code/day_1_logistic_as_neural_network.html#general-architecture-of-the-learning-algorithm",
    "href": "code/day_1_logistic_as_neural_network.html#general-architecture-of-the-learning-algorithm",
    "title": "Logistic Regression with a Neural Network mindset",
    "section": "3 - General Architecture of the learning algorithm",
    "text": "3 - General Architecture of the learning algorithm\nIt’s time to design a simple algorithm to distinguish cat images from non-cat images.\nYou will build a Logistic Regression, using a Neural Network mindset. The following Figure explains why Logistic Regression is actually a very simple Neural Network!\n\nMathematical expression of the algorithm:\nFor one example \\(x^{(i)}\\): \\[z^{(i)} = w^T x^{(i)} + b \\tag{1}\\] \\[\\hat{y}^{(i)} = a^{(i)} = sigmoid(z^{(i)})\\tag{2}\\] \\[ \\mathcal{L}(a^{(i)}, y^{(i)}) =  - y^{(i)}  \\log(a^{(i)}) - (1-y^{(i)} )  \\log(1-a^{(i)})\\tag{3}\\]\nThe cost is then computed by summing over all training examples: \\[ J = \\frac{1}{m} \\sum_{i=1}^m \\mathcal{L}(a^{(i)}, y^{(i)})\\tag{6}\\]\nKey steps: In this exercise, you will carry out the following steps:\n- Initialize the parameters of the model\n- Learn the parameters for the model by minimizing the cost  \n- Use the learned parameters to make predictions (on the test set)\n- Analyse the results and conclude"
  },
  {
    "objectID": "code/day_1_logistic_as_neural_network.html#building-the-parts-of-our-algorithm",
    "href": "code/day_1_logistic_as_neural_network.html#building-the-parts-of-our-algorithm",
    "title": "Logistic Regression with a Neural Network mindset",
    "section": "4 - Building the parts of our algorithm",
    "text": "4 - Building the parts of our algorithm\nThe main steps for building a Neural Network are: 1. Define the model structure (such as number of input features) 2. Initialize the model’s parameters 3. Loop: - Calculate current loss (forward propagation) - Calculate current gradient (backward propagation) - Update parameters (gradient descent)\nYou often build 1-3 separately and integrate them into one function we call model().\n\n4.1 - Helper functions\nAs you’ve seen in the figure above, you need to compute \\(sigmoid( w^T x + b) = \\frac{1}{1 + e^{-(w^T x + b)}}\\) to make predictions.\n\n# GRADED FUNCTION: sigmoid\n\ndef sigmoid(z):\n    \"\"\"\n    Compute the sigmoid of z\n\n    Arguments:\n    z -- A scalar or numpy array of any size.\n\n    Return:\n    s -- sigmoid(z)\n    \"\"\"\n    \n    ## Sigmoid function\n    s = (1/(1+np.exp(-z))) ## z=(wt + a)\n    \n    return s\n\n\nprint (\"sigmoid([0, 2]) = \" + str(sigmoid(np.array([0,2]))))\n\nsigmoid([0, 2]) = [0.5        0.88079708]\n\n\n\n\n4.2 - Initializing parameters\nNext step is to initialize w as a vector of zeros. If you don’t know what numpy function to use, look up np.zeros() in the Numpy library’s documentation.\n\n#initialize_with_zeros\n\ndef initialize_with_zeros(dim):\n    \"\"\"\n    This function creates a vector of zeros of shape (dim, 1) for w and initializes b to 0.\n    \n    Argument:\n    dim -- size of the w vector we want (or number of parameters in this case)\n    \n    Returns:\n    w -- initialized vector of shape (dim, 1)\n    b -- initialized scalar (corresponds to the bias)\n    \"\"\"\n    \n    # initialize parameter\n    w = np.zeros(shape=(dim, 1))\n    b = 0\n\n    assert(w.shape == (dim, 1))\n    assert(isinstance(b, float) or isinstance(b, int))\n    \n    return w, b\n\n\ndim = 2\nw, b = initialize_with_zeros(dim)\nprint (\"w = \" + str(w))\nprint (\"b = \" + str(b))\n\nw = [[0.]\n [0.]]\nb = 0\n\n\nIMPORTANT For image inputs, w will be of shape (num_px \\(\\times\\) num_px \\(\\times\\) 3, 1). This is the number of betas in the estimation\n\n\n4.3 - Forward and Backward propagation\nNow that your parameters are initialized, you can do the “forward” and “backward” propagation steps for learning the parameters.\nLet’s Implement a function propagate() that computes the cost function and its gradient.\nForward Propagation: - You get X - You compute \\(A = \\sigma(w^T X + b) = (a^{(1)}, a^{(2)}, ..., a^{(m-1)}, a^{(m)})\\) - You calculate the cost function: \\(J = -\\frac{1}{m}\\sum_{i=1}^{m}y^{(i)}\\log(a^{(i)})+(1-y^{(i)})\\log(1-a^{(i)})\\)\nBackward Propagation:\nHere are the two formulas you will be using for the derivatives:\n\\[ \\frac{\\partial J}{\\partial w} = \\frac{1}{m}X(A-Y)^T\\tag{7}\\] \\[ \\frac{\\partial J}{\\partial b} = \\frac{1}{m} \\sum_{i=1}^m (a^{(i)}-y^{(i)})\\tag{8}\\]\n\n# GRADED FUNCTION: propagate\n\ndef propagate(w, b, X, Y):\n    \"\"\"\n    Implement the cost function and its gradient for the propagation explained above\n\n    Arguments:\n    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n    b -- bias, a scalar\n    X -- data of size (num_px * num_px * 3, number of examples)\n    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat) of size (1, number of examples)\n    \n    # w=vector(dim=number of variables)\n    # X= matrix(variables,observations)\n    \n    Return:\n    cost -- negative log-likelihood cost for logistic regression\n    dw -- gradient of the loss with respect to w, thus same shape as w\n    db -- gradient of the loss with respect to b, thus same shape as b\n    \n    Tips:\n    - Write your code step by step for the propagation. np.log(), np.dot()\n    \"\"\"\n    \n    m = X.shape[1] # numer of observations\n    \n    # FORWARD PROPAGATION (FROM X TO COST)\n    A = sigmoid(np.dot(w.T, X) + b)  # compute activation\n    cost = (- 1 / m) * np.sum(Y * np.log(A) + (1 - Y) * (np.log(1 - A)))  # compute cost\n    \n    # BACKWARD PROPAGATION (TO FIND GRAD)\n    dw = (1 / m) * np.dot(X, (A - Y).T) # dim=number of variables\n    db = (1 / m) * np.sum(A - Y)\n\n    assert(dw.shape == w.shape)\n    assert(db.dtype == float)\n    cost = np.squeeze(cost)\n    assert(cost.shape == ())\n    \n    grads = {\"dw\": dw,\n             \"db\": db}\n    \n    return grads, cost\n\n\n## let's see if the function works\nw, b, X, Y = np.array([[1.],[2.]]), 2., np.array([[1.,2.,-1.],[3.,4.,-3.2]]), np.array([[1,0,1]])\ngrads, cost = propagate(w, b, X, Y)\nprint (\"dw = \" + str(grads[\"dw\"]))\nprint (\"db = \" + str(grads[\"db\"]))\nprint (\"cost = \" + str(cost))\n\ndw = [[0.99845601]\n [2.39507239]]\ndb = 0.001455578136784208\ncost = 5.801545319394553\n\n\n\n\n4.4 - Optimization\n\nYou have initialized your parameters.\nYou are also able to compute a cost function and its gradient.\nNow, you want to update the parameters using gradient descent.\n\nLet’s write an optimization function. The goal is to learn \\(w\\) and \\(b\\) by minimizing the cost function \\(J\\). For a parameter \\(\\theta\\), the update rule is $ = - d$, where \\(\\alpha\\) is the learning rate.\n\n# GRADED FUNCTION: optimize\n\ndef optimize(w, b, X, Y, num_iterations, learning_rate, print_cost = False):\n    \"\"\"\n    This function optimizes w and b by running a gradient descent algorithm\n    \n    Arguments:\n    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n    b -- bias, a scalar\n    X -- data of shape (num_px * num_px * 3, number of examples)\n    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat), of shape (1, number of examples)\n    num_iterations -- number of iterations of the optimization loop\n    learning_rate -- learning rate of the gradient descent update rule\n    print_cost -- True to print the loss every 100 steps\n    \n    Returns:\n    params -- dictionary containing the weights w and bias b\n    grads -- dictionary containing the gradients of the weights and bias with respect to the cost function\n    costs -- list of all the costs computed during the optimization, this will be used to plot the learning curve.\n    \n    Tips:\n    You basically need to write down two steps and iterate through them:\n        1) Calculate the cost and the gradient for the current parameters. Use propagate().\n        2) Update the parameters using gradient descent rule for w and b.\n    \"\"\"\n    \n    costs = []\n    \n    for i in range(num_iterations):\n        \n        \n        # Cost and gradient calculation (≈ 1-4 lines of code)\n        grads, cost = propagate(w, b, X, Y)\n        \n        # Retrieve derivatives from grads\n        dw = grads[\"dw\"]\n        db = grads[\"db\"]\n        \n        # update rule (≈ 2 lines of code)\n        w = w - learning_rate*dw\n        b = b - learning_rate*db\n        \n        # Record the costs\n        if i % 100 == 0:\n            costs.append(cost)\n        \n        # Print the cost every 100 training iterations\n        if print_cost and i % 100 == 0:\n            print (\"Cost after iteration %i: %f\" %(i, cost))\n    \n    params = {\"w\": w,\n              \"b\": b}\n    \n    grads = {\"dw\": dw,\n             \"db\": db}\n    \n    return params, grads, costs\n\n\nparams, grads, costs = optimize(w, b, X, Y, num_iterations= 1000, learning_rate = 0.009, print_cost = True)\n\nprint (\"w = \" + str(params[\"w\"]))\nprint (\"b = \" + str(params[\"b\"]))\nprint (\"dw = \" + str(grads[\"dw\"]))\nprint (\"db = \" + str(grads[\"db\"]))\n\nCost after iteration 0: 5.801545\nCost after iteration 100: 1.055933\nCost after iteration 200: 0.378303\nCost after iteration 300: 0.363595\nCost after iteration 400: 0.356242\nCost after iteration 500: 0.349210\nCost after iteration 600: 0.342420\nCost after iteration 700: 0.335860\nCost after iteration 800: 0.329517\nCost after iteration 900: 0.323380\nw = [[-0.64226437]\n [-0.43498153]]\nb = 2.2025594747904087\ndw = [[ 0.06282959]\n [-0.01416124]]\ndb = -0.04847508604218077\n\n\n\n\nPredictions\nWe are able to use w and b to predict the labels for a dataset X. Let’s implement a predict() function. There are two steps to computing predictions:\n\nCalculate \\(\\hat{Y} = A = \\sigma(w^T X + b)\\)\nConvert the entries of a into 0 (if activation &lt;= 0.5) or 1 (if activation &gt; 0.5), stores the predictions in a vector Y_prediction. If you wish, you can use an if/else statement in a for loop (though there is also a way to vectorize this).\n\n\n# GRADED FUNCTION: predict\n\ndef predict(w, b, X):\n    '''\n    Predict whether the label is 0 or 1 using learned logistic regression parameters (w, b)\n    \n    Arguments:\n    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n    b -- bias, a scalar\n    X -- data of size (num_px * num_px * 3, number of examples)\n    \n    Returns:\n    Y_prediction -- a numpy array (vector) containing all predictions (0/1) for the examples in X\n    '''\n    \n    m = X.shape[1]\n    Y_prediction = np.zeros((1,m))\n    w = w.reshape(X.shape[0], 1)\n    \n    # Compute vector \"A\" predicting the probabilities of a cat being present in the picture\n    A = sigmoid(np.dot(w.T, X) + b)\n    \n    for i in range(A.shape[1]):\n        \n        # Convert probabilities A[0,i] to actual predictions p[0,i]\n        Y_prediction[0, i] = 1 if A[0, i] &gt; 0.5 else 0\n    \n    assert(Y_prediction.shape == (1, m))\n    \n    return Y_prediction\n\n\nw = np.array([[0.1124579],[0.23106775]])\nb = -0.3\nX = np.array([[1.,-1.1,-3.2],[1.2,2.,0.1]])\nprint (\"predictions = \" + str(predict(w, b, X)))\n\npredictions = [[1. 1. 0.]]\n\n\n\nWhat to remember:\nYou’ve implemented several functions that:\n\nInitialize (w,b)\nOptimize the loss iteratively to learn parameters (w,b):\n\ncomputing the cost and its gradient\nupdating the parameters using gradient descent\n\nUse the learned (w,b) to predict the labels for a given set of examples"
  },
  {
    "objectID": "code/day_1_logistic_as_neural_network.html#merge-all-functions-into-a-model",
    "href": "code/day_1_logistic_as_neural_network.html#merge-all-functions-into-a-model",
    "title": "Logistic Regression with a Neural Network mindset",
    "section": "5 - Merge all functions into a model",
    "text": "5 - Merge all functions into a model\nYou will now see how the overall model is structured by putting together all the building blocks (functions implemented in the previous parts) together, in the right order.\nLet’s now implment a full model using the following notation:\n- Y_prediction_test for your predictions on the test set\n- Y_prediction_train for your predictions on the train set\n- w, costs, grads for the outputs of optimize()\n\n# Model\n\ndef model(X_train, Y_train, X_test, Y_test, num_iterations = 2000, learning_rate = 0.5, print_cost = False):\n    \"\"\"\n    Builds the logistic regression model by calling the function you've implemented previously\n    \n    Arguments:\n    X_train -- training set represented by a numpy array of shape (num_px * num_px * 3, m_train)\n    Y_train -- training labels represented by a numpy array (vector) of shape (1, m_train)\n    X_test -- test set represented by a numpy array of shape (num_px * num_px * 3, m_test)\n    Y_test -- test labels represented by a numpy array (vector) of shape (1, m_test)\n    num_iterations -- hyperparameter representing the number of iterations to optimize the parameters\n    learning_rate -- hyperparameter representing the learning rate used in the update rule of optimize()\n    print_cost -- Set to true to print the cost every 100 iterations\n    \n    Returns:\n    d -- dictionary containing information about the model.\n    \"\"\"\n    \n    # initialize parameters with zeros (≈ 1 line of code)\n    w, b = initialize_with_zeros(X_train.shape[0])\n\n    # Gradient descent (≈ 1 line of code)\n    parameters, grads, costs = optimize(w, b, X_train, Y_train, num_iterations, learning_rate, print_cost)\n    \n    # Retrieve parameters w and b from dictionary \"parameters\"\n    w = parameters[\"w\"]\n    b = parameters[\"b\"]\n    \n    # Predict test/train set examples (≈ 2 lines of code)\n    Y_prediction_test = predict(w, b, X_test)\n    Y_prediction_train = predict(w, b, X_train)\n\n    # Print train/test Errors\n    print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_train - Y_train)) * 100))\n    print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_test - Y_test)) * 100))\n\n    \n    d = {\"costs\": costs,\n         \"Y_prediction_test\": Y_prediction_test, \n         \"Y_prediction_train\" : Y_prediction_train, \n         \"w\" : w, \n         \"b\" : b,\n         \"learning_rate\" : learning_rate,\n         \"num_iterations\": num_iterations}\n    \n    return d\n\n\nTrain your model with real data\n\nd = model(train_set_x, train_set_y, test_set_x, test_set_y,\n          num_iterations = 2000, learning_rate = 0.005, \n          print_cost = True)\n\nCost after iteration 0: 0.693147\nCost after iteration 100: 0.584508\nCost after iteration 200: 0.466949\nCost after iteration 300: 0.376007\nCost after iteration 400: 0.331463\nCost after iteration 500: 0.303273\nCost after iteration 600: 0.279880\nCost after iteration 700: 0.260042\nCost after iteration 800: 0.242941\nCost after iteration 900: 0.228004\nCost after iteration 1000: 0.214820\nCost after iteration 1100: 0.203078\nCost after iteration 1200: 0.192544\nCost after iteration 1300: 0.183033\nCost after iteration 1400: 0.174399\nCost after iteration 1500: 0.166521\nCost after iteration 1600: 0.159305\nCost after iteration 1700: 0.152667\nCost after iteration 1800: 0.146542\nCost after iteration 1900: 0.140872\ntrain accuracy: 99.04306220095694 %\ntest accuracy: 70.0 %\n\n\nComment: Training accuracy is close to 100%. This is a good sanity check: your model is working and has high enough capacity to fit the training data. Test accuracy is 68%. It is actually not bad for this simple model, given the small dataset we used and that logistic regression is a linear classifier. But no worries, you’ll build an even better classifier next week!\nAlso, you see that the model is clearly overfitting the training data. Later in this specialization you will learn how to reduce overfitting, for example by using regularization. Using the code below (and changing the index variable) you can look at predictions on pictures of the test set.\nLet’s also plot the cost function and the gradients.\n\n# Plot learning curve (with costs)\ncosts = np.squeeze(d['costs'])\nplt.plot(costs)\nplt.ylabel('cost')\nplt.xlabel('iterations (per hundreds)')\nplt.title(\"Learning rate =\" + str(d[\"learning_rate\"]))\nplt.show()\n\n\n\n\nInterpretation: You can see the cost decreasing. It shows that the parameters are being learned. However, you see that you could train the model even more on the training set. Try to increase the number of iterations in the cell above and rerun the cells. You might see that the training set accuracy goes up, but the test set accuracy goes down. This is called overfitting.\n\n\nAll of this with a logistic regression\nEverything we did here was to implement a logistic regression by hand using a neural network framework.\nLet me show you how you would do this without a neural network\n\n# import logistic regression\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\n\n# Initialize the model\nclf = LogisticRegression(max_iter=2000)  \n\n# Fit the model\nclf.fit(train_set_x.T, train_set_y.ravel())  # transpose to shape (m, n_features)\n\n# Predict\nY_prediction_train = clf.predict(train_set_x.T)\nY_prediction_test = clf.predict(test_set_x.T)\n\n# Compute accuracy\ntrain_accuracy = accuracy_score(train_set_y.ravel(), Y_prediction_train) * 100\ntest_accuracy = accuracy_score(test_set_y.ravel(), Y_prediction_test) * 100\n\nprint(f\"Train accuracy: {train_accuracy:.2f} %\")\nprint(f\"Test accuracy: {test_accuracy:.2f} %\")\n\nTrain accuracy: 100.00 %\nTest accuracy: 72.00 %"
  },
  {
    "objectID": "slides/day_1.html#from-text-to-numbers",
    "href": "slides/day_1.html#from-text-to-numbers",
    "title": "Advanced Text-As-Data  Winter School - Iesp UERJ",
    "section": "From Text to Numbers",
    "text": "From Text to Numbers"
  },
  {
    "objectID": "slides/day_1.html#euclidean-distance",
    "href": "slides/day_1.html#euclidean-distance",
    "title": "Advanced Text-As-Data  Winter School - Iesp UERJ",
    "section": "Euclidean Distance",
    "text": "Euclidean Distance\nThe ordinary, straight line distance between two points in space. Using document vectors \\(y_a\\) and \\(y_b\\) with \\(j\\) dimensions\n\n\n\nEuclidean Distance\n\n\n\\[\n||y_a - y_b|| = \\sqrt{\\sum^{j}(y_{aj} - y_{bj})^2}\n\\]"
  },
  {
    "objectID": "slides/day_1.html#cosine-similarity",
    "href": "slides/day_1.html#cosine-similarity",
    "title": "Advanced Text-As-Data  Winter School - Iesp UERJ",
    "section": "Cosine Similarity",
    "text": "Cosine Similarity\nEuclidean distance rewards magnitude, rather than direction\n\\[\n\\text{cosine similarity}(\\mathbf{y_a}, \\mathbf{y_b}) = \\frac{\\mathbf{y_a} \\cdot \\mathbf{y_b}}{\\|\\mathbf{y_a}\\| \\|\\mathbf{y_b}\\|}\n\\]\nUnpacking the formula:\n\n\\(\\mathbf{y_a} \\cdot \\mathbf{y_b}\\) ~ dot product between vectors\n\nprojecting common magnitudes\nmeasure of similarity (see textbook)\n\\(\\sum_j{y_{aj}*y_{bj}}\\)\n\n\\(||\\mathbf{y_a}||\\) ~ vector magnitude, length ~ \\(\\sqrt{\\sum{y_{aj}^2}}\\)\nnormalizes similarity by documents’ length ~ independent of document length be because it deals only with the angle of the vectors\ncosine similarity captures some notion of relative direction (e.g. style or topics in the document)"
  },
  {
    "objectID": "slides/day_1.html#workhorse-representation-document-feature-matrix",
    "href": "slides/day_1.html#workhorse-representation-document-feature-matrix",
    "title": "Advanced Text-As-Data  Winter School - Iesp UERJ",
    "section": "Workhorse Representation: Document-Feature Matrix",
    "text": "Workhorse Representation: Document-Feature Matrix\n\n\n\n\n\n\n\n\n\n\n\nSource: Arthur Spirling TAD Class"
  },
  {
    "objectID": "slides/day_1.html#deep-neural-networks",
    "href": "slides/day_1.html#deep-neural-networks",
    "title": "Advanced Text-As-Data  Winter School - Iesp UERJ",
    "section": "Deep Neural Networks",
    "text": "Deep Neural Networks\nA Deep Neural Network is equivalent to stacking multiple logistic regressions vertically and repeat this process multiple times across many layers.\nAs a Matrix, instead of:\n\\[\\mathbf{W}_{previous} = \\begin{bmatrix} w_1 & w_2 & \\dots & w_p\\end{bmatrix}\\]\nWe use this set of parameters:\n\\[\n\\mathbf{W} = \\begin{bmatrix} w_{11} & w_{12} & w_{13} & \\dots & w_{1p} \\\\\n                               w_{21} & w_{22} & w_{23} & \\dots & w_{2p} \\\\\n                               w_{31} & w_{32} & w_{33} & \\dots & w_{3p} \\\\\n                               \\vdots & \\vdots & \\vdots & \\vdots & \\vdots \\\\\n                               w_{k1} & w_{k2} & w_{k3} & \\dots & w_{kp}\n                \\end{bmatrix}\n\\]"
  },
  {
    "objectID": "slides/day_3.html#what-to-expect",
    "href": "slides/day_3.html#what-to-expect",
    "title": "Advanced Text-As-Data - Winter School - Iesp UERJ",
    "section": "What to Expect?",
    "text": "What to Expect?\nIn the last two days you learned some heavy matrix algebra, dot-product, sigmoid-function, mostly-black-magic-believe-in-me content. Now we will enter the paradox of complexity: the material we will learn today is so complex that it is best understood through intuition and experience.\nThe material will be divided into three parts:\n\nTransformers: lecture-heavy introduction to the Transformers architecture.\nPre-trained models: applying the Transformers architecture to pre-trained models and implementing it (code).\nLarge Language Models (LLMs): learn about Large Language models (LLMs), their applications and limitations, and how to implement LLMs (code)."
  },
  {
    "objectID": "slides/day_3.html#material",
    "href": "slides/day_3.html#material",
    "title": "Advanced Text-As-Data - Winter School - Iesp UERJ",
    "section": "Material",
    "text": "Material\nHere is where you can find all the lectures and code we will be using during the next two days:\n\nSlides for “Introduction to Transformers”: link.\nSlides for “Pre-Trained Transformers-Based Models”: link.\nCode for:\n\nUsing Fine-Tuned Models: link.\nFine-Tuning a Model: link.\nFurther Pre-Training a Model: link.\nOther applications: link."
  },
  {
    "objectID": "slides/day_1.html#our-rules-for-a-sucessful-online-workshop",
    "href": "slides/day_1.html#our-rules-for-a-sucessful-online-workshop",
    "title": "Advanced Text-As-Data  Winter School - Iesp UERJ",
    "section": "Our rules for a sucessful online workshop",
    "text": "Our rules for a sucessful online workshop\n\nWe ask you to:\n\nKeep you cameras open\nAsk questions throughout, feel free to interrupt us.\nThat’s it!"
  },
  {
    "objectID": "slides/day_1.html#workshop-topics",
    "href": "slides/day_1.html#workshop-topics",
    "title": "Advanced Text-As-Data  Winter School - Iesp UERJ",
    "section": "Workshop Topics:",
    "text": "Workshop Topics:\nWe will cover four topics:\n\nDay 1: Motivation, Text Representation, and Introduction to Deep Learning\nDay 2: Word embeddings\nDay 3: Trasformer models\nDay 4: Large Language Models."
  },
  {
    "objectID": "slides/day_1.html#today",
    "href": "slides/day_1.html#today",
    "title": "Advanced Text-As-Data  Winter School - Iesp UERJ",
    "section": "Today",
    "text": "Today\nToday, we will cover:\n\nText Representation.\nRepresentation Learning & Distributional Semantics\nIntroduction to Deep Learning\nCoding Practice:\n\nNeural Networks from Scratch."
  },
  {
    "objectID": "slides/day_1.html#estimation-how-do-get-good-parameters",
    "href": "slides/day_1.html#estimation-how-do-get-good-parameters",
    "title": "Advanced Text-As-Data  Winter School - Iesp UERJ",
    "section": "Estimation: How do get good parameters?",
    "text": "Estimation: How do get good parameters?\nTo estimate the parameters, we will use a algorithm called Gradient Descent. As in any other estimation, we start definition\nLinear Regression: MSE\n\\[\\text{RSS} = \\sum_{i=1}^{n} (y_i - \\widehat{y}_i)^2 \\]\nLogistic Regression: Negative Log Likelihood of a Bernouli distribution\n\\[L(\\beta) = -\\sum_{i=1}^{n} \\left( y_i \\log(\\widehat{y}_i) + (1-y_i)\\log(1 - \\widehat{y}_i) \\right)\\]\nFully-Connected Neural Network (Classification, Binary): bBinary Cross Entropy\n\\[L(\\mathbf{W}) = -\\frac{1}{n}\\sum_{i=1}^{n} \\left( y_i \\log(\\widehat{y}_i) + (1-y_i)\\log(1 - \\widehat{y}_i) \\right)\\]"
  },
  {
    "objectID": "code/day_1_logistic_as_neural_network.html#deep-layer-neural-network.",
    "href": "code/day_1_logistic_as_neural_network.html#deep-layer-neural-network.",
    "title": "Logistic Regression with a Neural Network mindset",
    "section": "Deep layer neural network.",
    "text": "Deep layer neural network.\nNow you can expand what we have so far to a more complex/deeper neural networks. Let’s design a three layer neural network, with two hidden layers and a activation layer. This is what the model looks like:\n\n\nStep 1: define activation function\n\nimport numpy as np\n\n# Activation functions\ndef sigmoid(Z):\n    return 1 / (1 + np.exp(-Z))\n\ndef relu(Z):\n    return np.maximum(0, Z)\n\ndef relu_derivative(Z):\n    return Z &gt; 0\n\n\n\nStep 2: initialize the parameters\nHere is critical for you to get the dimensions of your matrices right.\nThe rule here is that your matrix will have rows equal to the number of neurons in the layers, and columns equal to number of neurons of the previous layers\nFor example:\n\nW1 matrix: n rows will be equal to the number of neurons on the second layer, and columns number of neurons (features/dimensions) of your observations.\n\n\n# Initialize parameters for 3-layer NN\ndef initialize_parameters(n_x, n_h1, n_h2, n_y):\n    np.random.seed(1)\n    W1 = np.random.randn(n_h1, n_x) * 0.01\n    b1 = np.zeros((n_h1, 1))\n    W2 = np.random.randn(n_h2, n_h1) * 0.01\n    b2 = np.zeros((n_h2, 1))\n    W3 = np.random.randn(n_y, n_h2) * 0.01\n    b3 = np.zeros((n_y, 1))\n    \n    parameters = {\"W1\": W1, \"b1\": b1,\n                  \"W2\": W2, \"b2\": b2,\n                  \"W3\": W3, \"b3\": b3}\n    return parameters\n\n\nForward Propagation\nThis is just the standard matrix multiplication + activation.\n\nLayer 1: A1 = relu(W1*X + b1)\nLayer 2: A2 = relu(W2 * A1 + b2)\nLayer 3: Y = Sigmoid (W3 * A3 + b3)\n\nNotice, the final layer is just the output. It means W3 will be a matrix of form (1, Dimension Previous Layer)\n\n# Forward propagation\ndef forward_propagation(X, parameters):\n    W1, b1 = parameters['W1'], parameters['b1']\n    W2, b2 = parameters['W2'], parameters['b2']\n    W3, b3 = parameters['W3'], parameters['b3']\n\n    Z1 = np.dot(W1, X) + b1\n    A1 = relu(Z1)\n    Z2 = np.dot(W2, A1) + b2\n    A2 = relu(Z2)\n    Z3 = np.dot(W3, A2) + b3\n    A3 = sigmoid(Z3)\n\n    cache = (Z1, A1, Z2, A2, Z3, A3)\n    return A3, cache\n\n\n\nTraining\nFrom here next, it is standard training.\n\nCompute Cost\nCalculate Derivative\nUpdate\nIterate many times until convergence\n\n\n# Compute cost\ndef compute_cost(A3, Y):\n    m = Y.shape[1]\n    cost = -1/m * np.sum(Y * np.log(A3) + (1 - Y) * np.log(1 - A3))\n    return np.squeeze(cost)\n\n# Backward propagation\ndef backward_propagation(X, Y, parameters, cache):\n    m = X.shape[1]\n    W1, W2, W3 = parameters['W1'], parameters['W2'], parameters['W3']\n    Z1, A1, Z2, A2, Z3, A3 = cache\n\n    dZ3 = A3 - Y\n    dW3 = 1/m * np.dot(dZ3, A2.T)\n    db3 = 1/m * np.sum(dZ3, axis=1, keepdims=True)\n\n    dA2 = np.dot(W3.T, dZ3)\n    dZ2 = dA2 * relu_derivative(Z2)\n    dW2 = 1/m * np.dot(dZ2, A1.T)\n    db2 = 1/m * np.sum(dZ2, axis=1, keepdims=True)\n\n    dA1 = np.dot(W2.T, dZ2)\n    dZ1 = dA1 * relu_derivative(Z1)\n    dW1 = 1/m * np.dot(dZ1, X.T)\n    db1 = 1/m * np.sum(dZ1, axis=1, keepdims=True)\n\n    grads = {\"dW1\": dW1, \"db1\": db1,\n             \"dW2\": dW2, \"db2\": db2,\n             \"dW3\": dW3, \"db3\": db3}\n    \n    return grads\n\n# Update parameters\ndef update_parameters(parameters, grads, learning_rate):\n    for l in range(1, 4):\n        parameters[\"W\" + str(l)] -= learning_rate * grads[\"dW\" + str(l)]\n        parameters[\"b\" + str(l)] -= learning_rate * grads[\"db\" + str(l)]\n    return parameters\n\n\n\n\nRun the Model\n\n# Model function\ndef model_deep(X, Y, n_h1, n_h2, X_test, Y_test, num_iterations=10000, learning_rate=0.01, print_cost=False):\n    np.random.seed(3)\n    n_x = X.shape[0]\n    n_y = Y.shape[0]\n\n    parameters = initialize_parameters(n_x, n_h1, n_h2, n_y)\n\n    for i in range(0, num_iterations):\n        A3, cache = forward_propagation(X, parameters)\n        cost = compute_cost(A3, Y)\n        grads = backward_propagation(X, Y, parameters, cache)\n        parameters = update_parameters(parameters, grads, learning_rate)\n\n        if print_cost and i % 1000 == 0:\n            print(f\"Cost after iteration {i}: {cost}\")\n    \n    # define predict function\n    print(\"getting accuracy\")\n    def predict(X_input, parameters):\n        A3, _ = forward_propagation(X_input, parameters)\n        predictions = A3 &gt; 0.5  # threshold at 0.5\n        return predictions.astype(int)\n\n    # Make predictions\n    print(X.shape)\n    Y_prediction_train = predict(X, parameters)\n    Y_prediction_test = predict(X_test, parameters)\n\n    # Compute accuracy\n    train_accuracy = 100 - np.mean(np.abs(Y_prediction_train - Y)) * 100\n    test_accuracy = 100 - np.mean(np.abs(Y_prediction_test - Y_test)) * 100\n\n    print(f\"Train accuracy: {train_accuracy:.2f} %\")\n    print(f\"Test accuracy: {test_accuracy:.2f} %\") \n\n    return parameters\n\n\n# Training\npar = model_deep(train_set_x, train_set_y, 128, 64,test_set_x, test_set_y,\n               num_iterations=10000, print_cost = True)\n\nCost after iteration 0: 0.6928232758641192\nCost after iteration 1000: 0.5556033301345353\nCost after iteration 2000: 0.10869684290403103\nCost after iteration 3000: 0.00901327220282444\nCost after iteration 4000: 0.0034411465825281607\nCost after iteration 5000: 0.001959035748986317\nCost after iteration 6000: 0.0013197818853752836\nCost after iteration 7000: 0.000974800703933675\nCost after iteration 8000: 0.0007629026553648688\nCost after iteration 9000: 0.0006211710517102307\ngetting accuracy\n(12288, 209)\nTrain accuracy: 100.00 %\nTest accuracy: 74.00 %\n\n\nsome overfitting…. this is a topic for a machine learning class!"
  }
]