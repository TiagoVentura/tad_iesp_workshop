---
title: "Advanced Text-As-Data - Winter School - Iesp UERJ"
subtitle: "<span style = 'font-size: 140%;'> <br> Days 3 and 4: Transformers, Pre-Trained Models, and LLMs"
author: "<span style = 'font-size: 120%;'> Professor: Sebasti√°n Vallejo </span>"
execute: 
  echo: false
  error: true
  cache: true
format:
  revealjs: 
    transition: slide
    background-transition: fade
    code-line-numbers: false
    width: 1200
    height: 800
    center: false
    slide-number: true
    incremental: false
    chalkboard: 
      buttons: false
    preview-links: auto
    footer: "Text-as-Data"
    theme: [simple, custom.scss]
editor_options: 
  chunk_output_type: console
---

# Transformers, Pre-Trained Models, and LLMs

## What to Expect?

In the last two days you learned some heavy matrix algebra, dot-product, sigmoid-function, mostly-black-magic-believe-in-me content. Now we will enter the paradox of complexity: the material we will learn today is so complex that it is best understood through intuition and experience. 

The material will be divided into three parts:

  - Transformers: lecture-heavy introduction to the Transformers architecture. 
  - Pre-trained models: applying the Transformers architecture to pre-trained models and implementing it (code).
  - Large Language Models (LLMs): learn about Large Language models (LLMs), their applications and limitations, and how to implement LLMs (code).

## Material

Here is where you can find all the lectures and code we will be using during the next two days:

1. Slides for "Introduction to Transformers": [link.](https://github.com/svallejovera/iesp-uerj/blob/main/IESP_Transformers.pdf)
2. Slides for "Pre-Trained Transformers-Based Models": [link.](https://github.com/svallejovera/iesp-uerj/blob/main/IESP_Pre_Trained_Models.pdf)
3. Code for:
    i) Using Fine-Tuned Models: link.
    ii) Fine-Tuning a Model: link.
    iii) Further Pre-Training a Model: link. 
    iv) Other applications: link. 
  
---

4. Slides for "A Primer on Large Language Models": link. 
5. Code for:
    i) Using LLMs: link. 
    ii) LLMs as Annotators: link. 
  
