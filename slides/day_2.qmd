---
title: "Advanced Text-As-Data - Winter School - Iesp UERJ"
subtitle: "<span style = 'font-size: 140%;'> <br> Day 2: Word Embeddings: Theory and Practice"
author: "<span style = 'font-size: 120%;'> Professor: Tiago Ventura </span>"
execute: 
  echo: false
  error: true
  cache: true
format:
  revealjs: 
    transition: slide
    background-transition: fade
    code-line-numbers: false
    width: 1200
    height: 800
    center: false
    slide-number: true
    incremental: false
    chalkboard: 
      buttons: false
    preview-links: auto
    footer: "Text-as-Data"
    theme: [simple, custom.scss]
editor_options: 
  chunk_output_type: console
---

## Plans for Today:

-   Word Embeddings

    -   Semantics, Distributional Hypothesis, Moving from Sparse to Dense Vectors

    -   Word2Vec Algorithm

        -   Mathematical Model

        -   Estimate with Neural Networks
        
        -   Estimate using co-occurance matrices

-   Practice

    -   Work through: 
    
        -   Estimating Word Embeddings
    
        -   Working with pre-trained models

-   Discuss Word Embeddings Applications

# Word Embeddings

## Vector Space Model: From yesterday

In the vector space model, we learned:

-   A document $D_i$ is represented as a collection of features $W$ (words, tokens, n-grams..)

-   Each feature $w_i$ can be place in a real line, then a document $D_i$ is a point in a $W$ dimensional space.

Embedded in this model, there is the idea we represent [words]{.red} as a [one-hot encoding]{.red}.

-   "cat": \[0,0, 0, 0, 0, 0, 1, 0, ....., V\] , on a V dimensional vector
-   "dog": \[0,0, 0, 0, 0, 0, 0, 1, ...., V\], on a V dimensional vector

**What these vectors look like?**

-   really sparse

-   vectors are orthogonal

-   no natural notion of similarity

# How can we embed some notion of meaning in the way we represent words?

## Distributional Semantics

> "you shall know a word by the company it keeps." J. R. Firth 1957

[Distributional semantics]{.red}: words that are used in the same contexts tend to be similar in their meaning.

::: incremental
-   How can we use this insight to build a word representation?

    -   Move from sparse representation to dense representation

    -   Represent words as vectors of numbers with high number of dimensions

    -   Each feature on this vectors embeds some information from the word (gender? noun? sentiment? stance?)

    -   Learn this representation from the unlabeled data.
:::

## Sparse vs Dense Vectors

**One-hot encoding / Sparse Representation:**

-   cat = $\begin{bmatrix} 0,0, 0, 0, 0, 0, 1, 0, 0 \end{bmatrix}$

-   dog = $\begin{bmatrix} 0,0, 0, 0, 0, 1, 0, 0, 0 \end{bmatrix}$

**Word Embedding / Dense Representation:**

-   cat = $\begin{bmatrix} 0.25, -0.75, 0.90, 0.12, -0.50, 0.33, 0.66, -0.88, 0.10, -0.45 \end{bmatrix}$

-   dog = $\begin{bmatrix} 0.25, 1.75, 0.90, 0.12, -0.50, 0.33, 0.66, -0.88, 0.10, -0.45 \end{bmatrix}$

## With colors and real word vectors

```{r echo=FALSE, out.width = "100%"}
knitr::include_graphics("./week7_figs/embed_color.png") 
```

Source: [Illustrated Word2Vec](https://jalammar.github.io/illustrated-word2vec/)

## Why Word Embeddings?

**Encoding similarity:** vectors are not ortogonal anymore!

**Automatic Generalization:** learn about one word allow us to automatically learn about related words

**Encodes Meaning:** by learning the context, I can learn what a word means.

**As a consequence:**

-   Word Embeddings improves several NLP/Text-as-Data Tasks.

-   Allows to deal with unseen words.

-   Form the core idea of state-of-the-art models, such as LLMs.

## Estimating Word Embeddings

### Approches:

::: fragment
-   [Count-based methods]{.red}: look at how often words co-occur with neighbors.
    -   use this matrix, and some some factorization to retrieve vectors for the words
    -   This approach is called "GloVE" algorithm
    -   fast, not computationally intensive, but not the best representation
    -   we will see code doing this next week
:::

::: fragment
-   [Neural Networks:]{.red} rely on the idea of **self-supervision**
    -   use unlabeled data and use words to predict sequence
    -   the famous word2vec algorithm
        -   Skipgram: predicts context words
        -   Continuous Bag of Words: predict center word
:::

## Word2Vec: a framework for learning word vectors (Mikolov et al. 2013)

### Core Idea:

::: incremental

-   We have a large corpus ("body") of text: a long list of words

-   Every word in a fixed vocabulary is represented by a vector

-   Go through each position t in the text, which has a center word $c$ and context ("outside") words $t$

-   Use the similarity of the word vectors for $c$ and $t$ to calculate the probability of o given c (or vice versa)

-   Keep adjusting the word vectors to maximize this probability

    -   Neural Network + Gradient Descent
:::

## Skigram Example: Self-Supervision

```{r echo=FALSE, out.width = "70%"}
knitr::include_graphics("./week7_figs/skipgram_0.png") 
```

Source: [CS224N](https://web.stanford.edu/class/cs224n/index.html#schedule)

## Skigram Example: Self-Supervision

```{r echo=FALSE, out.width = "70%"}
knitr::include_graphics("./week7_figs/skipgram.png") 
```

Source: [CS224N](https://web.stanford.edu/class/cs224n/index.html#schedule)

## Encoding Similarity

To estimate the model, we first need to formalize the probability function we want to estimate.

::: fragment
#### This is similar to a [logistic regression]{.red}
:::

::: fragment
-   **In logistic regression: probability of a event occur given data X and parameters** $\beta$.:
    -   \$ P(y=1\| X, \beta ) = X \* $\beta$ \$

    -   $X*\beta$ is not a proper probability function, so we make it to proper probability by using a logit transformation.

    -   $P(y=1|X, \beta ) = \frac{exp(XB)}{1 + exp(XB)}$

    -   Throw this transformation inside of a bernouilli distribution, get the likelihood function, and find the parameters using MLE.
:::

## Pametrizing $P(w_t|w_{t-1})$

-   $P(w_t|w_{t-1})$ must be condition on how similar these words are.
    -   [Exactly the same]{.red} intuition behind placing documents in the vector space model.
    -   Now words are vectors!
    -   $P(w_t|w_{t-1}) = u_c \cdot u_t$
        -   $u_c \cdot u_t$
        -   dot product between vectors
        -   measures similarity using vector projection
        -   $u_c$: center vector
        -   $u_t$: target vectors
-   $u_c \cdot u_t$ is also not a proper probability distribution: Logit on them!

$$P(w_t|w_{t-1}) = \frac{exp(u_c \cdot u_t)}{{\sum_{w}^V exp(u_c*u_w)}}$$

## Softmax Transformation

$$P(w_t|w_{t-1}) = \frac{exp(u_c \cdot u_t)}{{\sum_{w}^V exp(u_c*u_w)}}$$

-   Dot product compares similarity between vectors

-   numerator: center vs target vectors

-   exponentiation makes everything positive

-   Denominator: normalize over entire vocabulary to give probability distribution

-   What is the meaning of softmax?

    -   max: assign high values to be 1

    -   soft: still assigns some probability to smaller values

    -   generalization of the logit \~ multinomial logistic function.

## Word2Vec: Objective Function

::: fragment
For each position $t$, predict context words within a window of fixed size $m$, given center word $w$.

#### Likelihood Function

$$ L(\theta) = \prod_{t=1}^{T} \prod_{\substack{-m<= j<=m \\ j \neq 0}}^{m} P(w_{t+j} | w_t; \theta) $$

-   Assuming independence, this means you multiplying the probability of every target for every center word in your dictionary.

-   This likelihood function will change if you do skipgram with negative sampling (See SLP chapter 6)
:::

::: fragment
#### Objective Function: Negative log likelihood

$$J(\theta) = - \frac{1}{T}log(L(\theta))$$

-   better to take the gradient with sums

-   the average increases the numerical stability of the gradient.

:::

## Neural Networks

```{r echo=FALSE, out.width = "70%"}
knitr::include_graphics("./week7_figs/nn.png") 
```

## Skipgram Architecture

```{r echo=FALSE, out.width = "70%"}
knitr::include_graphics("./week7_figs/sg_arc.png") 
```


##

```{r echo=FALSE, out.width = "100%"}
knitr::include_graphics("./week7_figs/rustic.png") 
```


## Check your matrices

::: columns
::: {.column width="70%"}
```{r echo=FALSE, out.width = "100%"}
knitr::include_graphics("./week7_figs/skipgram_matrices.png") 
```
:::

::: {.column width="30%"}
<br> <br> <br>

Practice with a vocabulary of size 5, a embedding with 3 dimensions, and the task is to predict the next word.

-   **Step 1: v_1\^5 \* W_5\^3**

-   **Step 2: w_1\^3 \* C_3\^5**

-   **Step 3: Softmax entire vector**
:::
:::

## Word Embeddings Matrices

```{r echo=FALSE, out.width = "70%"}
knitr::include_graphics("./week7_figs/matrices.png") 
```

## Training Embeddings

Embeddings need quite a lot of text to train: e.g. want to disambiguate meanings from contexts. You can download [pre-trained]{.red}, or get the code and [train locally]{.red}

-   [Word2Vec]{.red} is trained on the Google News dataset (∼ 100B words, 2013)

-   [GloVe]{.red} are trained on different things: Wikipedia (2014) + Gigaword (6B words), Common Crawl, Twitter. And uses a co-occurence matrix instead of Neural Networks

-   [fastext]{.red} from facebook

## Applications:

Once we've optimized, we can extract the word specific vectors from W as embedding vectors. These real valued vectors can be used for analogies and related tasks

```{r echo=FALSE, out.width = "80%", fig.align="center"}
knitr::include_graphics("./week7_figs/king.png") 
```

We will see several applications next week. Most important:

-   Alternative to bag-of-words feature representation in supervised learning tasks

-   Support for other automated text analysis tasks: expand dictionaries

-   Understanding word meaning: variation over time, bias, variation by groups

-   as a scaling method 


## Applications

Let's discuss now several applications of embeddings on social science papers. These paper show:

-   How to use embeddings to track semantic changes over time

-   How to use embeddings to measure emotion in political language.

-   How to use embeddings to measure gender and ethnic stereotypes

-   And a favorite of political scientists, how to use embeddings to measure ideology.


## Capturing cultural dimensions with embeddings

> Austin C. Kozlowski, Austin C., Matt Taddy, and James A. Evans. 2019. "The Geometry of Culture: Analyzing the Meanings of Class through Word Embeddings." American Sociological Review 84, no. 5: 905--49. https://doi.org/10.1177/0003122419877135.

-   Word Embeddings can be use to capture cultural dimensions

-   Dimensions of word embedding vector space models closely correspond to meaningful "cultural dimensions," such as rich-poor, moral-immoral, and masculine-feminine.

-   a word vector's position on these dimensions reflects the word's respective cultural associations


## Method

```{r echo=FALSE, out.width = "100%", fig.align="center"}
knitr::include_graphics("cultural_vector.jpg") 
```

## Results

```{r echo=FALSE, out.width = "100%", fig.align="center"}
knitr::include_graphics("./week9_figs/culture_1.png") 
```


## Ideological Scaling

> Rheault, Ludovic, and Christopher Cochrane. "Word embeddings for the analysis of ideological placement in parliamentary corpora." Political Analysis 28, no. 1 (2020): 112-133.

-   Can word vectors be used to produce scaling estimates of ideological placement on political text?

    -   Yes, and word vectors are even better

        -   It captures semantics

        -   No need of training data (self-supervision)

## Method

```{r echo=FALSE, out.width = "100%"}
knitr::include_graphics("./week9_figs/scaling_00.png") 
```


## Results

::: columns
::: {.column width="50%"}
```{r echo=FALSE, out.width = "100%"}
knitr::include_graphics("./week9_figs/scaling_01.png") 
```
:::

::: {.column width="50%"}
```{r echo=FALSE, out.width = "100%"}
knitr::include_graphics("./week9_figs/scaling_02.png") 
```
:::
:::

## Measuring Emotion

> Gennaro, Gloria, and Elliott Ash. "Emotion and reason in political language." The Economic Journal 132, no. 643 (2022): 1037-1059.

```{r echo=FALSE, out.width = "90%"}
knitr::include_graphics("./week9_figs/emotion.png") 
```

## Method

- Building seed lists: They start with small seed lists of words clearly associated with “emotion” and “reason”

- Expanding dictionaries with word embeddings: Instead of just using these short lists, they expand them automatically using word embeddings. 

- Emotionality Score: 

$$
Y_i = \frac{\text{sim}(\vec{d}_i, \vec{A}) + b}{\text{sim}(\vec{d}_i, \vec{C}) + b}
$$
# Coding