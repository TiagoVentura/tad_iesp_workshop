{"title":"Advanced Text-As-Data - Winter School - Iesp UERJ","markdown":{"yaml":{"title":"Advanced Text-As-Data - Winter School - Iesp UERJ","subtitle":"<span style = 'font-size: 140%;'> <br> Day 1: Word Representation & Introduction to Neural Networks","author":"<span style = 'font-size: 120%;'> Professor: Tiago Ventura </span>","execute":{"echo":false,"error":true,"cache":true},"format":{"revealjs":{"transition":"slide","background-transition":"fade","code-line-numbers":false,"width":1200,"height":800,"center":false,"slide-number":true,"incremental":true,"chalkboard":{"buttons":false},"preview-links":"auto","footer":"Text-as-Data","theme":["simple","custom.scss"]}},"editor_options":{"chunk_output_type":"console"}},"headingText":"Welcome to Advanced Text-as-Data: Introduction","containsRefs":false,"markdown":"\n\n\n## Logistics\n\n- All materials will be on this website: \n\n- Syllabus is more dense than we can cover in a week. Take your time to work through it!\n\n- Our time: Lectures in the Afternoon, and code/exercises for you to work through at your time. \n\n- This is an advanced class in text-as-data. It requires:\n\n  - some background in programing\n  \n  - some notion of probability theory, particularly maximum likelihood estimation (LEGO III at IESP)\n  \n- You have a TA: Felipe Lamarca. Use him for your questions!\n\n- Classes in English, but feel free to ask questions in portuguese if you prefer!\n\n# Motivation\n\n## Rise of the digital information age\n\n```{r fig.align=\"center\"}\nknitr::include_graphics(\"figs/digital.jpeg\") \n```\n\n## Official Documents: Congressional Speeches, Bills, Press Releases, Transcripts, from all over the world!!\n\n```{r  echo=FALSE, out.width = \"70%\", fig.align=\"center\"}\nknitr::include_graphics(\"https://www.brookings.edu/wp-content/uploads/2018/11/RTS10VM4.jpg\") \n```\n\n## Social Media\n\n```{r  echo=FALSE, out.width = \"80%\", fig.align=\"center\"}\nknitr::include_graphics(\"week1_figs/redes.png\") \n```\n\n## The internet: News, Comments, Blogs, etc...\n\n```{r  echo=FALSE, out.width = \"80%\", fig.align=\"center\"}\nknitr::include_graphics(\"https://miro.medium.com/v2/resize:fit:1400/1*t2u_0FHoQSpBK8i7j1NOBg.png\") \n```\n\n## What is this class about?\n\n-   For many years, social scientists uses text in their empirical analysis:\n\n   - Close reading of documents.\n   - Qualitative Analysis of interviews\n   - Content Analysis\n   \n-   Digital Revolution:\n\n    -   Production of text increased\n    -   The computational capacity to analyze them at scale as well.\n    -   Powerful computational models became easily accessible. \n-   This class covers methods (and many applications) of using Text as Data to [answer social science problems]{.red} and [test social science theories]{.red}\n\n- But... with an modern flavor. \n   \n   - Deep Learning Revolution / Large Language Models / Representation Learning. \n\n# Text Representation: From Sparse to Dense Vector\n\n## Vector Space Model\n\nTo represent documents as numbers, we will use the [vector space model representation]{.red}:\n\n-   A document $D_i$ is represented as a collection of features $W$ (words, tokens, n-grams..)\n\n-   Each feature $w_i$ can be place in a real line, then a document $D_i$ is a point in a $W$ dimensional space\n\n::: fragment\nImagine the sentence below: *\"If that is a joke, I love it. If not, can't wait to unpack that with you later.\"*\n\n-   **Sorted Vocabulary** =(a, can't, i, if, is, it, joke, later, love, not, that, to, unpack, wait, with, you\")\n\n-   **Feature Representation** = (1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1)\n\n-   Features will typically be the n-gram (mostly unigram) [frequencies]{.red} of the tokens in the document, or some [function]{.red} of those frequencies\n:::\n\n::: fragment\nNow each document is now a vector (vector space model)\n\n-   stacking these vectors will give you our workhose representation for text: [**Document Feature Matrix**]{.red}\n:::\n\n## Visualizing Vector Space Model\n\n::: {.callout-note icon=\"false\"}\n## Documents\n\nDocument 1 = \"yes yes yes no no no\"\n\nDocument 2 = \"yes yes yes yes yes yes\"\n:::\n\n```{r echo=FALSE, fig.align=\"center\",fig.width=8}\n# Load necessary libraries\npacman::p_load(ggplot, tidyverse)\n\n# Define a simple vocabulary of two words\n\n# Sample texts using only words from the vocabulary\ndocument1 <- c(\"yes\", \"yes\", \"yes\", \"no\", \"no\", \"no\")\ndocument2 <- c(\"no\", \"no\", \"no\", \"no\", \"no\", \"no\")\n\n# Convert the documents to a dataframe to a data frame for ggplot\ndf <- tibble(document1, document2) %>% \n       pivot_longer(cols=contains(\"document\"), names_to = \"document\", values_to = \"word\") %>%\n       group_by(document, word) %>%\n       count() %>%\n       pivot_wider(names_from=word, values_from=n, values_fill = 0)\n\n# Plot the documents in 2D space using ggplot\nggplot(df, aes(x = no, y = yes, label = document)) +\n  geom_label(nudge_y =.5) +\n  geom_segment(aes(x = 0, y = 0, xend = 3, yend = 3), \n               arrow = arrow(), \n               size=1, color=\"navy\") +\n  geom_segment(aes(x = 0, y = 0, xend = 6, yend = 0),  \n               arrow = arrow(), \n               size=1, color=\"navy\") +\n  xlim(0, 7) +\n  xlab(\"Frequency of 'yes'\") +\n  ylab(\"Frequency of 'no'\") +\n  ggtitle(\"Vector Representation of Texts\") +\n  theme_minimal()\n```\n\n## Visualizing Vector Space Model\n\nIn the vector space, we can use geometry to build well-defined comparison measures between the documents \n\n```{r echo=FALSE, fig.align=\"center\",fig.width=8}\n\n# Plot the documents in 2D space using ggplot\nggplot(df, aes(x = no, y = yes, label = document)) +\n  geom_point(size=2) +\n  geom_segment(aes(x = 0, y = 0, xend = 3, yend = 3), \n               arrow = arrow(), \n               size=1, color=\"navy\", alpha=.2) +\n  geom_segment(aes(x = 0, y = 0, xend = 6, yend = 0),  \n               arrow = arrow(), \n               size=1, color=\"navy\", alpha=.3) +\n geom_segment(aes(x = 3, y = 3, xend = 6, yend = 0),  \n               linetype=2, \n               size=1, color=\"tomato\", alpha=1) +\nannotate(geom=\"label\", x=5, y=1, label=\"Distance\",\n              color=\"black\")  +\n  xlim(0, 7) +\n  xlab(\"Frequency of 'yes'\") +\n  ylab(\"Frequency of 'no'\") +\n  ggtitle(\"Vector Representation of Texts\") +\n  theme_minimal()\n```\n\n## 6. Document-Feature Matrix\n\n```{r echo=FALSE, out.width = \"80%\", fig.align=\"center\"}\nknitr::include_graphics(\"figs/DTM.png\") \n```\n\n::: aside\nSource: [Arthur Spirling TAD Class](https://github.com/ArthurSpirling/text-as-data-class-spring2021)\n:::\n\n## Vector Space Model vs Representation Learning\n\nThis vector space representation is super useful, and has been used in many many many applications in computational linguistics and social science applications of text-as-data. Including:\n\n- Descriptive statistics of documents (count words, text-similarity, complexity, etc..)\n\n- Supervised Machine Learning Models for Text Classification (DFM becomes the input of the models)\n\n- Unsupervised Machine Learning (Topic Models & Clustering)\n\n**But**... Embedded in this model, there is the idea we represent [words]{.red} as a [one-hot encoding]{.red}.\n\n-   \"cat\": \\[0,0, 0, 0, 0, 0, 1, 0, ....., V\\] , on a V dimensional vector\n-   \"dog\": \\[0,0, 0, 0, 0, 0, 0, 1, ...., V\\], on a V dimensional vector\n\n**What these vectors look like?**\n\n-   really sparse\n\n-   those vectors are orthogonal\n\n-   no natural notion of similarity\n\n# How can we embed some notion of meaning in the way we represent words?\n\n## Distributional Semantics\n\n> \"you shall know a word by the company it keeps.\" J. R. Firth 1957\n\n[Distributional semantics]{.red}: words that are used in the same contexts tend to be similar in their meaning.\n\n::: incremental\n-   How can we use this insight to build a word representation?\n\n    -   Move from sparse representation to dense representation\n\n    -   Represent words as vectors of numbers with high number of dimensions\n\n    -   Each feature on this vectors embeds some information from the word (gender? noun? sentiment? stance?)\n\n    -   Learn this representation from the unlabeled data.\n:::\n\n## Sparse vs Dense Vectors\n\n**One-hot encoding / Sparse Representation:**\n\n-   cat = $\\begin{bmatrix} 0,0, 0, 0, 0, 0, 1, 0, 0 \\end{bmatrix}$\n\n-   dog = $\\begin{bmatrix} 0,0, 0, 0, 0, 1, 0, 0, 0 \\end{bmatrix}$\n\n**Word Embedding / Dense Representation:**\n\n-   cat = $\\begin{bmatrix} 0.25, -0.75, 0.90, 0.12, -0.50, 0.33, 0.66, -0.88, 0.10, -0.45 \\end{bmatrix}$\n\n-   dog = $\\begin{bmatrix} 0.25, 1.75, 0.90, 0.12, -0.50, 0.33, 0.66, -0.88, 0.10, -0.45 \\end{bmatrix}$\n\n## With colors and real word vectors\n\n```{r echo=FALSE, out.width = \"70%\"}\nknitr::include_graphics(\"./week7_figs/embed_color.png\") \n```\n\nSource: [Illustrated Word2Vec](https://jalammar.github.io/illustrated-word2vec/)\n\n\n## Word Embeddings\n\n**Encoding similarity:** vectors are not ortogonal anymore!\n\n**Automatic Generalization:** learn about one word allow us to automatically learn about related words\n\n**Encodes Meaning:** by learning the context, I can learn what a word means.\n\n**As a consequence:**\n\n-   Word Embeddings improves by ORDERS OF MAGNITUDE several Text-as-Data Tasks.\n\n-   Allows to deal with unseen words.\n\n-   Form the core idea of state-of-the-art models, such as LLMs.\n\n## Deep Learning for Text Analysis\n\n```{r echo=FALSE, out.width = \"70%\"}\nknitr::include_graphics(\"./figs/deep_learning.png\") \n```\n\n# Introduction to Deep Learning\n\n## Basics of Machine Learning\n\n**Deep Learning** is a subfield of machine learning based on using neural networks models to learn.\n\nAs in any other statistical model, the goal of machine learning is to use data to learn about some output. \n\n$$ y = f(X) + \\epsilon$$\n\n- **Where:**\n\n -   $y$ is the outcome/dependent/response variable\n\n -   $X$ is a matrix of predictors/features/independent variables\n\n -   $f()$ is some fixed but unknown function mapping X to y. The \"signal\" in the data\n\n -   $\\epsilon$ is some random error term. The \"noise\" in the data.\n\n\n## Linear models (OLS)\n\nThe simplest model we can use is an linear model (the classic OLS regression)\n\n$$ y = b_0 + WX + \\epsilon$$\n\n- Where: \n\n  - $W$ is a vector of dimension p, \n  - $X$ is the feature vector of dimension p\n  - $b$ is a bias term (intercept)\n\n## Using Matrix Algebra\n\n$$\\mathbf{W} = \\begin{bmatrix} w_1 & w_2 & \\dots & w_p\\end{bmatrix}$$\n\n\n$$\\mathbf{X} = \\begin{bmatrix} X_1 \\\\ X_2 \\\\ X_3 \\\\ \\vdots \\\\ X_p \\end{bmatrix}$$\n\nWith matrix multiplication:\n\n$$\\mathbf{W} \\mathbf{X} + b = w_1 X_1 + w_2 X_2 + \\dots + w_p X_p + b$$\n\n## Logistic Regression\n\nIf we want to model some type of non-linearity, necessary for example when our outcome is binary, we can add a transformation function to make things non-linear:\n\n$$ y = \\sigma (b_0 + WX + \\epsilon)$$\n\nWhere: \n\n$$ \\sigma(b_0 + WX + \\epsilon) = \\frac{1}{1 + \\exp(-b_0 + WX + \\epsilon)}$$\n\n```{r echo=FALSE, out.width = \"100%\"}\nknitr::include_graphics(\"./figs/sigmoid.png\") \n```\n\n## Logistic Regression as a Neural Network\n\nLet's assume we have a simple model of voting. We want to predict if individual $i$ will vote ($y=1$), and we will use four socio demographic factors to make this prediction. \n\nClassic statistical approach with logistic regression: \n\n$$ \\hat{P(Y_i=1|X)} = \\sigma(b_0 + WX + \\epsilon) $$\nWe use MLE (Maximum Likelihood estimation) to find the parameters $W$ and $b_0$. We assume:\n\n$$ Y_i \\sim \\text{Bernoulli}(\\pi_i) $$\n\nThe likelihood function for $n$ independent observations is:\n\n$$L(W, b_0) = \\prod_{i=1}^{n} \\pi_i^{y_i} (1 - \\pi_i)^{1 - y_i}.$$\n\n\n## Neural Network Graphical Representation\n\n```{r echo=FALSE, out.width = \"100%\"}\nknitr::include_graphics(\"./figs/nn.png\") \n```\n\n::: aside\nSource: https://carpentries-incubator.github.io/ml4bio-workshop/05-logit-ann/index.html\n:::\n\n## Neural Networks\n\nA **Neural Network** is equivalent to stacking multiple logistic regressions vertically and repeat this process multiple times across many layers.\n\nAs a Matrix, instead of: \n\n$$\\mathbf{W}_{previous} = \\begin{bmatrix} w_1 & w_2 & \\dots & w_p\\end{bmatrix}$$\n\n\nWe use this set of parameters: \n\n$$\n\\mathbf{W} = \\begin{bmatrix} w_{11} & w_{12} & w_{13} & \\dots & w_{1p} \\\\\n                               w_{21} & w_{22} & w_{23} & \\dots & w_{2p} \\\\\n                               w_{31} & w_{32} & w_{33} & \\dots & w_{3p} \\\\\n                               \\vdots & \\vdots & \\vdots & \\vdots & \\vdots \\\\\n                               w_{k1} & w_{k2} & w_{k3} & \\dots & w_{kp}\n                \\end{bmatrix}\n$$                \n\n\n## \n\nThen, every line becomes a different logistic regression\n\n$$\\mathbf{WX} = \\begin{bmatrix}  \\sigma(w_{11} X_1 + w_{12} X_2 + \\dots + w_{1p}X_p) \\\\ \\sigma(w_{21} X_1 + w_{22} X_2 + \\dots + w_{2p}X_p) \\\\ \\sigma(w_{31} X_1 + w_{32} X_2 + \\dots + w_{3p}X_p )\\\\ \\vdots \\\\ \\sigma(w_{k1} X_1 + w_{k2} X_2 + \\dots + w_{kp}X_p) \\end{bmatrix}$$\nWe then combine all of those with another set of parameters: \n\n$$\n\\begin{align*}\n    \\mathbf{HA} &= h1 \\cdot \\sigma(w_{11} X_1 + w_{12} X_2 + \\dots + w_{1p}X_p)\\\\\n                &+ h_2\\cdot \\sigma(w_{21} X_1 + w_{22} X_2 + \\dots + w_{2p}X_p)\\\\\n                &+ h_3 \\cdot \\sigma (w_{31} X_1 + w_{32} X_2 + \\dots + w_{3p}X_p)\\\\\n                &+ \\dots + h_k + \\sigma(w_{k1} X_1 + w_{k2} X_2 + \\dots + w_{kp}X_p)\n\\end{align*}\n$$\n\n## Feed Forward on a Deep Neural Network\n\n```{r echo=FALSE, out.width = \"100%\"}\nknitr::include_graphics(\"./figs/feed_forward.png\") \n```\n\n## Key Components \n\n- Input: p features (the original data) of an observation are linearly transformed into k1 features using a weight matrix of size k1×p\n\n- Embedding Matrices: Paremeters you multiply your data by. \n\n- Neurons: Number of dimensions on your embedding matrices\n\n- Hidden Layer: the transformation that consists of the linear transformation and an activation function\n\n- Output Layer: the transformation that consists of the linear transformation and then (usually) a sigmoid (or some other activation function) to produce the final output predictions\n\n##\n\n![](https://media4.giphy.com/media/v1.Y2lkPTc5MGI3NjExYmY4dWFlNDgzM3FiY3RnM3ZweXZyejBpcjM2c291cWduZnhoYjVudCZlcD12MV9pbnRlcm5hbF9naWZfYnlfaWQmY3Q9Zw/lJnAXeJO8tE7E37mxq/giphy.gif)\n\n\n## Let's take a step back! \n\nWeights matrices are just parameters... the $\\beta_s$ of our regression models. \n\n- There are MANY of them!! Neural Networks are a black box!\n\n- But... they also serve as way to project features (words) in a dense dimensional space.\n\n- Remember: \n\n   - **One-hot encoding / Sparse Representation:**\n\n      -   cat = $\\begin{bmatrix} 0,0, 0, 0, 0, 0, 1, 0, 0 \\end{bmatrix}$\n\n      -   dog = $\\begin{bmatrix} 0,0, 0, 0, 0, 1, 0, 0, 0 \\end{bmatrix}$\n\n   - **Word Embedding / Dense Representation:**\n\n      -   cat = $\\begin{bmatrix} 0.25, -0.75, 0.90, 0.12, -0.50, 0.33, 0.66, -0.88, 0.10, -0.45 \\end{bmatrix}$\n\n      -   dog = $\\begin{bmatrix} 0.25, 1.75, 0.90, 0.12, -0.50, 0.33, 0.66, -0.88, 0.10, -0.45 \\end{bmatrix}$\n\n\n## Deep Neural Network for Textual Data\n\n```{r echo=FALSE, out.width = \"100%\"}\nknitr::include_graphics(\"./figs/nlp_nn.png\") \n```\n\n\n## Estimation: Gradient Descent\n\n**Linear Regression**\n\n$$\\text{RSS} = \\sum_{i=1}^{n} (y_i - \\widehat{y}_i)^2 $$\n\n\n**Logistic Regression**\n\n$$L(\\beta) = -\\sum_{i=1}^{n} \\left( y_i \\log(\\widehat{y}_i) + (1-y_i)\\log(1 - \\widehat{y}_i) \\right)$$\n\n**Fully-Connected Neural Network (Classification, Binary)**\n\n$$L(\\mathbf{W}) = -\\frac{1}{n}\\sum_{i=1}^{n} \\left( y_i \\log(\\widehat{y}_i) + (1-y_i)\\log(1 - \\widehat{y}_i) \\right)$$\n\nThis is called a binary cross entropy loss function\n\n##\n\n\n```{r echo=FALSE, out.width = \"100%\"}\nknitr::include_graphics(\"./figs/loss.png\") \n```\n\n\n## Updating parameters\n\n- Define a loss function\n\n- Gradient Descent Algorithm: \n\n   - Initialize weights randomly\n   \n   - Feed Forward: Matrix Multiplication + Activation function\n   \n   - Get the loss for this iteration\n   \n   - Compute gradient (partial derivatives): $\\frac{\\partial J(\\mathbf{W})}{\\partial \\mathbf{W}}$\n\n   \n   - Update weights: $W_{new}: W_{old} -\\eta \\cdot \\frac{\\partial J(\\mathbf{W})}{\\partial \\mathbf{W}}$\n\n   - Loop until convergence:\n\n# Code!","srcMarkdownNoYaml":"\n\n# Welcome to Advanced Text-as-Data: Introduction\n\n## Logistics\n\n- All materials will be on this website: \n\n- Syllabus is more dense than we can cover in a week. Take your time to work through it!\n\n- Our time: Lectures in the Afternoon, and code/exercises for you to work through at your time. \n\n- This is an advanced class in text-as-data. It requires:\n\n  - some background in programing\n  \n  - some notion of probability theory, particularly maximum likelihood estimation (LEGO III at IESP)\n  \n- You have a TA: Felipe Lamarca. Use him for your questions!\n\n- Classes in English, but feel free to ask questions in portuguese if you prefer!\n\n# Motivation\n\n## Rise of the digital information age\n\n```{r fig.align=\"center\"}\nknitr::include_graphics(\"figs/digital.jpeg\") \n```\n\n## Official Documents: Congressional Speeches, Bills, Press Releases, Transcripts, from all over the world!!\n\n```{r  echo=FALSE, out.width = \"70%\", fig.align=\"center\"}\nknitr::include_graphics(\"https://www.brookings.edu/wp-content/uploads/2018/11/RTS10VM4.jpg\") \n```\n\n## Social Media\n\n```{r  echo=FALSE, out.width = \"80%\", fig.align=\"center\"}\nknitr::include_graphics(\"week1_figs/redes.png\") \n```\n\n## The internet: News, Comments, Blogs, etc...\n\n```{r  echo=FALSE, out.width = \"80%\", fig.align=\"center\"}\nknitr::include_graphics(\"https://miro.medium.com/v2/resize:fit:1400/1*t2u_0FHoQSpBK8i7j1NOBg.png\") \n```\n\n## What is this class about?\n\n-   For many years, social scientists uses text in their empirical analysis:\n\n   - Close reading of documents.\n   - Qualitative Analysis of interviews\n   - Content Analysis\n   \n-   Digital Revolution:\n\n    -   Production of text increased\n    -   The computational capacity to analyze them at scale as well.\n    -   Powerful computational models became easily accessible. \n-   This class covers methods (and many applications) of using Text as Data to [answer social science problems]{.red} and [test social science theories]{.red}\n\n- But... with an modern flavor. \n   \n   - Deep Learning Revolution / Large Language Models / Representation Learning. \n\n# Text Representation: From Sparse to Dense Vector\n\n## Vector Space Model\n\nTo represent documents as numbers, we will use the [vector space model representation]{.red}:\n\n-   A document $D_i$ is represented as a collection of features $W$ (words, tokens, n-grams..)\n\n-   Each feature $w_i$ can be place in a real line, then a document $D_i$ is a point in a $W$ dimensional space\n\n::: fragment\nImagine the sentence below: *\"If that is a joke, I love it. If not, can't wait to unpack that with you later.\"*\n\n-   **Sorted Vocabulary** =(a, can't, i, if, is, it, joke, later, love, not, that, to, unpack, wait, with, you\")\n\n-   **Feature Representation** = (1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1)\n\n-   Features will typically be the n-gram (mostly unigram) [frequencies]{.red} of the tokens in the document, or some [function]{.red} of those frequencies\n:::\n\n::: fragment\nNow each document is now a vector (vector space model)\n\n-   stacking these vectors will give you our workhose representation for text: [**Document Feature Matrix**]{.red}\n:::\n\n## Visualizing Vector Space Model\n\n::: {.callout-note icon=\"false\"}\n## Documents\n\nDocument 1 = \"yes yes yes no no no\"\n\nDocument 2 = \"yes yes yes yes yes yes\"\n:::\n\n```{r echo=FALSE, fig.align=\"center\",fig.width=8}\n# Load necessary libraries\npacman::p_load(ggplot, tidyverse)\n\n# Define a simple vocabulary of two words\n\n# Sample texts using only words from the vocabulary\ndocument1 <- c(\"yes\", \"yes\", \"yes\", \"no\", \"no\", \"no\")\ndocument2 <- c(\"no\", \"no\", \"no\", \"no\", \"no\", \"no\")\n\n# Convert the documents to a dataframe to a data frame for ggplot\ndf <- tibble(document1, document2) %>% \n       pivot_longer(cols=contains(\"document\"), names_to = \"document\", values_to = \"word\") %>%\n       group_by(document, word) %>%\n       count() %>%\n       pivot_wider(names_from=word, values_from=n, values_fill = 0)\n\n# Plot the documents in 2D space using ggplot\nggplot(df, aes(x = no, y = yes, label = document)) +\n  geom_label(nudge_y =.5) +\n  geom_segment(aes(x = 0, y = 0, xend = 3, yend = 3), \n               arrow = arrow(), \n               size=1, color=\"navy\") +\n  geom_segment(aes(x = 0, y = 0, xend = 6, yend = 0),  \n               arrow = arrow(), \n               size=1, color=\"navy\") +\n  xlim(0, 7) +\n  xlab(\"Frequency of 'yes'\") +\n  ylab(\"Frequency of 'no'\") +\n  ggtitle(\"Vector Representation of Texts\") +\n  theme_minimal()\n```\n\n## Visualizing Vector Space Model\n\nIn the vector space, we can use geometry to build well-defined comparison measures between the documents \n\n```{r echo=FALSE, fig.align=\"center\",fig.width=8}\n\n# Plot the documents in 2D space using ggplot\nggplot(df, aes(x = no, y = yes, label = document)) +\n  geom_point(size=2) +\n  geom_segment(aes(x = 0, y = 0, xend = 3, yend = 3), \n               arrow = arrow(), \n               size=1, color=\"navy\", alpha=.2) +\n  geom_segment(aes(x = 0, y = 0, xend = 6, yend = 0),  \n               arrow = arrow(), \n               size=1, color=\"navy\", alpha=.3) +\n geom_segment(aes(x = 3, y = 3, xend = 6, yend = 0),  \n               linetype=2, \n               size=1, color=\"tomato\", alpha=1) +\nannotate(geom=\"label\", x=5, y=1, label=\"Distance\",\n              color=\"black\")  +\n  xlim(0, 7) +\n  xlab(\"Frequency of 'yes'\") +\n  ylab(\"Frequency of 'no'\") +\n  ggtitle(\"Vector Representation of Texts\") +\n  theme_minimal()\n```\n\n## 6. Document-Feature Matrix\n\n```{r echo=FALSE, out.width = \"80%\", fig.align=\"center\"}\nknitr::include_graphics(\"figs/DTM.png\") \n```\n\n::: aside\nSource: [Arthur Spirling TAD Class](https://github.com/ArthurSpirling/text-as-data-class-spring2021)\n:::\n\n## Vector Space Model vs Representation Learning\n\nThis vector space representation is super useful, and has been used in many many many applications in computational linguistics and social science applications of text-as-data. Including:\n\n- Descriptive statistics of documents (count words, text-similarity, complexity, etc..)\n\n- Supervised Machine Learning Models for Text Classification (DFM becomes the input of the models)\n\n- Unsupervised Machine Learning (Topic Models & Clustering)\n\n**But**... Embedded in this model, there is the idea we represent [words]{.red} as a [one-hot encoding]{.red}.\n\n-   \"cat\": \\[0,0, 0, 0, 0, 0, 1, 0, ....., V\\] , on a V dimensional vector\n-   \"dog\": \\[0,0, 0, 0, 0, 0, 0, 1, ...., V\\], on a V dimensional vector\n\n**What these vectors look like?**\n\n-   really sparse\n\n-   those vectors are orthogonal\n\n-   no natural notion of similarity\n\n# How can we embed some notion of meaning in the way we represent words?\n\n## Distributional Semantics\n\n> \"you shall know a word by the company it keeps.\" J. R. Firth 1957\n\n[Distributional semantics]{.red}: words that are used in the same contexts tend to be similar in their meaning.\n\n::: incremental\n-   How can we use this insight to build a word representation?\n\n    -   Move from sparse representation to dense representation\n\n    -   Represent words as vectors of numbers with high number of dimensions\n\n    -   Each feature on this vectors embeds some information from the word (gender? noun? sentiment? stance?)\n\n    -   Learn this representation from the unlabeled data.\n:::\n\n## Sparse vs Dense Vectors\n\n**One-hot encoding / Sparse Representation:**\n\n-   cat = $\\begin{bmatrix} 0,0, 0, 0, 0, 0, 1, 0, 0 \\end{bmatrix}$\n\n-   dog = $\\begin{bmatrix} 0,0, 0, 0, 0, 1, 0, 0, 0 \\end{bmatrix}$\n\n**Word Embedding / Dense Representation:**\n\n-   cat = $\\begin{bmatrix} 0.25, -0.75, 0.90, 0.12, -0.50, 0.33, 0.66, -0.88, 0.10, -0.45 \\end{bmatrix}$\n\n-   dog = $\\begin{bmatrix} 0.25, 1.75, 0.90, 0.12, -0.50, 0.33, 0.66, -0.88, 0.10, -0.45 \\end{bmatrix}$\n\n## With colors and real word vectors\n\n```{r echo=FALSE, out.width = \"70%\"}\nknitr::include_graphics(\"./week7_figs/embed_color.png\") \n```\n\nSource: [Illustrated Word2Vec](https://jalammar.github.io/illustrated-word2vec/)\n\n\n## Word Embeddings\n\n**Encoding similarity:** vectors are not ortogonal anymore!\n\n**Automatic Generalization:** learn about one word allow us to automatically learn about related words\n\n**Encodes Meaning:** by learning the context, I can learn what a word means.\n\n**As a consequence:**\n\n-   Word Embeddings improves by ORDERS OF MAGNITUDE several Text-as-Data Tasks.\n\n-   Allows to deal with unseen words.\n\n-   Form the core idea of state-of-the-art models, such as LLMs.\n\n## Deep Learning for Text Analysis\n\n```{r echo=FALSE, out.width = \"70%\"}\nknitr::include_graphics(\"./figs/deep_learning.png\") \n```\n\n# Introduction to Deep Learning\n\n## Basics of Machine Learning\n\n**Deep Learning** is a subfield of machine learning based on using neural networks models to learn.\n\nAs in any other statistical model, the goal of machine learning is to use data to learn about some output. \n\n$$ y = f(X) + \\epsilon$$\n\n- **Where:**\n\n -   $y$ is the outcome/dependent/response variable\n\n -   $X$ is a matrix of predictors/features/independent variables\n\n -   $f()$ is some fixed but unknown function mapping X to y. The \"signal\" in the data\n\n -   $\\epsilon$ is some random error term. The \"noise\" in the data.\n\n\n## Linear models (OLS)\n\nThe simplest model we can use is an linear model (the classic OLS regression)\n\n$$ y = b_0 + WX + \\epsilon$$\n\n- Where: \n\n  - $W$ is a vector of dimension p, \n  - $X$ is the feature vector of dimension p\n  - $b$ is a bias term (intercept)\n\n## Using Matrix Algebra\n\n$$\\mathbf{W} = \\begin{bmatrix} w_1 & w_2 & \\dots & w_p\\end{bmatrix}$$\n\n\n$$\\mathbf{X} = \\begin{bmatrix} X_1 \\\\ X_2 \\\\ X_3 \\\\ \\vdots \\\\ X_p \\end{bmatrix}$$\n\nWith matrix multiplication:\n\n$$\\mathbf{W} \\mathbf{X} + b = w_1 X_1 + w_2 X_2 + \\dots + w_p X_p + b$$\n\n## Logistic Regression\n\nIf we want to model some type of non-linearity, necessary for example when our outcome is binary, we can add a transformation function to make things non-linear:\n\n$$ y = \\sigma (b_0 + WX + \\epsilon)$$\n\nWhere: \n\n$$ \\sigma(b_0 + WX + \\epsilon) = \\frac{1}{1 + \\exp(-b_0 + WX + \\epsilon)}$$\n\n```{r echo=FALSE, out.width = \"100%\"}\nknitr::include_graphics(\"./figs/sigmoid.png\") \n```\n\n## Logistic Regression as a Neural Network\n\nLet's assume we have a simple model of voting. We want to predict if individual $i$ will vote ($y=1$), and we will use four socio demographic factors to make this prediction. \n\nClassic statistical approach with logistic regression: \n\n$$ \\hat{P(Y_i=1|X)} = \\sigma(b_0 + WX + \\epsilon) $$\nWe use MLE (Maximum Likelihood estimation) to find the parameters $W$ and $b_0$. We assume:\n\n$$ Y_i \\sim \\text{Bernoulli}(\\pi_i) $$\n\nThe likelihood function for $n$ independent observations is:\n\n$$L(W, b_0) = \\prod_{i=1}^{n} \\pi_i^{y_i} (1 - \\pi_i)^{1 - y_i}.$$\n\n\n## Neural Network Graphical Representation\n\n```{r echo=FALSE, out.width = \"100%\"}\nknitr::include_graphics(\"./figs/nn.png\") \n```\n\n::: aside\nSource: https://carpentries-incubator.github.io/ml4bio-workshop/05-logit-ann/index.html\n:::\n\n## Neural Networks\n\nA **Neural Network** is equivalent to stacking multiple logistic regressions vertically and repeat this process multiple times across many layers.\n\nAs a Matrix, instead of: \n\n$$\\mathbf{W}_{previous} = \\begin{bmatrix} w_1 & w_2 & \\dots & w_p\\end{bmatrix}$$\n\n\nWe use this set of parameters: \n\n$$\n\\mathbf{W} = \\begin{bmatrix} w_{11} & w_{12} & w_{13} & \\dots & w_{1p} \\\\\n                               w_{21} & w_{22} & w_{23} & \\dots & w_{2p} \\\\\n                               w_{31} & w_{32} & w_{33} & \\dots & w_{3p} \\\\\n                               \\vdots & \\vdots & \\vdots & \\vdots & \\vdots \\\\\n                               w_{k1} & w_{k2} & w_{k3} & \\dots & w_{kp}\n                \\end{bmatrix}\n$$                \n\n\n## \n\nThen, every line becomes a different logistic regression\n\n$$\\mathbf{WX} = \\begin{bmatrix}  \\sigma(w_{11} X_1 + w_{12} X_2 + \\dots + w_{1p}X_p) \\\\ \\sigma(w_{21} X_1 + w_{22} X_2 + \\dots + w_{2p}X_p) \\\\ \\sigma(w_{31} X_1 + w_{32} X_2 + \\dots + w_{3p}X_p )\\\\ \\vdots \\\\ \\sigma(w_{k1} X_1 + w_{k2} X_2 + \\dots + w_{kp}X_p) \\end{bmatrix}$$\nWe then combine all of those with another set of parameters: \n\n$$\n\\begin{align*}\n    \\mathbf{HA} &= h1 \\cdot \\sigma(w_{11} X_1 + w_{12} X_2 + \\dots + w_{1p}X_p)\\\\\n                &+ h_2\\cdot \\sigma(w_{21} X_1 + w_{22} X_2 + \\dots + w_{2p}X_p)\\\\\n                &+ h_3 \\cdot \\sigma (w_{31} X_1 + w_{32} X_2 + \\dots + w_{3p}X_p)\\\\\n                &+ \\dots + h_k + \\sigma(w_{k1} X_1 + w_{k2} X_2 + \\dots + w_{kp}X_p)\n\\end{align*}\n$$\n\n## Feed Forward on a Deep Neural Network\n\n```{r echo=FALSE, out.width = \"100%\"}\nknitr::include_graphics(\"./figs/feed_forward.png\") \n```\n\n## Key Components \n\n- Input: p features (the original data) of an observation are linearly transformed into k1 features using a weight matrix of size k1×p\n\n- Embedding Matrices: Paremeters you multiply your data by. \n\n- Neurons: Number of dimensions on your embedding matrices\n\n- Hidden Layer: the transformation that consists of the linear transformation and an activation function\n\n- Output Layer: the transformation that consists of the linear transformation and then (usually) a sigmoid (or some other activation function) to produce the final output predictions\n\n##\n\n![](https://media4.giphy.com/media/v1.Y2lkPTc5MGI3NjExYmY4dWFlNDgzM3FiY3RnM3ZweXZyejBpcjM2c291cWduZnhoYjVudCZlcD12MV9pbnRlcm5hbF9naWZfYnlfaWQmY3Q9Zw/lJnAXeJO8tE7E37mxq/giphy.gif)\n\n\n## Let's take a step back! \n\nWeights matrices are just parameters... the $\\beta_s$ of our regression models. \n\n- There are MANY of them!! Neural Networks are a black box!\n\n- But... they also serve as way to project features (words) in a dense dimensional space.\n\n- Remember: \n\n   - **One-hot encoding / Sparse Representation:**\n\n      -   cat = $\\begin{bmatrix} 0,0, 0, 0, 0, 0, 1, 0, 0 \\end{bmatrix}$\n\n      -   dog = $\\begin{bmatrix} 0,0, 0, 0, 0, 1, 0, 0, 0 \\end{bmatrix}$\n\n   - **Word Embedding / Dense Representation:**\n\n      -   cat = $\\begin{bmatrix} 0.25, -0.75, 0.90, 0.12, -0.50, 0.33, 0.66, -0.88, 0.10, -0.45 \\end{bmatrix}$\n\n      -   dog = $\\begin{bmatrix} 0.25, 1.75, 0.90, 0.12, -0.50, 0.33, 0.66, -0.88, 0.10, -0.45 \\end{bmatrix}$\n\n\n## Deep Neural Network for Textual Data\n\n```{r echo=FALSE, out.width = \"100%\"}\nknitr::include_graphics(\"./figs/nlp_nn.png\") \n```\n\n\n## Estimation: Gradient Descent\n\n**Linear Regression**\n\n$$\\text{RSS} = \\sum_{i=1}^{n} (y_i - \\widehat{y}_i)^2 $$\n\n\n**Logistic Regression**\n\n$$L(\\beta) = -\\sum_{i=1}^{n} \\left( y_i \\log(\\widehat{y}_i) + (1-y_i)\\log(1 - \\widehat{y}_i) \\right)$$\n\n**Fully-Connected Neural Network (Classification, Binary)**\n\n$$L(\\mathbf{W}) = -\\frac{1}{n}\\sum_{i=1}^{n} \\left( y_i \\log(\\widehat{y}_i) + (1-y_i)\\log(1 - \\widehat{y}_i) \\right)$$\n\nThis is called a binary cross entropy loss function\n\n##\n\n\n```{r echo=FALSE, out.width = \"100%\"}\nknitr::include_graphics(\"./figs/loss.png\") \n```\n\n\n## Updating parameters\n\n- Define a loss function\n\n- Gradient Descent Algorithm: \n\n   - Initialize weights randomly\n   \n   - Feed Forward: Matrix Multiplication + Activation function\n   \n   - Get the loss for this iteration\n   \n   - Compute gradient (partial derivatives): $\\frac{\\partial J(\\mathbf{W})}{\\partial \\mathbf{W}}$\n\n   \n   - Update weights: $W_{new}: W_{old} -\\eta \\cdot \\frac{\\partial J(\\mathbf{W})}{\\partial \\mathbf{W}}$\n\n   - Loop until convergence:\n\n# Code!"},"formats":{"revealjs":{"identifier":{"display-name":"RevealJS","target-format":"revealjs","base-format":"revealjs"},"execute":{"fig-width":10,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":true,"eval":true,"cache":true,"freeze":false,"echo":false,"output":true,"warning":false,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"knitr"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","html-math-method":{"method":"mathjax","url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS_HTML-full"},"slide-level":2,"to":"revealjs","incremental":true,"output-file":"day_1.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items"},"metadata":{"lang":"en","fig-responsive":false,"quarto-version":"1.3.450","auto-stretch":true,"editor":"visual","title":"Advanced Text-As-Data - Winter School - Iesp UERJ","subtitle":"<span style = 'font-size: 140%;'> <br> Day 1: Word Representation & Introduction to Neural Networks","author":"<span style = 'font-size: 120%;'> Professor: Tiago Ventura </span>","editor_options":{"chunk_output_type":"console"},"transition":"slide","backgroundTransition":"fade","width":1200,"height":800,"center":false,"slideNumber":true,"chalkboard":{"buttons":false},"previewLinks":"auto","footer":"Text-as-Data","theme":["simple","custom.scss"]}}},"projectFormats":["html"]}