{"title":"Advanced Text-As-Data: Word Embeddings, Deep Learning, and Large Language Models","markdown":{"yaml":{"title":"Advanced Text-As-Data: Word Embeddings, Deep Learning, and Large Language Models","subtitle":"Winter School - Iesp UERJ","author":"Tiago Ventura and Sebastian Vallejo Vera","format":"html"},"headingText":"Course Description","containsRefs":false,"markdown":"\n\n\n\nIn recent years, the surge in the availability of textual data, ranging from the digitalization of archival documents, political speeches, social media posts, and online news articles, has led to a growing demand for statistical analysis using large corpora. Once dominated by sparse bag-of-words models, the field of Natural Language Processing (NLP) is now increasingly driven by dense vector representations, deep learning, and the rapid evolution of Large Language Models (LLMs). This course offers an introduction to this new generation of models, serving as hands-on approach to this new landscape of computational text analysis with a focus on political science research and applications.\n\nThe class will cover four broad topics. We start with an overview of how to represent text as data, from a sparse representation via bag-of-words models, to a dense representation using word embeddings. We then discuss the use of deep learning models for text representation and downstream classification tasks. From here, we will discuss the foundation of the state-of-art machine learning models for text analysis: transformer models. Lastly, we will discuss several applications of Large Language Models in social science tasks.\n\nThe course will consist of lectures and hands-on coding in class. The lecture will be conducted in English, but students are free to ask questions in Portuguese. Students will have time in the afternoon to practice the code seen in class, and we will suggest additional coding exercises. We assume students attending this class have taken, at a minimum, an introductory course in statistics and have basic knowledge of probability distributions, calculus, hypothesis testing, and linear models. The course will use a mix of R and Python, two computational languages students should be familiar with. That being said, students should be able to follow the course even if they are just starting with any of the two programming languages.\n\n## Instructors\n\n[Tiago Ventura](https://www.venturatiago.com/)\n\n-   Assistant Professor in Computational Social Science, Georgetown University\n-   Pronouns: He/Him\n-   Email: [tv186\\@georgetown.edu](mailto:tv186@georgetown.edu)\n\n[Sebastian Vallejo](https://www.svallejovera.com/)\n\n-   Assistant Professor in the Department of Political Science at the University of Western Ontario\n-   Pronouns: He/Him\n-   Email: [sebastian.vallejo\\@uwo.ca](mailto:sebastian.vallejo@uwo.ca)\n\n<!-- add zoom link for virtual office hours -->\n\n<!-- Download the [syllabus in pdf here](\"./_syllabus.pdf\") -->\n\n## Required Materials\n\n**Readings**: We will rely primarily on the following textbook for this course. The textbook is freely available online\n\n-   Daniel Jurafsky and James H. Martin. [Speech and Language Processing, 3nd Edition.](https://web.stanford.edu/~jurafsky/slp3/ed3book.pdf) - \\[SLP\\]\n\n**The weekly articles are listed in the syllabus**\n\n## Schedule & Readings\n\n### Day 1: Text Representation: Sparse & Dense Vectors. Deep Learning Models for Text Analysis\n\n-   **Readings**:\n\n    -   [SLP: Chapters 7](https://stanford.edu/~jurafsky/slp3/7.pdf)\n\n    -   [Lin, Gechun, and Christopher Lucas. \"An Introduction to Neural Networks for the Social Sciences.\" (2023)](https://christopherlucas.org/files/PDFs/nn_chapter.pdf)\n\n### Day 2: Word Embeddings: Theory and Applications\n\n-   **Theory Papers**:\n\n    -   [SLP Chapter 6, Vector Semantics and Embeddings](https://stanford.edu/~jurafsky/slp3/6.pdf)\n\n    -   [Meyer, David. How Exactly Does Word2Vec Work?](https://swimm.io/learn/large-language-models/what-is-word2vec-and-how-does-it-work)\n\n    -   [Spirling and Rodriguez, Word embeddings: What works, what doesn't, and how to tell the difference for applied research](https://github.com/ArthurSpirling/EmbeddingsPaper)\n\n-   **Applied Papers**:\n\n    -   [Austin C. Kozlowski, Austin C., Matt Taddy, and James A. Evans. 2019. \"The Geometry of Culture: Analyzing the Meanings of Class through Word Embeddings.\" American Sociological Review 84, no. 5: 905--49. https://doi.org/10.1177/0003122419877135]()\n\n    -   [Garg, Nikhil, Londa Schiebinger, Dan Jurafsky and James Zou. 2018. \"Word embeddings quantify 100 years of gender and ethnic stereotypes.\" Proceedings of the National Academy of Sciences 115(16):E3635--E3644.](https://www.pnas.org/doi/10.1073/pnas.1720347115)\n\n    -   [Rodman, E., 2020. A Timely Intervention: Tracking the Changing Meanings of Political Concepts with Word Vectors. Political Analysis, 28(1), pp.87-111]()\n\n    -   [Rheault, Ludovic, and Christopher Cochrane. \"Word embeddings for the analysis of ideological placement in parliamentary corpora.\" Political Analysis 28, no. 1 (2020): 112-133.](https://www.cambridge.org/core/journals/political-analysis/article/word-embeddings-for-the-analysis-of-ideological-placement-in-parliamentary-corpora/017F0CEA9B3DB6E1B94AC36A509A8A7B)\n\n    -   [Gennaro, Gloria, and Elliott Ash. \"Emotion and reason in political language.\" The Economic Journal 132, no. 643 (2022): 1037-1059.](https://academic.oup.com/ej/article/132/643/1037/6490125)\n\n### Day 3: Transformes: Theory and Fine-tuning a Transformers-based model\n\n-   **Theory Papers**\n\n    -   \\[SLP\\] - Chapter 10.\n\n    -   Jay Alammar. 2018. [\"The Illustrated Transformer.\"](https://jalammar.github.io/illustratedtransformer/)\n\n    -   [Vaswani, A., et al. (2017). Attention is all you need. Advances in neural information processing systems, 30.](https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf)\n\n    -   [Timoneda and Vallejo Vera (2025). BERT, RoBERTa or DeBERTa? Comparing Performance Across Transformer Models in Political Science Text. Journal of Politics.](https://svallejovera.github.io/files/bert_roberta_jop.pdf)\n\n-   **Applied Papers**\n\n    -   [Vallejo Vera et al. (2025). Semi-Supervised Classification of Overt and Covert Racism in Text. Working Paper.](https://github.com/svallejovera/svallejovera.github.io/blob/master/files/racism_nlp.pdf)\n\n### Day 4: Large Language Models: Social Science Applications\n\n-   **Applied Papers**\n\n    -   [Wu, Patrick Y., Joshua A. Tucker, Jonathan Nagler, and Solomon Messing. \"Large language models can be used to estimate the ideologies of politicians in a zero-shot learning setting.\" arXiv preprint arXiv:2303.12057 (2023)](https://arxiv.org/abs/2303.12057).\n\n    -   [Rathje, Steve, Dan-Mircea Mirea, Ilia Sucholutsky, Raja Marjieh, Claire Robertson, and Jay J. Van Bavel. GPT is an effective tool for multilingual psychological text analysis. (2023)](https://osf.io/preprints/psyarxiv/sekf5).\n\n    -   [Davidson, Thomas: Start Generating: Harnessing Generative Artificial Intelligence for Sociological Research, PrePrint](https://osf.io/preprints/socarxiv/u9nft)\n\n    -   [Bisbee, James, Joshua Clinton, Cassy Dorff, Brenton Kenkel, and Jennifer Larson. 2023. \"Synthetic Replacements for Human Survey Data? the Perils of Large Language Models.\" SocArXiv. May 4.](https://osf.io/preprints/socarxiv/5ecfa)\n\n    -   [Argyle, L.P., Busby, E.C., Fulda, N., Gubler, J.R., Rytting, C. and Wingate, D., 2023. Out of one, many: Using language models to simulate human samples. Political Analysis, 31(3), pp.337-351.](https://www.cambridge.org/core/journals/political-analysis/article/out-of-one-many-using-language-models-to-simulate-human-samples/035D7C8A55B237942FB6DBAD7CAA4E49)\n\n    -   [Walker, C., & Timoneda, J. C. (2024). Identifying the sources of ideological bias in GPT models through linguistic variation in output. arXiv preprint arXiv:2409.06043.](https://arxiv.org/pdf/2409.06043)\n\n    -   Timoneda, Joan C., and Sebasti√°n Vallejo Vera. \"Memory Is All You Need: Testing How Model Memory Affects LLM Performance in Annotation Tasks.\" arXiv preprint arXiv:2503.04874 (2025).\n","srcMarkdownNoYaml":"\n\n\n## Course Description\n\nIn recent years, the surge in the availability of textual data, ranging from the digitalization of archival documents, political speeches, social media posts, and online news articles, has led to a growing demand for statistical analysis using large corpora. Once dominated by sparse bag-of-words models, the field of Natural Language Processing (NLP) is now increasingly driven by dense vector representations, deep learning, and the rapid evolution of Large Language Models (LLMs). This course offers an introduction to this new generation of models, serving as hands-on approach to this new landscape of computational text analysis with a focus on political science research and applications.\n\nThe class will cover four broad topics. We start with an overview of how to represent text as data, from a sparse representation via bag-of-words models, to a dense representation using word embeddings. We then discuss the use of deep learning models for text representation and downstream classification tasks. From here, we will discuss the foundation of the state-of-art machine learning models for text analysis: transformer models. Lastly, we will discuss several applications of Large Language Models in social science tasks.\n\nThe course will consist of lectures and hands-on coding in class. The lecture will be conducted in English, but students are free to ask questions in Portuguese. Students will have time in the afternoon to practice the code seen in class, and we will suggest additional coding exercises. We assume students attending this class have taken, at a minimum, an introductory course in statistics and have basic knowledge of probability distributions, calculus, hypothesis testing, and linear models. The course will use a mix of R and Python, two computational languages students should be familiar with. That being said, students should be able to follow the course even if they are just starting with any of the two programming languages.\n\n## Instructors\n\n[Tiago Ventura](https://www.venturatiago.com/)\n\n-   Assistant Professor in Computational Social Science, Georgetown University\n-   Pronouns: He/Him\n-   Email: [tv186\\@georgetown.edu](mailto:tv186@georgetown.edu)\n\n[Sebastian Vallejo](https://www.svallejovera.com/)\n\n-   Assistant Professor in the Department of Political Science at the University of Western Ontario\n-   Pronouns: He/Him\n-   Email: [sebastian.vallejo\\@uwo.ca](mailto:sebastian.vallejo@uwo.ca)\n\n<!-- add zoom link for virtual office hours -->\n\n<!-- Download the [syllabus in pdf here](\"./_syllabus.pdf\") -->\n\n## Required Materials\n\n**Readings**: We will rely primarily on the following textbook for this course. The textbook is freely available online\n\n-   Daniel Jurafsky and James H. Martin. [Speech and Language Processing, 3nd Edition.](https://web.stanford.edu/~jurafsky/slp3/ed3book.pdf) - \\[SLP\\]\n\n**The weekly articles are listed in the syllabus**\n\n## Schedule & Readings\n\n### Day 1: Text Representation: Sparse & Dense Vectors. Deep Learning Models for Text Analysis\n\n-   **Readings**:\n\n    -   [SLP: Chapters 7](https://stanford.edu/~jurafsky/slp3/7.pdf)\n\n    -   [Lin, Gechun, and Christopher Lucas. \"An Introduction to Neural Networks for the Social Sciences.\" (2023)](https://christopherlucas.org/files/PDFs/nn_chapter.pdf)\n\n### Day 2: Word Embeddings: Theory and Applications\n\n-   **Theory Papers**:\n\n    -   [SLP Chapter 6, Vector Semantics and Embeddings](https://stanford.edu/~jurafsky/slp3/6.pdf)\n\n    -   [Meyer, David. How Exactly Does Word2Vec Work?](https://swimm.io/learn/large-language-models/what-is-word2vec-and-how-does-it-work)\n\n    -   [Spirling and Rodriguez, Word embeddings: What works, what doesn't, and how to tell the difference for applied research](https://github.com/ArthurSpirling/EmbeddingsPaper)\n\n-   **Applied Papers**:\n\n    -   [Austin C. Kozlowski, Austin C., Matt Taddy, and James A. Evans. 2019. \"The Geometry of Culture: Analyzing the Meanings of Class through Word Embeddings.\" American Sociological Review 84, no. 5: 905--49. https://doi.org/10.1177/0003122419877135]()\n\n    -   [Garg, Nikhil, Londa Schiebinger, Dan Jurafsky and James Zou. 2018. \"Word embeddings quantify 100 years of gender and ethnic stereotypes.\" Proceedings of the National Academy of Sciences 115(16):E3635--E3644.](https://www.pnas.org/doi/10.1073/pnas.1720347115)\n\n    -   [Rodman, E., 2020. A Timely Intervention: Tracking the Changing Meanings of Political Concepts with Word Vectors. Political Analysis, 28(1), pp.87-111]()\n\n    -   [Rheault, Ludovic, and Christopher Cochrane. \"Word embeddings for the analysis of ideological placement in parliamentary corpora.\" Political Analysis 28, no. 1 (2020): 112-133.](https://www.cambridge.org/core/journals/political-analysis/article/word-embeddings-for-the-analysis-of-ideological-placement-in-parliamentary-corpora/017F0CEA9B3DB6E1B94AC36A509A8A7B)\n\n    -   [Gennaro, Gloria, and Elliott Ash. \"Emotion and reason in political language.\" The Economic Journal 132, no. 643 (2022): 1037-1059.](https://academic.oup.com/ej/article/132/643/1037/6490125)\n\n### Day 3: Transformes: Theory and Fine-tuning a Transformers-based model\n\n-   **Theory Papers**\n\n    -   \\[SLP\\] - Chapter 10.\n\n    -   Jay Alammar. 2018. [\"The Illustrated Transformer.\"](https://jalammar.github.io/illustratedtransformer/)\n\n    -   [Vaswani, A., et al. (2017). Attention is all you need. Advances in neural information processing systems, 30.](https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf)\n\n    -   [Timoneda and Vallejo Vera (2025). BERT, RoBERTa or DeBERTa? Comparing Performance Across Transformer Models in Political Science Text. Journal of Politics.](https://svallejovera.github.io/files/bert_roberta_jop.pdf)\n\n-   **Applied Papers**\n\n    -   [Vallejo Vera et al. (2025). Semi-Supervised Classification of Overt and Covert Racism in Text. Working Paper.](https://github.com/svallejovera/svallejovera.github.io/blob/master/files/racism_nlp.pdf)\n\n### Day 4: Large Language Models: Social Science Applications\n\n-   **Applied Papers**\n\n    -   [Wu, Patrick Y., Joshua A. Tucker, Jonathan Nagler, and Solomon Messing. \"Large language models can be used to estimate the ideologies of politicians in a zero-shot learning setting.\" arXiv preprint arXiv:2303.12057 (2023)](https://arxiv.org/abs/2303.12057).\n\n    -   [Rathje, Steve, Dan-Mircea Mirea, Ilia Sucholutsky, Raja Marjieh, Claire Robertson, and Jay J. Van Bavel. GPT is an effective tool for multilingual psychological text analysis. (2023)](https://osf.io/preprints/psyarxiv/sekf5).\n\n    -   [Davidson, Thomas: Start Generating: Harnessing Generative Artificial Intelligence for Sociological Research, PrePrint](https://osf.io/preprints/socarxiv/u9nft)\n\n    -   [Bisbee, James, Joshua Clinton, Cassy Dorff, Brenton Kenkel, and Jennifer Larson. 2023. \"Synthetic Replacements for Human Survey Data? the Perils of Large Language Models.\" SocArXiv. May 4.](https://osf.io/preprints/socarxiv/5ecfa)\n\n    -   [Argyle, L.P., Busby, E.C., Fulda, N., Gubler, J.R., Rytting, C. and Wingate, D., 2023. Out of one, many: Using language models to simulate human samples. Political Analysis, 31(3), pp.337-351.](https://www.cambridge.org/core/journals/political-analysis/article/out-of-one-many-using-language-models-to-simulate-human-samples/035D7C8A55B237942FB6DBAD7CAA4E49)\n\n    -   [Walker, C., & Timoneda, J. C. (2024). Identifying the sources of ideological bias in GPT models through linguistic variation in output. arXiv preprint arXiv:2409.06043.](https://arxiv.org/pdf/2409.06043)\n\n    -   Timoneda, Joan C., and Sebasti√°n Vallejo Vera. \"Memory Is All You Need: Testing How Model Memory Affects LLM Performance in Annotation Tasks.\" arXiv preprint arXiv:2503.04874 (2025).\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"markdown"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true,"format-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["styles.scss"],"toc":true,"output-file":"syllabus.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.3.450","editor":"visual","theme":"cosmo","title":"Advanced Text-As-Data: Word Embeddings, Deep Learning, and Large Language Models","subtitle":"Winter School - Iesp UERJ","author":"Tiago Ventura and Sebastian Vallejo Vera"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}