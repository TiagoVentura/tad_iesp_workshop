{"headings":["plans-for-today","word-embeddings","vector-space-model-from-yesterday","how-can-we-embed-some-notion-of-meaning-in-the-way-we-represent-words","distributional-semantics","sparse-vs-dense-vectors","with-colors-and-real-word-vectors","why-word-embeddings","estimating-word-embeddings","approches","word2vec-a-framework-for-learning-word-vectors-mikolov-et-al.-2013","core-idea","skigram-example-self-supervision","skigram-example-self-supervision-1","encoding-similarity","this-is-similar-to-a-logistic-regression","pametrizing-pw_tw_t-1","softmax-transformation","word2vec-objective-function","likelihood-function","objective-function-negative-log-likelihood","neural-networks","skipgram-architecture","section","check-your-matrices","word-embeddings-matrices","training-embeddings","applications","applications-1","capturing-cultural-dimensions-with-embeddings","method","results","ideological-scaling","method-1","results-1","measuring-emotion","method-2"],"entries":[]}