{"entries":[],"headings":["survey-responses","plans-for-today","coding","word-embeddings","vector-space-model","how-can-we-embed-some-notion-of-meaning-in-the-way-we-represent-words","distributional-semantics","sparse-vs-dense-vectors","with-colors-and-real-word-vectors","why-word-embeddings","estimating-word-embeddings","approches","word2vec-a-framework-for-learning-word-vectors-mikolov-et-al.-2013","core-idea","skigram-example-self-supervision","skigram-example-self-supervision-1","encoding-similarity","this-is-similar-to-a-logistic-regression","pametrizing-pw_tw_t-1","softmax-transformation","word2vec-objective-function","likelihood-function","objective-function-negative-log-likelihood","neural-networks-brief-overview","skipgram-architecture","check-your-matrices","word-embeddings-matrices","applications","training-embeddings","decisions-on-embeddings-rodriguez-and-spirling-2022","findings","more-next-week"]}